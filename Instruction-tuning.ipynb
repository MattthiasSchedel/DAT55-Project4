{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from random import randrange\n",
    "from functools import partial\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "AutoTokenizer,\n",
    "BitsAndBytesConfig,\n",
    "HfArgumentParser,\n",
    "Trainer,\n",
    "TrainingArguments,\n",
    "DataCollatorForLanguageModeling,\n",
    "EarlyStoppingCallback,\n",
    "pipeline,\n",
    "logging,\n",
    "set_seed)\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BitsandBytes\n",
    "By using 4-bit transformer language models, we can achieve impressive results while significantly reducing memory and computational requirements.\n",
    "\n",
    "Hugging Face Transformers (`transformers`) is closely integrated with `bitsandbytes`. The `BitsAndBytesConfig` class from the `transformers` library allows configuring the model quantization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype):\n",
    "    \"\"\"\n",
    "    Configures model quantization method using bitsandbytes to speed up training and inference\n",
    "\n",
    "    :param load_in_4bit: Load model in 4-bit precision mode\n",
    "    :param bnb_4bit_use_double_quant: Nested quantization for 4-bit model\n",
    "    :param bnb_4bit_quant_type: Quantization data type for 4-bit model\n",
    "    :param bnb_4bit_compute_dtype: Computation data type for 4-bit model\n",
    "    \"\"\"\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        bnb_4bit_use_double_quant = bnb_4bit_use_double_quant,\n",
    "        bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Hugging Face Model and Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "    \"\"\"\n",
    "    Loads model and model tokenizer\n",
    "\n",
    "    :param model_name: Hugging Face model name\n",
    "    :param bnb_config: Bitsandbytes configuration\n",
    "    \"\"\"\n",
    "\n",
    "    # Get number of GPU device and set maximum memory\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{40960}MB'\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config = bnb_config,\n",
    "        device_map = \"auto\", # dispatch the model efficiently on the available resources\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "\n",
    "    # Load model tokenizer with the user authentication token\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token = False)\n",
    "\n",
    "    # Set padding token as EOS token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Transformers and Bitsandbytes Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"beomi/llama-2-ko-7b\"\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "load_in_4bit = True\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "bnb_4bit_use_double_quant = True\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Compute data type for 4-bit base models\n",
    "bnb_4bit_compute_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 15/15 [00:21<00:00,  1.45s/it]\n",
      "/home/ubuntu/myenv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:748: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bnb_config = create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype)\n",
    "\n",
    "model, tokenizer = load_model(model_name, bnb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "control_df = pd.read_csv(\"./Control_db.csv\")\n",
    "dementia_df = pd.read_csv(\"./Dementia_db.csv\")\n",
    "test_df = pd.read_csv(\"./Testing_db.csv\")\n",
    "\n",
    "\n",
    "# Combine the datasets\n",
    "df = pd.concat([control_df, dementia_df], ignore_index=True)\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts: 108\n",
      "Column names are: ['Language', 'Data', 'Participant', 'Age', 'Gender', 'Diagnosis', 'Category', 'mmse', 'Filename', 'Transcript']\n"
     ]
    }
   ],
   "source": [
    "# The instruction dataset to use\n",
    "dataset_name = [\"./Control_db.csv\",\"./Dementia_db.csv\", ]\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"csv\", data_files = dataset_name, split='train')\n",
    "\n",
    "print(f'Number of prompts: {len(dataset)}')\n",
    "print(f'Column names are: {dataset.column_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Language', 'Data', 'Participant', 'Age', 'Gender', 'Diagnosis', 'Category', 'mmse', 'Filename', 'Transcript'],\n",
       "    num_rows: 108\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample, instruction= None):\n",
    "    \"\"\"\n",
    "    Creates a formatted prompt template for a prompt in the instruction dataset\n",
    "\n",
    "    :param sample: Prompt or sample from the instruction dataset\n",
    "    \"\"\"\n",
    "    label_map = {0: \"healthy\", 1: \"alzheimers\"}\n",
    "    instruction = \"The input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\"\n",
    "\n",
    "    # Initialize static strings for the prompt template\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruction:\"\n",
    "    INPUT_KEY = \"Input:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "\n",
    "    # Combine a prompt with the static strings\n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\\n{instruction}\"\n",
    "    input_context = f\"{INPUT_KEY}\\n{sample['Transcript']}\" if sample['Transcript'] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{label_map[sample['Category']]}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "\n",
    "    # Create a list of prompt template elements\n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    # Join prompt template elements into a single string to create the prompt template\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "\n",
    "    # Store the formatted prompt template in a new key “text\"\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Language': 'eng',\n",
       " 'Data': 'Pitt',\n",
       " 'Participant': 'PAR',\n",
       " 'Age': 77,\n",
       " 'Gender': 'female',\n",
       " 'Diagnosis': 'Control',\n",
       " 'Category': 0,\n",
       " 'mmse': 29.0,\n",
       " 'Filename': 'S077',\n",
       " 'Transcript': \" alright . the little boy [//] girl's reaching up there and she's got her. and the boy's <on a stool> [//] standing on a stool but its gonna. he's reaching for cookies . and the mother has their [: her] [* s:r] back to (th)em . she doesn't see them but her [//] she hasn't payed any attention to. it's running all over the place . her window is open and there's a nice garden and a garage out back. she's drying some dishes . she has an apron on . her &s dress is sleeveless . has a pretty nice kitchen like &uh pretty nice curtains on it . and &um <the jar> [//] the cookie jar lid is off . she has a couple cups and she has a couple plates but she better. the little girl has socks and shoes on and a short dress and <long. and the boy has his shorts on and a short sleeved s:irt [: shirt]. trees are in bloom outside and the flowers out there .\",\n",
       " 'text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n alright . the little boy [//] girl's reaching up there and she's got her. and the boy's <on a stool> [//] standing on a stool but its gonna. he's reaching for cookies . and the mother has their [: her] [* s:r] back to (th)em . she doesn't see them but her [//] she hasn't payed any attention to. it's running all over the place . her window is open and there's a nice garden and a garage out back. she's drying some dishes . she has an apron on . her &s dress is sleeveless . has a pretty nice kitchen like &uh pretty nice curtains on it . and &um <the jar> [//] the cookie jar lid is off . she has a couple cups and she has a couple plates but she better. the little girl has socks and shoes on and a short dress and <long. and the boy has his shorts on and a short sleeved s:irt [: shirt]. trees are in bloom outside and the flowers out there .\\n\\n### Response:\\nhealthy\\n\\n### End\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_prompt_formats(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Maximum Sequence Length of the Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(model):\n",
    "    \"\"\"\n",
    "    Extracts maximum token length from the model configuration\n",
    "\n",
    "    :param model: Hugging Face model\n",
    "    \"\"\"\n",
    "\n",
    "    # Pull model configuration\n",
    "    conf = model.config\n",
    "    # Initialize a \"max_length\" variable to store maximum sequence length as null\n",
    "    max_length = None\n",
    "    # Find maximum sequence length in the model configuration and save it in \"max_length\" if found\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "        # Set \"max_length\" to 1024 (default value) if maximum sequence length is not found in the model configuration\n",
    "        if not max_length:\n",
    "            max_length = 1024\n",
    "            print(f\"Using default max length: {max_length}\")\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Dataset Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizes dataset batch\n",
    "\n",
    "    :param batch: Dataset batch\n",
    "    :param tokenizer: Model tokenizer\n",
    "    :param max_length: Maximum number of tokens to emit from the tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length = max_length,\n",
    "        truncation = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
    "    \"\"\"\n",
    "    Tokenizes dataset for fine-tuning\n",
    "\n",
    "    :param tokenizer (AutoTokenizer): Model tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from the tokenizer\n",
    "    :param seed: Random seed for reproducibility\n",
    "    :param dataset (str): Instruction dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset…\")\n",
    "    dataset = dataset.map(create_prompt_formats)\n",
    "\n",
    "    # Apply preprocessing to each batch of the dataset & and remove \"instruction\", \"input\", \"output\", and \"text\" fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length = max_length, tokenizer = tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched = True,\n",
    "        remove_columns = ['Language', 'Data','Participant','Age','Gender','Diagnosis','Category','mmse','Filename','Transcript','text'],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have \"input_ids\" exceeding \"max_length\"\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed = seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default max length: 1024\n",
      "Found max lenth: 2048\n",
      "Preprocessing dataset…\n"
     ]
    }
   ],
   "source": [
    "# Random seed\n",
    "seed = 33\n",
    "\n",
    "max_length = get_max_length(model)\n",
    "preprocessed_dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 108\n",
      "})\n",
      "{'input_ids': [1, 13866, 338, 385, 15278, 393, 16612, 263, 3414, 29889, 14350, 263, 2933, 393, 7128, 2486, 1614, 2167, 278, 2009, 29889, 13, 13, 2277, 29937, 2799, 4080, 29901, 13, 1576, 1881, 338, 263, 1301, 3395, 310, 263, 16500, 1058, 1033, 505, 278, 394, 29920, 6391, 414, 17135, 29889, 16564, 373, 278, 1301, 3395, 10049, 411, 525, 354, 4298, 29891, 29915, 470, 525, 284, 29920, 6391, 414, 29915, 5034, 304, 278, 22069, 24876, 19263, 29889, 13, 13, 4290, 29901, 13, 20759, 869, 278, 11379, 29915, 29879, 471, 11222, 29898, 29887, 29897, 278, 270, 17006, 7878, 433, 6129, 29879, 763, 896, 2337, 437, 869, 263, 2217, 8023, 29915, 29879, 701, 373, 278, 380, 1507, 6416, 262, 29898, 29887, 29897, 975, 869, 540, 29915, 29879, 1018, 262, 29898, 29887, 29897, 304, 679, 263, 15327, 869, 322, 278, 7826, 29915, 29879, 1018, 262, 29898, 29887, 29897, 304, 1371, 322, 1183, 29915, 29879, 6159, 262, 29898, 29887, 29897, 902, 1361, 701, 869, 322, 896, 529, 1028, 24455, 4094, 714, 29918, 974, 278, 29958, 518, 458, 29962, 669, 29879, 669, 1610, 975, 661, 278, 28169, 869, 805, 24455, 4094, 599, 975, 869, 13, 13, 2277, 29937, 13291, 29901, 13, 284, 29920, 6391, 414, 13, 13, 2277, 29937, 2796], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_dataset)\n",
    "print(preprocessed_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating PEFT Configuration\n",
    "\n",
    "Fine-tuning pretrained LLMs on downstream datasets results in huge performance gains when compared to using the pretrained LLMs out-of-the-box. However, as models get larger and larger, full fine-tuning becomes infeasible to train on consumer hardware. In addition, storing and deploying fine-tuned models independently for each downstream task becomes very expensive, because fine-tuned models are the same size as the original pretrained model. Parameter-Efficient Fine-tuning (PEFT) approaches are meant to address both problems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_peft_config(r, lora_alpha, target_modules, lora_dropout, bias, task_type):\n",
    "    \"\"\"\n",
    "    Creates Parameter-Efficient Fine-Tuning configuration for the model\n",
    "\n",
    "    :param r: LoRA attention dimension\n",
    "    :param lora_alpha: Alpha parameter for LoRA scaling\n",
    "    :param modules: Names of the modules to apply LoRA to\n",
    "    :param lora_dropout: Dropout Probability for LoRA layers\n",
    "    :param bias: Specifies if the bias parameters should be trained\n",
    "    \"\"\"\n",
    "    config = LoraConfig(\n",
    "        r = r,\n",
    "        lora_alpha = lora_alpha,\n",
    "        target_modules = target_modules,\n",
    "        lora_dropout = lora_dropout,\n",
    "        bias = bias,\n",
    "        task_type = task_type,\n",
    "    )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Modules for LoRA Application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    \"\"\"\n",
    "    Find modules to apply LoRA to.\n",
    "\n",
    "    :param model: PEFT model\n",
    "    \"\"\"\n",
    "\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "        if 'lm_head' in lora_module_names:\n",
    "            lora_module_names.remove('lm_head')\n",
    "            print(f\"LoRA module names: {list(lora_module_names)}\")\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Trainable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit = False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "\n",
    "    :param model: PEFT model\n",
    "    \"\"\"\n",
    "\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "\n",
    "        if use_4bit:\n",
    "            trainable_params /= 2\n",
    "\n",
    "    print(f\"All Parameters: {all_param:,d} || Trainable Parameters: {trainable_params:,d} || Trainable Parameters %: {100 * trainable_params / all_param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    lora_r,\n",
    "    lora_alpha,\n",
    "    lora_dropout,\n",
    "    bias,\n",
    "    task_type,\n",
    "    per_device_train_batch_size,\n",
    "    gradient_accumulation_steps,\n",
    "    warmup_steps,\n",
    "    max_steps,\n",
    "    learning_rate,\n",
    "    fp16,\n",
    "    logging_steps,\n",
    "    output_dir,\n",
    "    optim):\n",
    "    \"\"\"\n",
    "    Prepares and fine-tune the pre-trained model.\n",
    "\n",
    "    :param model: Pre-trained Hugging Face model\n",
    "    :param tokenizer: Model tokenizer\n",
    "    :param dataset: Preprocessed training dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Enable gradient checkpointing to reduce memory usage during fine-tuning\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Prepare the model for training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Get LoRA module names\n",
    "    target_modules = find_all_linear_names(model)\n",
    "\n",
    "    # Create PEFT configuration for these modules and wrap the model to PEFT\n",
    "    peft_config = create_peft_config(lora_r, lora_alpha, target_modules, lora_dropout, bias, task_type)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    # Print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(model)\n",
    "\n",
    "    # Training parameters\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        train_dataset = dataset,\n",
    "        args = TrainingArguments(\n",
    "            per_device_train_batch_size = per_device_train_batch_size,\n",
    "            gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "            warmup_steps = warmup_steps,\n",
    "            max_steps = max_steps,\n",
    "            learning_rate = learning_rate,\n",
    "            fp16 = fp16,\n",
    "            logging_steps = logging_steps,\n",
    "            output_dir = output_dir,\n",
    "            optim = optim,\n",
    "        ),\n",
    "        data_collator = DataCollatorForLanguageModeling(tokenizer, mlm = False)\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    do_train = True\n",
    "\n",
    "    # Launch training and log metrics\n",
    "    print(\"Training…\")\n",
    "\n",
    "    # if do_train:\n",
    "    train_result = trainer.train()\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    print(metrics)\n",
    "\n",
    "    # Save model\n",
    "    print(\"Saving last checkpoint of the model…\")\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "\n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing QLoRA and TrainingArguments parameters below for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Parameters: 3,657,830,400 || Trainable Parameters: 39,976,960 || Trainable Parameters %: 1.0929145320679712\n",
      "Training…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/myenv/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 12:22, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.203500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.552800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.037700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.674400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.069500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.356200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.609200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.885900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.117300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.686100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.783200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.528500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.877900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.413400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.519600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.503600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.497200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.241100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.479000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.721000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.077400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.477500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.160100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.482400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.514500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.562000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.212800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.991900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.085300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.089300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.205100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.393600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.448900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.065100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.474300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.183300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.213800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.074500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.941200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.113400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.964700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.928800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.314400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.294700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.180800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.176400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.986500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.265200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.412100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.081400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.838600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.850900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.093900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.079300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.079900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.794300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.074700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.782000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.957500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.878900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.947700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.799800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.790500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.183700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.883600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.674600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.839400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.826400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.844500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.032100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.789500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.764500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.803200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.977600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.744200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.711300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.170800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.676800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.730800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.740700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.877600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.759400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.007700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.596800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.478800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.722800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.451300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.510700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.467700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.807300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.919700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.848200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.656100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =     3.7037\n",
      "  total_flos               =  3776183GF\n",
      "  train_loss               =     1.2614\n",
      "  train_runtime            = 0:12:30.41\n",
      "  train_samples_per_second =      0.533\n",
      "  train_steps_per_second   =      0.133\n",
      "{'train_runtime': 750.4131, 'train_samples_per_second': 0.533, 'train_steps_per_second': 0.133, 'total_flos': 4054646644088832.0, 'train_loss': 1.2613574823737144, 'epoch': 3.7037037037037037}\n",
      "Saving last checkpoint of the model…\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 16\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 64\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "# Bias\n",
    "bias = \"none\"\n",
    "\n",
    "# Task type\n",
    "task_type = \"CAUSAL_LM\"\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = 100\n",
    "\n",
    "# Linear warmup steps from 0 to learning_rate\n",
    "warmup_steps = 2\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = True\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 1\n",
    "\n",
    "\"\"\"Calling the `fine_tune` function below to fine-tune or instruction-tune the pre-trained model on our preprocessed news classification instruction dataset.\"\"\"\n",
    "\n",
    "fine_tune(model,\n",
    "tokenizer,\n",
    "preprocessed_dataset,\n",
    "lora_r,\n",
    "lora_alpha,\n",
    "lora_dropout,\n",
    "bias,\n",
    "task_type,\n",
    "per_device_train_batch_size,\n",
    "gradient_accumulation_steps,\n",
    "warmup_steps,\n",
    "max_steps,\n",
    "learning_rate,\n",
    "fp16,\n",
    "logging_steps,\n",
    "output_dir,\n",
    "optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 15/15 [00:11<00:00,  1.25it/s]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# Load fine-tuned weights\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map = \"auto\", torch_dtype = torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={'':0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_test(sample, instruction= None):\n",
    "    \"\"\"\n",
    "    Creates a formatted prompt template for a prompt in the instruction dataset\n",
    "\n",
    "    :param sample: Prompt or sample from the instruction dataset\n",
    "    \"\"\"\n",
    "    instruction = \"The input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\"\n",
    "\n",
    "    # Initialize static strings for the prompt template\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruction:\"\n",
    "    INPUT_KEY = \"Input:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "\n",
    "    # Combine a prompt with the static strings\n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\\n{instruction}\"\n",
    "    input_context = f\"{INPUT_KEY}\\n{sample['Transcript']}\" if sample['Transcript'] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n\"\n",
    "\n",
    "    # Create a list of prompt template elements\n",
    "    parts = [part for part in [blurb, instruction, input_context, response] if part]\n",
    "\n",
    "    # Join prompt template elements into a single string to create the prompt template\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "\n",
    "    # Store the formatted prompt template in a new key “text\"\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts: 48\n",
      "Column names are: ['Data', 'Participant', 'Age', 'Gender', 'Diagnosis', 'Category', 'mmse', 'Filename', 'Transcript']\n"
     ]
    }
   ],
   "source": [
    "# The instruction dataset to use\n",
    "dataset_name = [\"./Testing_db.csv\"]\n",
    "\n",
    "# Load dataset\n",
    "test_dataset = load_dataset(\"csv\", data_files = dataset_name, split='train')\n",
    "\n",
    "print(f'Number of prompts: {len(dataset)}')\n",
    "print(f'Column names are: {dataset.column_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = test_dataset.map(create_prompt_test)['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/myenv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# query = query['text']\n",
    "sequences = pipeline(\n",
    "    queries,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=10,\n",
    "    early_stopping=True,\n",
    "    # do_sample=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n &uh this boy is about to fall off o(f) the stool . the mother is &uh washing dishes and the water's spilling over on. the wind is blowing the curtains . the little girl is laughing at her brother who's taking a cookie. think that's it .\\n\\n### Response:\\nhealthy\\n\\n### End Response:\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n &uh a boy with a cookie in his one hand and his hand in the cookie. standing on a stool which is tipping over . the little girl's got her hand up for one . and I don't know what the hand means to the mouth . does it mean she wants to eat ? the kitchen sink is running over, the water running out on the. the mother's drying the dishes, frowning but not turning off the. &uh (. the mother's standing in the middle o(f) the water . curtains at the windows . the lid's fallin(g) off the cookie jar . I don't know what else is +//. I could see their cupboard's half open . that's it .\\n\\n### Response:\\nalzheimers\\n\\n### End Response\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n I see &uh two kids up at the cookie jar, one on a stool the other. cupboard door is opened . mother's washing the dishes . the water is running [//] overflowing the sink . and &uh there's two cups and a plate on the counter . and she's <washing> [//] holding a plate in her hand . curtains at the windows . the cookie jar has the lid off . (. cupboards underneath the sink . cupboards underneath the other cupboards . &uh kid falling off the stool . the girl laughing at him . cookies in the cookie jar with the lid off . he has a cookie in his hand . and that's it .\\n\\n### Response:\\nhealthy\\n\\n### End Response:\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n okay the water's running out_of the sink overflowing . the mother's doing the dishes . &uh the boy is falling off o(f) the stool stealing the cookies out. the young lady is holding her hand up ready to receive one o(f) the. &uh there is a cup and a saucer on the &uh &uh sink board . and &uh there's <the &uh drapes on the kitchen> [//] &uh the. and the top cabinets, the door is open and the cookie jar is in the. the boy is up there reaching for it and it looks like he's gonna. and outside is a nice garden with the path leading around the house. she has a dish towel in her hand &wa doing [//] drying one o(f) the. and she has &uh her hairdo . she's wearing an apron . and the kids were wearing shoes . and &sh she's wearing shoes . and the water's dripping on the floor . what else you wanna know ?\\n\\n### Response:\\nalzheimers\\n\\n### End Response\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n well he's into the cookie jar and the [/] the &uh stool is falling. she's doin(g) dishes and the sink's runnin(g) over and wettin(g). and she look [//] doesn't look too happy . anything else ? she's drying the dishes . right ? and the kid's in the cookie jars . how many things are sposta be wrong xxx ?\\n\\n### Response:\\nhealthy\\n\\n### End Response:\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n yeah I see the woman's in a kitchen . and +/. +, now it looks like she's +. I [/] &me I [/] &c I can't really pick it out but +. oh and there's a gɪɾəl@u [: little] [* p:n] girl here talking and a. and this is a highchair [: stool] [* s:r] here (. and &uh (. I can't see what that is . (. did I talk about this girl up here ? she's &uh +. I can't see too plain what she's doing . oh yes I think so . (..) where was she xxx ? this girl ? (. (. yeah, that's awfully hard for me to distinguish .\\n\\n### Response:\\nalzheimers\\n\\n### End Response\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n oh it's the same picture . you should give me a different one . why &=laughs ? it would be more fun if you had some variety . okay . we have here a scene in a kitchen and it looks as though that's a. and the two children are hoping that mama will not notice because. so the boy climbs up on a stool to reach the cookie jar . and as he's getting up there the [/] &s <the tool [: stool]. and the little girl is telling him to be quiet by putting her. he has one cookie in his hand, is reaching for another one . the lid is off the cookie jar . the cupboard door is open . he's supposedly going to hand one cookie, it looks as though he is,. <on the other> [//] around the corner of the kitchen &uh &um. there are cupboards below the counter and above the counter where. then when you get over and have turned a corner in the kitchen you. there are things growing out there . there's a walk . there's &uh &uh some shrubbery underneath another window that looks. there's a tree beyond . there are tie back curtains at the window . the mother is working at the sink and the faucet is left on and. and &uh she has on a sleeveless dress which also indicates that. and it looks though she has an apron on . she has short hair . &um she is drying <a dish with a tea towel> [//] a plate with a tea. &um beside the sink there are two cups and a plate . looks as though there's some suds and some dishes in the sink . and two faucets with x@l type handles one on either side and a. she has shoes on with no ties, just sort of slip+on shoes . there's a little scallop valance across the top of the curtains . &hm (. well ‡ the handles on the door are the kind that are just &um. and what's really strange is that you can't see any handles on the.\\n\\n### Response:\\nhealthy\\n\\n### End of Response\"}],\n",
       " [{'generated_text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with \\'healthy\\' or \\'alzheimers\\' according to the patients diagnosis.\\n\\nInput:\\n the pearl [: poor] [* p:w] &mo mom\\'s gettin(g) her wet [//] feet. (. (. unless that\\'s the way all silly girls act at that age . pardon ? +< oh sure . there\\'s no handles on the [/] &uh the cabinet drawers under the. (. (. maybe they build them that way xxx . no, but <I did> [/] &n I did notice that\\'s a_little I wouldn\\'t say. pardon ? I said that the sink was overflowing . the mother\\'s wet [//] gettin(g) her feet wet . the boy\\'s going to fall very hard and hurt himself . and nobody\\'s gonna help him . well happening +. did I say the cabinet doors where the kids are have no handles on. (. responsible ? well I\\'ll tell you the way mom is lookin(g) there\\'s not much. she\\'s thinking of dad . and the little girl is saying +\"/. +\" sh@o don\\'t tell mom . the conflict\\'s in <whereas the> [//] what she\\'s thinkin(g) whereas. (.) and did I say +..? xxx . the cabinet doors <aren\\'t the same> [//] are different sizes . xxx have no handles . I guess you take a big sledge hammer to that .\\n\\n### Response:\\nalzheimers\\n\\n### End of'}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n well the [/] &uh the little boy is (. he's &uh partially gonna fall off the stool . his sister is tellin(g) him to be quiet . the mother is washing dishes . an(d) &uh the water is overflowing onto the floor and she's.\\n\\n### Response:\\nalzheimers\\n\\n### End Response\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n well the sink is running over . she's drying the dishes . they're getting in the cookie jar and they're upsetting the stool . the girl is reaching for a cookie . the lady here's standing right in the water . she seems to be looking out the window at the lawn . that's about it right +/.\\n\\n### Response:\\nhealthy\\n\\n### End Response:\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n I &s see this woman who's standin(g) by the sink washin(g) dishes. and I see <a little> [//] this little boy standing on a stool. (.\\n\\n### Response:\\nalzheimers\\n\\n### End Inst\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n it's a family affair, a mother and the children . and &uh they're each busy at [/] at &uh something . and the [//] &um (. (. (. I think I +.\\n\\n### Response:\\nalzheimers\\n\\n### End of\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n well the little girl is reaching for a cookie . and the boy is on a stool which is falling over to get in the. the faucet is running and it's overflowing onto the floor because. the woman is drying a plate . she's also looking out the window . there's a couple of bowls and a plate on the &uh counter . curtains at the windows . there's a driveway or it looks like a driveway outside . and you can see another window and some bushes out there and the. and <I guess that's> [//] &pa looks like a <part of the garage>. and &uh &um the cookie jar has the lid off . and the boy has one hand in the cookie jar and one with a cookie in.\\n\\n### Response:\\nhealthy\\n\\n### End Response:\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n a little boy is gettin(g) himself some cookies out o(f) the jar . and the &s stool is turnin(g) over on him (. he's handing the little girl some cookies out_of the jar . is that it ? and [//] well that's in the first one . he's handing the little girl cookies out_of the cookie jar . and he's on a stool and it's turning over . and he's handing her some cookies . and in the second picture the little girl &uh looks as though she's. +< and &uh <oh let me think> [//] the sink is running over on the. she has the spigots on . a nice view from the window . +< <is that> [/] is that it ? that's it .\\n\\n### Response:\\nhealthy\\n\\n### End Response:\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n I see a little boy on a stool almost falling over, taking cookies. and the little girl is putting her finger to her mouth <that she's>. the &uh mother is washing dishes . she's drying the dishes and letting the water (. and then water is running over and she is standing <in the sink. &uh there's a window there she's looking at, at the grass and the. and the &uh &uh curtains seem to be shaking from the (. the dishes that she's through (. and the little girl's raising her hands for the little boy to hand. and he has one cookie in his hand and he's going after another one. &uh he's ready to hand her a cookie . mother is &uh holding a dish cloth that she's drying the dishes. she has a platter that she's drying . I don't see any other action .\\n\\n### Response:\\nhealthy\\n\\n### End Response:\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n &uh the sink's runnin(g) over . the water's goin(g) all over the floor . here <the boy> [//] the step+ladder [: step+stool] [* s:r] is. and she's begging for cookies the girl is . coming back to the sink let's see here . &aw mama's steppin(g) in the water . and I [/] &s &=clears:throat I said the sink was running over . she's dryin(g) dishes . wait a minute . what the devil is xxx ? (. +< what is that ? and (. that's a p@l p@l somethin(g) there . hm (. I don't see anything else there . she's steppin(g) in the water . the sink's runnin(g) over . (. &=coughs xxx so far xxx . and he's on a [/] a stool that's gonna fall over while they're. and there's a plate and two cups on the sink and she's got a plate. (.\\n\\n### Response:\\nalzheimers\\n\\n### End Response\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n whew &=laughs . &uh do I hafta use my [/] my [/] &uh my personal &uh description ? or &=laughs like &uh +. <the &c cookie jar> [//] stealing the &g &uh cookie out o(f) the. the [/] &t &uh the &tripar not [//] &t stool the &tr triple. &uh I wonder if she's puttin(g) her finger to her [/] her lips . and &uh is that she wants one to eat or she's &ha &h reachin(g) up. huh ? and then spillin(g) the water in the kitchen sink . she's dryin(g) the dishes . (. so <that would be the> [//] that's why it's overflowin(g) because. she's dryin(g) the dishes . and let's see what else &uh +. am I missin(g) somethin(g) ? you_know I'm pretty tired . (. xxx huh ?\\n\\n### Response:\\nhealthy\\n\\n### End of Response\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n cookie jar . &uh a lad standing on a stool teetering, grabbing for the cookies . sister I guess laughing at him . mother washing dishes . sink is overflowing . &um view of the yard and the kitchen window with its curtain . two cups and a dish remain . looks like they're dried . mother standing in the overflowed water . &um her <four vased> [//] two faced cabinets four doors . +< and a valance and the curtain . window's half open . and there's landscaping along the wall of <the &uh view from the. and the walk+way pictured from the window . water is running, overflowing . boy is holding a cookie in his left hand, grabbing for another one. the lid of the cookie jar is over [//] uff [: off] [* nk-ret] [//]. mother's wearing an apron drying dishes . okay .\\n\\n### Response:\\nhealthy\\n\\n### End Response:\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n a little girl is reaching for her brother to give her a cookie . the stool is falling over . the boy has taken the lid off the cookie jar, has a cookie in his. the mother is drying the dishes . the water is flowing [//] &o running and flowing out_of the sink . &um (. (. &uh the door is open . (. (. (.) okay ? +< I think that's it .\\n\\n### Response:\\nalzheimers\\n\\n### End Response\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n well <the table [: stool] [* s:r-ret]> [//] the seat [: stool]. the sink is running over . the girl's reaching for a cookie . the mom is dryin(g) a dish . &hm &hm cup and saucers there . (. I <didn't hit> [//] did this and then I did that . &hm well yeah here's some outside (. &uh a garden I guess .\\n\\n### Response:\\nhealthy\\n\\n### End Response:\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n well the boy is &=clears:throat taking cookies out_of the cookie. the girl is reachin(g) for a cookie . &uh the mother is looking out the window while the water runs over. she has wet feet standing in the water . that's the action that I see . well naturally the stool falls over the boy is going to fall on the.\\n\\n### Response:\\nalzheimers\\n\\n### End Response\"}],\n",
       " [{'generated_text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with \\'healthy\\' or \\'alzheimers\\' according to the patients diagnosis.\\n\\nInput:\\n oh yeah . kid\\'s climbin(g) up on the stool and reachin(g) up in the cupboard. they aren\\'t gonna knock things off . and &uh <the mother> [//] oh boy the water\\'s all spillin(g) out. she\\'s just looking at it like +\"/. +\" oh for goodness sakes . (. &uh &hm (. that\\'s a good way to break his neck . break his back I shoulda said . woman left her faucet running and it rolled over the floor . that\\'s bad enough for me to do . &um let me scan it a little bit more . &uh (...) oh it\\'s there more in time ? +< oh [/] oh . yeah .\\n\\n### Response:\\nhealthy\\n\\n### End Response:'}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n (. I'm going to hafta quit reading this because this [/] this is. getting my xxx . xxx . xxx . this [/] this string is not on this is it ? yes . they're reaching up for a cookie in cookie jar . and they're cooking a turkey dinner I guess . cooking a turkey dinner . she's doing the dishes . (. and the water's splashing out all over the floor and she doesn't. <and the boy might> [//] he's looking for the cookie jar and he.\\n\\n### Response:\\nhealthy\\n\\n### End Response:\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n &uh the boy is reaching into the cookie jar . he's falling off the stool . the little girl is reaching for a cookie . mother is drying the dishes . the sink is running over . mother's getting her feet wet . they all have shoes on . there's <a cup> [//] two cups and a saucer on the sink . the window has draw [//] withdrawn [: drawn] [* s:r] drapes . you look out on the driveway . there's kitchen cabinets . oh what's happening . mother is looking out the window . the girl is touching her lips . the boy is standing on his right foot . his left foot is sort_of up in the air . mother's right foot is flat on the floor and <her left> [//] she's. &uh she's holding the dish cloth in her right hand and the plate. I think I've run out of +/. +< yeah &=laughs .\\n\\n### Response:\\nalzheimers\\n\\n### End Response\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n young boy is up at the cookie jar . he has the cookie jar opened . he's reachin(g) for one for himself . he's handing one down to his sister . the three legged stool is coming down with him . &uh they're nicely dressed kids but &uh the boy needs his socks. &mm that's all I see there . mother [//] I presume it's mother is at the sink . the water is &r running over out_of sink . she's drying a dish . she's looking out the window . there are two cups and a dish, I presume that's a dish, that are. &uh nicely groomed . nice curtains pulled back . it's apparently summer outside because the window's open and it. you can see a window in the other part of the house . I presume that's still their house . and there's a driveway bordered with grass . and there's a tree out there somewhere that you can see part of the. and &uh I see cabinets around the kitchen, other cabinets . and that's about it .\\n\\n### Response:\\nalzheimers\\n\\n### End of\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n oh little boy's in the cookie jar . the girl's standin(g) down and waitin(g) for him to <give him. the mother's doin(g) the dishes . and she's also <letting him> [//] letting &hm +. oh the water just fell off [//] over on the floor . &uh can I turn the page ? okay . that's all there is except she's just gettin(g) all wet . no . the little girl was helpin(g) xxx the boy to get to the cookie jar. but he started stumblin(g) . he must ha(ve) fell over (be)cause he seems to be falling there . and the mother's over at the sink drying dishes . and it's goin(g) on the floor . and three cups [//] three bowls there .\\n\\n### Response:\\nhealthy\\n\\n### End Response:\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n mhm . &=sighs the little boy is reaching for a cookie . and he's falling at the same time because the stool has tipped over. his little sister is reaching for a cookie and I think beginning to. the sink is running over and dripping water on the floor while the. &um the window is open and the curtains are blowing in the breeze. the water is running in the sink . that's why it's overflowing . (. (. that's it . that's all I see .\\n\\n### Response:\\nhealthy\\n\\n### End Response:\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n hm +. it's a little boy climbin(g) up gettin(g) some &coo cookies out. and his little sister reachin(g) for some . and the little boy is standin(g) on a ladder [: stool] [* s:r] . an(d) his big sister washin(g) the dishes (. (. and I think she's &uh runnin(g) water . and I said <the &uh little sister's> [//] &uh &reach Johnny's [//]. +< <he he he> [//] he's passin(g) it down to her . and <the ladder [: stool] [* s:r-rep]> [/] the ladder [: stool]. the [//] &k &uh this [//] the cups <what she's> [//] &uh &sh maybe. and <maybe runnin(g)> [/] maybe runnin(g) water on the sink and. since dishes [//] the dishes stacked up . they might be on the sink . no that be about all xxx .\\n\\n### Response:\\nalzheimers\\n\\n### End Response\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n &hm (. <and he's stepping on a> [//] &s well he's on a stool but it's [//]. and the water's running out_of the sink . and &uh let's see . yes .\\n\\n### Response:\\nalzheimers\\n\\n### End of\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n (. &uh the little girl &s wants cookie . the boy is getting her one but <the &s stool is going to> [//] he's. it's tilty [: tilting] [* p:n] . &uh the mother is doing the dishes . and unfortunately the sink is overflowing on the floor &=laughs . it's wet . xxx . xxx . (. oh that's his feet . I thought there was something around the little boy's foot or <at. the little girl I [/] I don't whether she's laughing or asking for.\\n\\n### Response:\\nalzheimers\\n\\n### End Response\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n &um the girl's <in the> [//] (. (...) &hm did I get that ? and girl that &=clears:throat (.) is dryin(g) does dishes, right ? she's bʌʃɪŋ@u [: washing] [* n:k-ret] [//] gushin(g) [: washing]. and there's two boys . one [/] &uh one is get a &choc cookies [/] cookies jar (. and <the [/] the girl's cryin(g) for the> [//] &=laughs &s she. and the girl +. what's it always say that ? upset the water up the sink .\\n\\n### Response:\\nalzheimers\\n\\n### End Response\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n mhm . &hm well &sh (. it's [/] it's runnin(g) over rather . in the (. and in the meantime he's tiltin(g) his chair [: stool] [* s:r]. hm ? <he is> [//] he's [/] he's trying to get the cake [* s:uk] down. <and then> [//] <and the mother is> [//] &uh &lis the water is. and <his chair [: stool] [* s:r-ret] is slippin(g) out from> [//].\\n\\n### Response:\\nalzheimers\\n\\n### End Response\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n all the action ? (. (. (. it's goin(g) all the way down in there . &um they're gettin(g) somethin(g) to eat here . cookeiejar@k . and they're gettin(g) something to eat here . (. but they put that stuff around in there . <it looks> [//] &n it looks nice . and then here when they had some stuff in through here . and &uh +. I like these things in through here too . +< yeah .\\n\\n### Response:\\nhealthy\\n\\n### End Response:\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n okay the kid on the bench who's got his hand in the cookie jar and. &uh his mother is standing in a puddle of water because she didn't. she oughta dry her feet instead . &uh the window is open . &um &uh well the sink is overflowing . &uh it's obviously summer because the window as I said was open . there's &uh supposedly leaves on the trees . &uh anything else that I'm sposta pick up ? well the kid's gonna fall off . and it xxx . the lid is off the cookie jar . and he's got one in his hand and handing it to his sister . and one and he's sneaking another one . not sneaking . &uh the water is still running in the sink . and splashing on the floor .\\n\\n### Response:\\nhealthy\\n\\n### End Response:\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n well the mother has water spillin(g) all over the floor for one. then she's startin(g) to dry the dishes . and then she's looking out the window . and &uh the little girl's there and the little boy and he almost is. but he didn't so +. he [/] &d he survived anyway huh ? yeah [/] yeah on this paper yeah .\\n\\n### Response:\\nalzheimers\\n\\n### End Response\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n okay . well in the first place the [/] &m the mother forgot to turn off. and &sh she's standing there &=laughs . it's falling on the floor . the child is got a stool and reaching up into the cookie jar . and the [x 3] stool is tipping over . and he's sorta [: sort_of] put down the plates . and she's reaching up to get it but I don't see anything wrong with. yeah that's it . I can't see anything .\\n\\n### Response:\\nhealthy\\n\\n### End Response:\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n well let's see . the girl is whisperin(g) to be quiet (be)cause mother might find. and he's reachin(g) in a cookie jar and he has a cookie . and she's grabbing for the one that he has in his left hand . and the sink is running over with water for some reason or other. (. no .\\n\\n### Response:\\nalzheimers\\n\\n### End Response\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n (. (..) anywhere ? &um the little girl's laughing . the little boy's getting cookies and he's falling . &uh I guess the mother would be doing dishes and the water's. &uh the window's open and I would say the wind is blowing . I guess that's it .\\n\\n### Response:\\nalzheimers\\n\\n### End Response\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n (. little boy is gettin(g) in the cookie jar . the s:tool is falling over . little girl wants one . hmhunh no . the dishes sittin(g) on the sink . and the window . the grass and stuff outside . (. I don't know where everything +. I don't see anything else on that .\\n\\n### Response:\\nalzheimers\\n\\n### End Response\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n there's &um the two children are in the process o(f) stealing. and the little girl has her finger up to her mouth to be quiet . and the stool on which the boy is standing is about to fall . and the mother must be lookin(g) out the window because the sink is. and the [//] meanwhile she's dryin(g) the dishes . an(d) the water is fallin(g) down onto the floor . and it's a nice day . the window's up it looks like . and it's a nice day outside . that's about all I can see on that .\\n\\n### Response:\\nhealthy\\n\\n### End Response:\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n (. she's washing dishes and the water's runnin(g) all over the floor . and the boy he's reaching up for cookies . and he [//] his ladder [: stool] [* s:r] slipped from underneath. and I guess he's gonna have an accident . and the little girl she's waiting for the cookies, but I don't.\\n\\n### Response:\\nalzheimers\\n\\n### End Response\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n well the boy is getting cookies and he's gonna fall off the stool . and the mother is doing dishes . and the water is running over out_of the sink . she has the faucet on . she's spilling water all over the floor . well the boy is handing the little girl a cookie . &uh nothing that I can see of action .\\n\\n### Response:\\nhealthy\\n\\n### End Response:\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n (. tell me . oh . +< oh by lookin(g) at the +. +< okay <the boy> [/] the boy is standing on a &ch chair [: stool]. he's &uh taking cookies out_of the cookie [/] cookie jar . the girl is like snickering at what's happening to him in a sense . so she's reaching up to take a cookie from his hand . &uh the woman the mother is drying a dish . and the sink is <flooding over> [//] &uh overflowing . and there's water coming out_of the spigot . and there's well &y actually happening or (..) &al +/? well that's [/] that's all that's in a way of movement but there's.\\n\\n### Response:\\nhealthy\\n\\n### End Response:\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n okay the boy's in the cookie jar . he's [/] (. the mother's (. kid's gonna hit on the floor &=laughs . where's the girl at there ? an(d) she's [/] she reachin(g) for &uh a cookie . (. that's about it, huh ?\\n\\n### Response:\\nhealthy\\n\\n### End Response:\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n &=tapping well this happens to be a kitchen . and the mother of the house is washing the dishes . and the water is overflowing from the sink onto the floor . and the &uh two children evidently a brother and a sister are &uh. and &uh he's standing on a three legged stool and is losing its.\\n\\n### Response:\\nalzheimers\\n\\n### End Response\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n well the little kid's falling off his chair [: stool] [* s:r] . and the mother is having water run over the &uh sink . well the water's running on the floor +/. +, under her feet . I'm looking outside but that yard is okay . the windows are open . the little girl is laughing at the boy falling off the chair. that [//] that's bad &=laughs .\\n\\n### Response:\\nalzheimers\\n\\n### End Response\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n well for one thing the boy is stealing cookies . and the girl is asking for a cookie . and the boy is standing on a (. and the lady's drying a plate . and the (. and the window is open . and (. seems to me <that> [/] &uh that's essentially the things that are. the girl is picking her nose . the boy is handing her a cookie . (.\\n\\n### Response:\\nalzheimers\\n\\n### End Response\"}],\n",
       " [{'generated_text': \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nThe input is a transcription of a patient who could have the alzheimers disease. Based on the transcription respond with 'healthy' or 'alzheimers' according to the patients diagnosis.\\n\\nInput:\\n little girl with her finger to her lips . the boy on the stool . stool tipping over . getting cookies out_of the cookie jar . &uh mother washing dishes . water running . sink overflowing . xxx those curtains are blowing or not . (.\\n\\n### Response:\\nalzheimers\\n\\n### End Inst\"}]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['healthy', 'alzheimers', 'healthy', 'alzheimers', 'healthy', 'alzheimers', 'healthy', 'alzheimers', 'alzheimers', 'healthy', 'alzheimers', 'alzheimers', 'healthy', 'healthy', 'healthy', 'alzheimers', 'healthy', 'healthy', 'alzheimers', 'healthy', 'alzheimers', 'healthy', 'healthy', 'alzheimers', 'alzheimers', 'healthy', 'healthy', 'alzheimers', 'alzheimers', 'alzheimers', 'alzheimers', 'alzheimers', 'healthy', 'healthy', 'alzheimers', 'healthy', 'alzheimers', 'alzheimers', 'alzheimers', 'healthy', 'alzheimers', 'healthy', 'healthy', 'healthy', 'alzheimers', 'alzheimers', 'alzheimers', 'alzheimers']\n"
     ]
    }
   ],
   "source": [
    "def extract_responses(sequences):\n",
    "    responses = []\n",
    "    for sequence in sequences:\n",
    "        for item in sequence:\n",
    "            # Split the text to find the part after \"### Response:\\n\"\n",
    "            parts = item['generated_text'].split(\"### Response:\\n\")\n",
    "            if len(parts) > 1:\n",
    "                # Further split to isolate the response before \"\\n\\n### End\"\n",
    "                response_part = parts[1].split(\"\\n\\n### End\")[0]\n",
    "                responses.append(response_part.strip())\n",
    "    return responses\n",
    "\n",
    "responses = extract_responses(sequences)\n",
    "print(responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: \"healthy\", 1: \"alzheimers\"}\n",
    "maps = [(label_map[label], result) for label, result in zip(test_dataset['Category'], responses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(data):\n",
    "    correct_predictions = sum(1 for actual, predicted in data if actual == predicted)\n",
    "    total_predictions = len(data)\n",
    "    accuracy = (correct_predictions / total_predictions) * 100  # accuracy as a percentage\n",
    "    return correct_predictions, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 50.0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_accuracy(maps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
