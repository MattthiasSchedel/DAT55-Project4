{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 37.03703703703704,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.037037037037037035,
      "grad_norm": 3.2600209712982178,
      "learning_rate": 0.0001,
      "loss": 4.2035,
      "step": 1
    },
    {
      "epoch": 0.07407407407407407,
      "grad_norm": 3.2333619594573975,
      "learning_rate": 0.0002,
      "loss": 4.5528,
      "step": 2
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 2.380991220474243,
      "learning_rate": 0.0001997995991983968,
      "loss": 4.0357,
      "step": 3
    },
    {
      "epoch": 0.14814814814814814,
      "grad_norm": 2.765772819519043,
      "learning_rate": 0.0001995991983967936,
      "loss": 3.6696,
      "step": 4
    },
    {
      "epoch": 0.18518518518518517,
      "grad_norm": 2.5421836376190186,
      "learning_rate": 0.0001993987975951904,
      "loss": 3.0501,
      "step": 5
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 2.6310431957244873,
      "learning_rate": 0.00019919839679358718,
      "loss": 2.3289,
      "step": 6
    },
    {
      "epoch": 0.25925925925925924,
      "grad_norm": 4.456593990325928,
      "learning_rate": 0.00019899799599198398,
      "loss": 2.5925,
      "step": 7
    },
    {
      "epoch": 0.2962962962962963,
      "grad_norm": 2.8797264099121094,
      "learning_rate": 0.00019879759519038077,
      "loss": 1.8522,
      "step": 8
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 1.8992916345596313,
      "learning_rate": 0.00019859719438877757,
      "loss": 2.0924,
      "step": 9
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 2.644618272781372,
      "learning_rate": 0.00019839679358717434,
      "loss": 1.6053,
      "step": 10
    },
    {
      "epoch": 0.4074074074074074,
      "grad_norm": 3.0858139991760254,
      "learning_rate": 0.00019819639278557117,
      "loss": 1.6746,
      "step": 11
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 4.56724739074707,
      "learning_rate": 0.00019799599198396794,
      "loss": 1.8159,
      "step": 12
    },
    {
      "epoch": 0.48148148148148145,
      "grad_norm": 3.72298526763916,
      "learning_rate": 0.00019779559118236474,
      "loss": 1.4988,
      "step": 13
    },
    {
      "epoch": 0.5185185185185185,
      "grad_norm": 1.5782835483551025,
      "learning_rate": 0.00019759519038076154,
      "loss": 1.8296,
      "step": 14
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 1.4014689922332764,
      "learning_rate": 0.00019739478957915834,
      "loss": 1.3705,
      "step": 15
    },
    {
      "epoch": 0.5925925925925926,
      "grad_norm": 1.4834474325180054,
      "learning_rate": 0.0001971943887775551,
      "loss": 1.4725,
      "step": 16
    },
    {
      "epoch": 0.6296296296296297,
      "grad_norm": 1.2691401243209839,
      "learning_rate": 0.00019699398797595193,
      "loss": 1.47,
      "step": 17
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 1.3222731351852417,
      "learning_rate": 0.0001967935871743487,
      "loss": 1.4334,
      "step": 18
    },
    {
      "epoch": 0.7037037037037037,
      "grad_norm": 1.7213683128356934,
      "learning_rate": 0.0001965931863727455,
      "loss": 1.2113,
      "step": 19
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 1.3776555061340332,
      "learning_rate": 0.0001963927855711423,
      "loss": 1.462,
      "step": 20
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 1.3702009916305542,
      "learning_rate": 0.0001961923847695391,
      "loss": 1.6785,
      "step": 21
    },
    {
      "epoch": 0.8148148148148148,
      "grad_norm": 1.2096178531646729,
      "learning_rate": 0.00019599198396793587,
      "loss": 1.0526,
      "step": 22
    },
    {
      "epoch": 0.8518518518518519,
      "grad_norm": 1.3369700908660889,
      "learning_rate": 0.00019579158316633267,
      "loss": 1.4668,
      "step": 23
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.2637170553207397,
      "learning_rate": 0.00019559118236472947,
      "loss": 1.1368,
      "step": 24
    },
    {
      "epoch": 0.9259259259259259,
      "grad_norm": 1.4016330242156982,
      "learning_rate": 0.00019539078156312627,
      "loss": 1.4648,
      "step": 25
    },
    {
      "epoch": 0.9629629629629629,
      "grad_norm": 1.4903804063796997,
      "learning_rate": 0.00019519038076152304,
      "loss": 1.5073,
      "step": 26
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.3637804985046387,
      "learning_rate": 0.00019498997995991986,
      "loss": 1.5589,
      "step": 27
    },
    {
      "epoch": 1.037037037037037,
      "grad_norm": 1.1243873834609985,
      "learning_rate": 0.00019478957915831664,
      "loss": 1.191,
      "step": 28
    },
    {
      "epoch": 1.074074074074074,
      "grad_norm": 1.115077018737793,
      "learning_rate": 0.00019458917835671343,
      "loss": 0.9932,
      "step": 29
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 1.4155547618865967,
      "learning_rate": 0.00019438877755511023,
      "loss": 1.0679,
      "step": 30
    },
    {
      "epoch": 1.1481481481481481,
      "grad_norm": 1.1026086807250977,
      "learning_rate": 0.00019418837675350703,
      "loss": 1.0592,
      "step": 31
    },
    {
      "epoch": 1.1851851851851851,
      "grad_norm": 1.2125457525253296,
      "learning_rate": 0.0001939879759519038,
      "loss": 1.1141,
      "step": 32
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 1.401923656463623,
      "learning_rate": 0.0001937875751503006,
      "loss": 1.1795,
      "step": 33
    },
    {
      "epoch": 1.2592592592592593,
      "grad_norm": 1.8125901222229004,
      "learning_rate": 0.0001935871743486974,
      "loss": 1.3733,
      "step": 34
    },
    {
      "epoch": 1.2962962962962963,
      "grad_norm": 1.4012309312820435,
      "learning_rate": 0.0001933867735470942,
      "loss": 1.394,
      "step": 35
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 1.1758899688720703,
      "learning_rate": 0.000193186372745491,
      "loss": 1.0333,
      "step": 36
    },
    {
      "epoch": 1.3703703703703702,
      "grad_norm": 1.2705086469650269,
      "learning_rate": 0.0001929859719438878,
      "loss": 1.4512,
      "step": 37
    },
    {
      "epoch": 1.4074074074074074,
      "grad_norm": 1.15554678440094,
      "learning_rate": 0.00019278557114228457,
      "loss": 1.1658,
      "step": 38
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 1.256921648979187,
      "learning_rate": 0.00019258517034068137,
      "loss": 1.1827,
      "step": 39
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 1.0442330837249756,
      "learning_rate": 0.00019238476953907816,
      "loss": 1.0369,
      "step": 40
    },
    {
      "epoch": 1.5185185185185186,
      "grad_norm": 1.0422521829605103,
      "learning_rate": 0.00019218436873747496,
      "loss": 0.9085,
      "step": 41
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 1.2278326749801636,
      "learning_rate": 0.00019198396793587173,
      "loss": 1.1097,
      "step": 42
    },
    {
      "epoch": 1.5925925925925926,
      "grad_norm": 1.07001531124115,
      "learning_rate": 0.00019178356713426856,
      "loss": 0.9633,
      "step": 43
    },
    {
      "epoch": 1.6296296296296298,
      "grad_norm": 1.380233645439148,
      "learning_rate": 0.00019158316633266533,
      "loss": 0.9401,
      "step": 44
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 1.2931700944900513,
      "learning_rate": 0.00019138276553106213,
      "loss": 1.4351,
      "step": 45
    },
    {
      "epoch": 1.7037037037037037,
      "grad_norm": 1.2116975784301758,
      "learning_rate": 0.00019118236472945893,
      "loss": 0.9442,
      "step": 46
    },
    {
      "epoch": 1.7407407407407407,
      "grad_norm": 1.2408857345581055,
      "learning_rate": 0.00019098196392785573,
      "loss": 1.2328,
      "step": 47
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 1.0986156463623047,
      "learning_rate": 0.0001907815631262525,
      "loss": 1.2582,
      "step": 48
    },
    {
      "epoch": 1.8148148148148149,
      "grad_norm": 1.1385661363601685,
      "learning_rate": 0.0001905811623246493,
      "loss": 1.1774,
      "step": 49
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 1.4167741537094116,
      "learning_rate": 0.0001903807615230461,
      "loss": 1.1366,
      "step": 50
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 1.219950556755066,
      "learning_rate": 0.0001901803607214429,
      "loss": 1.2133,
      "step": 51
    },
    {
      "epoch": 1.925925925925926,
      "grad_norm": 1.262392282485962,
      "learning_rate": 0.00018997995991983967,
      "loss": 0.9796,
      "step": 52
    },
    {
      "epoch": 1.9629629629629628,
      "grad_norm": 1.2027333974838257,
      "learning_rate": 0.0001897795591182365,
      "loss": 1.2318,
      "step": 53
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.1816511154174805,
      "learning_rate": 0.00018957915831663326,
      "loss": 1.4023,
      "step": 54
    },
    {
      "epoch": 2.037037037037037,
      "grad_norm": 1.2705891132354736,
      "learning_rate": 0.00018937875751503006,
      "loss": 1.0257,
      "step": 55
    },
    {
      "epoch": 2.074074074074074,
      "grad_norm": 1.1032556295394897,
      "learning_rate": 0.00018917835671342686,
      "loss": 0.807,
      "step": 56
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 1.2496329545974731,
      "learning_rate": 0.00018897795591182366,
      "loss": 0.7872,
      "step": 57
    },
    {
      "epoch": 2.148148148148148,
      "grad_norm": 1.359811782836914,
      "learning_rate": 0.00018877755511022046,
      "loss": 1.0112,
      "step": 58
    },
    {
      "epoch": 2.185185185185185,
      "grad_norm": 1.374604344367981,
      "learning_rate": 0.00018857715430861726,
      "loss": 1.0188,
      "step": 59
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 1.3402553796768188,
      "learning_rate": 0.00018837675350701405,
      "loss": 0.9896,
      "step": 60
    },
    {
      "epoch": 2.259259259259259,
      "grad_norm": 1.2919420003890991,
      "learning_rate": 0.00018817635270541083,
      "loss": 0.7265,
      "step": 61
    },
    {
      "epoch": 2.2962962962962963,
      "grad_norm": 1.650044560432434,
      "learning_rate": 0.00018797595190380762,
      "loss": 0.9699,
      "step": 62
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 1.359078049659729,
      "learning_rate": 0.00018777555110220442,
      "loss": 0.6941,
      "step": 63
    },
    {
      "epoch": 2.3703703703703702,
      "grad_norm": 1.4954184293746948,
      "learning_rate": 0.00018757515030060122,
      "loss": 0.887,
      "step": 64
    },
    {
      "epoch": 2.4074074074074074,
      "grad_norm": 1.3558403253555298,
      "learning_rate": 0.000187374749498998,
      "loss": 0.8461,
      "step": 65
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 1.3271030187606812,
      "learning_rate": 0.00018717434869739482,
      "loss": 0.8737,
      "step": 66
    },
    {
      "epoch": 2.4814814814814814,
      "grad_norm": 1.2492914199829102,
      "learning_rate": 0.0001869739478957916,
      "loss": 0.6847,
      "step": 67
    },
    {
      "epoch": 2.5185185185185186,
      "grad_norm": 1.5184757709503174,
      "learning_rate": 0.0001867735470941884,
      "loss": 0.766,
      "step": 68
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 1.7182962894439697,
      "learning_rate": 0.0001865731462925852,
      "loss": 0.9644,
      "step": 69
    },
    {
      "epoch": 2.5925925925925926,
      "grad_norm": 1.5412291288375854,
      "learning_rate": 0.00018637274549098199,
      "loss": 0.9857,
      "step": 70
    },
    {
      "epoch": 2.6296296296296298,
      "grad_norm": 1.7022440433502197,
      "learning_rate": 0.00018617234468937876,
      "loss": 1.1581,
      "step": 71
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 1.5433094501495361,
      "learning_rate": 0.00018597194388777556,
      "loss": 0.8917,
      "step": 72
    },
    {
      "epoch": 2.7037037037037037,
      "grad_norm": 1.6329478025436401,
      "learning_rate": 0.00018577154308617235,
      "loss": 0.6838,
      "step": 73
    },
    {
      "epoch": 2.7407407407407405,
      "grad_norm": 1.5379884243011475,
      "learning_rate": 0.00018557114228456915,
      "loss": 0.7569,
      "step": 74
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 1.3653799295425415,
      "learning_rate": 0.00018537074148296592,
      "loss": 0.8181,
      "step": 75
    },
    {
      "epoch": 2.814814814814815,
      "grad_norm": 1.3928877115249634,
      "learning_rate": 0.00018517034068136275,
      "loss": 0.8491,
      "step": 76
    },
    {
      "epoch": 2.851851851851852,
      "grad_norm": 1.3978042602539062,
      "learning_rate": 0.00018496993987975952,
      "loss": 0.9942,
      "step": 77
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 1.2704546451568604,
      "learning_rate": 0.00018476953907815632,
      "loss": 0.7612,
      "step": 78
    },
    {
      "epoch": 2.925925925925926,
      "grad_norm": 1.2398830652236938,
      "learning_rate": 0.00018456913827655312,
      "loss": 0.7415,
      "step": 79
    },
    {
      "epoch": 2.962962962962963,
      "grad_norm": 1.5060638189315796,
      "learning_rate": 0.00018436873747494992,
      "loss": 0.7809,
      "step": 80
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.2785682678222656,
      "learning_rate": 0.0001841683366733467,
      "loss": 0.8897,
      "step": 81
    },
    {
      "epoch": 3.037037037037037,
      "grad_norm": 1.5882500410079956,
      "learning_rate": 0.00018396793587174351,
      "loss": 0.5659,
      "step": 82
    },
    {
      "epoch": 3.074074074074074,
      "grad_norm": 1.4899312257766724,
      "learning_rate": 0.00018376753507014029,
      "loss": 0.6022,
      "step": 83
    },
    {
      "epoch": 3.111111111111111,
      "grad_norm": 1.6911715269088745,
      "learning_rate": 0.00018356713426853708,
      "loss": 0.9682,
      "step": 84
    },
    {
      "epoch": 3.148148148148148,
      "grad_norm": 1.6938618421554565,
      "learning_rate": 0.00018336673346693388,
      "loss": 0.484,
      "step": 85
    },
    {
      "epoch": 3.185185185185185,
      "grad_norm": 2.0903923511505127,
      "learning_rate": 0.00018316633266533068,
      "loss": 0.5776,
      "step": 86
    },
    {
      "epoch": 3.2222222222222223,
      "grad_norm": 2.3108112812042236,
      "learning_rate": 0.00018296593186372745,
      "loss": 0.5499,
      "step": 87
    },
    {
      "epoch": 3.259259259259259,
      "grad_norm": 2.824068307876587,
      "learning_rate": 0.00018276553106212425,
      "loss": 0.6522,
      "step": 88
    },
    {
      "epoch": 3.2962962962962963,
      "grad_norm": 2.093212127685547,
      "learning_rate": 0.00018256513026052105,
      "loss": 0.5688,
      "step": 89
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 2.0325851440429688,
      "learning_rate": 0.00018236472945891785,
      "loss": 0.8459,
      "step": 90
    },
    {
      "epoch": 3.3703703703703702,
      "grad_norm": 1.5132871866226196,
      "learning_rate": 0.00018216432865731462,
      "loss": 0.4202,
      "step": 91
    },
    {
      "epoch": 3.4074074074074074,
      "grad_norm": 1.4377998113632202,
      "learning_rate": 0.00018196392785571145,
      "loss": 0.381,
      "step": 92
    },
    {
      "epoch": 3.4444444444444446,
      "grad_norm": 1.6881704330444336,
      "learning_rate": 0.00018176352705410822,
      "loss": 0.6236,
      "step": 93
    },
    {
      "epoch": 3.4814814814814814,
      "grad_norm": 1.4380109310150146,
      "learning_rate": 0.00018156312625250502,
      "loss": 0.3422,
      "step": 94
    },
    {
      "epoch": 3.5185185185185186,
      "grad_norm": 1.392889142036438,
      "learning_rate": 0.00018136272545090181,
      "loss": 0.3666,
      "step": 95
    },
    {
      "epoch": 3.5555555555555554,
      "grad_norm": 1.4576659202575684,
      "learning_rate": 0.0001811623246492986,
      "loss": 0.3403,
      "step": 96
    },
    {
      "epoch": 3.5925925925925926,
      "grad_norm": 1.8847644329071045,
      "learning_rate": 0.00018096192384769538,
      "loss": 0.6671,
      "step": 97
    },
    {
      "epoch": 3.6296296296296298,
      "grad_norm": 1.9853122234344482,
      "learning_rate": 0.0001807615230460922,
      "loss": 0.8104,
      "step": 98
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 2.1643080711364746,
      "learning_rate": 0.00018056112224448898,
      "loss": 0.7433,
      "step": 99
    },
    {
      "epoch": 3.7037037037037037,
      "grad_norm": 2.091169595718384,
      "learning_rate": 0.00018036072144288578,
      "loss": 0.5675,
      "step": 100
    },
    {
      "epoch": 3.7407407407407405,
      "grad_norm": 1.9354125261306763,
      "learning_rate": 0.00018016032064128258,
      "loss": 0.4769,
      "step": 101
    },
    {
      "epoch": 3.7777777777777777,
      "grad_norm": 1.7266980409622192,
      "learning_rate": 0.00017995991983967938,
      "loss": 0.4234,
      "step": 102
    },
    {
      "epoch": 3.814814814814815,
      "grad_norm": 1.7699120044708252,
      "learning_rate": 0.00017975951903807615,
      "loss": 0.4671,
      "step": 103
    },
    {
      "epoch": 3.851851851851852,
      "grad_norm": 1.736796259880066,
      "learning_rate": 0.00017955911823647295,
      "loss": 0.5563,
      "step": 104
    },
    {
      "epoch": 3.888888888888889,
      "grad_norm": 1.7573307752609253,
      "learning_rate": 0.00017935871743486975,
      "loss": 0.5731,
      "step": 105
    },
    {
      "epoch": 3.925925925925926,
      "grad_norm": 1.7645262479782104,
      "learning_rate": 0.00017915831663326654,
      "loss": 0.6842,
      "step": 106
    },
    {
      "epoch": 3.962962962962963,
      "grad_norm": 1.7148582935333252,
      "learning_rate": 0.00017895791583166331,
      "loss": 0.385,
      "step": 107
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.7204468250274658,
      "learning_rate": 0.00017875751503006014,
      "loss": 0.7006,
      "step": 108
    },
    {
      "epoch": 4.037037037037037,
      "grad_norm": 1.4926263093948364,
      "learning_rate": 0.0001785571142284569,
      "loss": 0.375,
      "step": 109
    },
    {
      "epoch": 4.074074074074074,
      "grad_norm": 1.3980250358581543,
      "learning_rate": 0.0001783567134268537,
      "loss": 0.5166,
      "step": 110
    },
    {
      "epoch": 4.111111111111111,
      "grad_norm": 1.6604615449905396,
      "learning_rate": 0.0001781563126252505,
      "loss": 0.2993,
      "step": 111
    },
    {
      "epoch": 4.148148148148148,
      "grad_norm": 1.6195340156555176,
      "learning_rate": 0.0001779559118236473,
      "loss": 0.2258,
      "step": 112
    },
    {
      "epoch": 4.185185185185185,
      "grad_norm": 1.7794873714447021,
      "learning_rate": 0.0001777555110220441,
      "loss": 0.2659,
      "step": 113
    },
    {
      "epoch": 4.222222222222222,
      "grad_norm": 2.164018154144287,
      "learning_rate": 0.00017755511022044088,
      "loss": 0.377,
      "step": 114
    },
    {
      "epoch": 4.2592592592592595,
      "grad_norm": 2.1146342754364014,
      "learning_rate": 0.0001773547094188377,
      "loss": 0.6284,
      "step": 115
    },
    {
      "epoch": 4.296296296296296,
      "grad_norm": 2.3617093563079834,
      "learning_rate": 0.00017715430861723447,
      "loss": 0.3763,
      "step": 116
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 2.033402681350708,
      "learning_rate": 0.00017695390781563127,
      "loss": 0.275,
      "step": 117
    },
    {
      "epoch": 4.37037037037037,
      "grad_norm": 2.2245230674743652,
      "learning_rate": 0.00017675350701402807,
      "loss": 0.3569,
      "step": 118
    },
    {
      "epoch": 4.407407407407407,
      "grad_norm": 1.782441258430481,
      "learning_rate": 0.00017655310621242487,
      "loss": 0.281,
      "step": 119
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 1.7154085636138916,
      "learning_rate": 0.00017635270541082164,
      "loss": 0.3016,
      "step": 120
    },
    {
      "epoch": 4.481481481481482,
      "grad_norm": 1.800824761390686,
      "learning_rate": 0.00017615230460921847,
      "loss": 0.2578,
      "step": 121
    },
    {
      "epoch": 4.518518518518518,
      "grad_norm": 2.191301107406616,
      "learning_rate": 0.00017595190380761524,
      "loss": 0.3216,
      "step": 122
    },
    {
      "epoch": 4.555555555555555,
      "grad_norm": 1.4987871646881104,
      "learning_rate": 0.00017575150300601204,
      "loss": 0.2096,
      "step": 123
    },
    {
      "epoch": 4.592592592592593,
      "grad_norm": 1.7957292795181274,
      "learning_rate": 0.00017555110220440884,
      "loss": 0.4464,
      "step": 124
    },
    {
      "epoch": 4.62962962962963,
      "grad_norm": 1.6755832433700562,
      "learning_rate": 0.00017535070140280563,
      "loss": 0.3581,
      "step": 125
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 1.6955136060714722,
      "learning_rate": 0.0001751503006012024,
      "loss": 0.2817,
      "step": 126
    },
    {
      "epoch": 4.703703703703704,
      "grad_norm": 1.714913010597229,
      "learning_rate": 0.0001749498997995992,
      "loss": 0.3918,
      "step": 127
    },
    {
      "epoch": 4.7407407407407405,
      "grad_norm": 1.9383610486984253,
      "learning_rate": 0.000174749498997996,
      "loss": 0.3387,
      "step": 128
    },
    {
      "epoch": 4.777777777777778,
      "grad_norm": 1.7921701669692993,
      "learning_rate": 0.0001745490981963928,
      "loss": 0.2669,
      "step": 129
    },
    {
      "epoch": 4.814814814814815,
      "grad_norm": 1.7002424001693726,
      "learning_rate": 0.00017434869739478957,
      "loss": 0.1772,
      "step": 130
    },
    {
      "epoch": 4.851851851851852,
      "grad_norm": 1.924132227897644,
      "learning_rate": 0.0001741482965931864,
      "loss": 0.2409,
      "step": 131
    },
    {
      "epoch": 4.888888888888889,
      "grad_norm": 2.004477024078369,
      "learning_rate": 0.00017394789579158317,
      "loss": 0.1841,
      "step": 132
    },
    {
      "epoch": 4.925925925925926,
      "grad_norm": 2.505732536315918,
      "learning_rate": 0.00017374749498997997,
      "loss": 0.2304,
      "step": 133
    },
    {
      "epoch": 4.962962962962963,
      "grad_norm": 2.8560328483581543,
      "learning_rate": 0.00017354709418837677,
      "loss": 0.3688,
      "step": 134
    },
    {
      "epoch": 5.0,
      "grad_norm": 2.2831168174743652,
      "learning_rate": 0.00017334669338677357,
      "loss": 0.2975,
      "step": 135
    },
    {
      "epoch": 5.037037037037037,
      "grad_norm": 1.1333521604537964,
      "learning_rate": 0.00017314629258517034,
      "loss": 0.1063,
      "step": 136
    },
    {
      "epoch": 5.074074074074074,
      "grad_norm": 1.8609040975570679,
      "learning_rate": 0.00017294589178356714,
      "loss": 0.2203,
      "step": 137
    },
    {
      "epoch": 5.111111111111111,
      "grad_norm": 12.76461410522461,
      "learning_rate": 0.00017274549098196393,
      "loss": 0.1821,
      "step": 138
    },
    {
      "epoch": 5.148148148148148,
      "grad_norm": 1.265764832496643,
      "learning_rate": 0.00017254509018036073,
      "loss": 0.1178,
      "step": 139
    },
    {
      "epoch": 5.185185185185185,
      "grad_norm": 1.7132660150527954,
      "learning_rate": 0.0001723446893787575,
      "loss": 0.1804,
      "step": 140
    },
    {
      "epoch": 5.222222222222222,
      "grad_norm": 1.5263041257858276,
      "learning_rate": 0.00017214428857715433,
      "loss": 0.2013,
      "step": 141
    },
    {
      "epoch": 5.2592592592592595,
      "grad_norm": 1.3951983451843262,
      "learning_rate": 0.0001719438877755511,
      "loss": 0.1203,
      "step": 142
    },
    {
      "epoch": 5.296296296296296,
      "grad_norm": 1.8628424406051636,
      "learning_rate": 0.0001717434869739479,
      "loss": 0.1231,
      "step": 143
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 1.2684612274169922,
      "learning_rate": 0.0001715430861723447,
      "loss": 0.0912,
      "step": 144
    },
    {
      "epoch": 5.37037037037037,
      "grad_norm": 2.115062952041626,
      "learning_rate": 0.0001713426853707415,
      "loss": 0.1735,
      "step": 145
    },
    {
      "epoch": 5.407407407407407,
      "grad_norm": 1.859980821609497,
      "learning_rate": 0.00017114228456913827,
      "loss": 0.2075,
      "step": 146
    },
    {
      "epoch": 5.444444444444445,
      "grad_norm": 2.6165812015533447,
      "learning_rate": 0.0001709418837675351,
      "loss": 0.1963,
      "step": 147
    },
    {
      "epoch": 5.481481481481482,
      "grad_norm": 2.036933422088623,
      "learning_rate": 0.00017074148296593187,
      "loss": 0.1228,
      "step": 148
    },
    {
      "epoch": 5.518518518518518,
      "grad_norm": 1.8361951112747192,
      "learning_rate": 0.00017054108216432866,
      "loss": 0.2255,
      "step": 149
    },
    {
      "epoch": 5.555555555555555,
      "grad_norm": 1.722771167755127,
      "learning_rate": 0.00017034068136272546,
      "loss": 0.099,
      "step": 150
    },
    {
      "epoch": 5.592592592592593,
      "grad_norm": 1.568169355392456,
      "learning_rate": 0.00017014028056112226,
      "loss": 0.3964,
      "step": 151
    },
    {
      "epoch": 5.62962962962963,
      "grad_norm": 2.0233349800109863,
      "learning_rate": 0.00016993987975951903,
      "loss": 0.2495,
      "step": 152
    },
    {
      "epoch": 5.666666666666667,
      "grad_norm": 1.7420716285705566,
      "learning_rate": 0.00016973947895791583,
      "loss": 0.1659,
      "step": 153
    },
    {
      "epoch": 5.703703703703704,
      "grad_norm": 1.9544821977615356,
      "learning_rate": 0.00016953907815631263,
      "loss": 0.1875,
      "step": 154
    },
    {
      "epoch": 5.7407407407407405,
      "grad_norm": 1.994842767715454,
      "learning_rate": 0.00016933867735470943,
      "loss": 0.3396,
      "step": 155
    },
    {
      "epoch": 5.777777777777778,
      "grad_norm": 1.790401816368103,
      "learning_rate": 0.0001691382765531062,
      "loss": 0.1567,
      "step": 156
    },
    {
      "epoch": 5.814814814814815,
      "grad_norm": 1.754883050918579,
      "learning_rate": 0.00016893787575150303,
      "loss": 0.1304,
      "step": 157
    },
    {
      "epoch": 5.851851851851852,
      "grad_norm": 2.0050129890441895,
      "learning_rate": 0.0001687374749498998,
      "loss": 0.2007,
      "step": 158
    },
    {
      "epoch": 5.888888888888889,
      "grad_norm": 1.9634181261062622,
      "learning_rate": 0.0001685370741482966,
      "loss": 0.2057,
      "step": 159
    },
    {
      "epoch": 5.925925925925926,
      "grad_norm": 1.6359065771102905,
      "learning_rate": 0.0001683366733466934,
      "loss": 0.1221,
      "step": 160
    },
    {
      "epoch": 5.962962962962963,
      "grad_norm": 1.5538983345031738,
      "learning_rate": 0.0001681362725450902,
      "loss": 0.1781,
      "step": 161
    },
    {
      "epoch": 6.0,
      "grad_norm": 1.386238694190979,
      "learning_rate": 0.00016793587174348696,
      "loss": 0.1223,
      "step": 162
    },
    {
      "epoch": 6.037037037037037,
      "grad_norm": 1.2018523216247559,
      "learning_rate": 0.0001677354709418838,
      "loss": 0.0883,
      "step": 163
    },
    {
      "epoch": 6.074074074074074,
      "grad_norm": 1.0496031045913696,
      "learning_rate": 0.00016753507014028056,
      "loss": 0.0685,
      "step": 164
    },
    {
      "epoch": 6.111111111111111,
      "grad_norm": 0.6003137230873108,
      "learning_rate": 0.00016733466933867736,
      "loss": 0.0486,
      "step": 165
    },
    {
      "epoch": 6.148148148148148,
      "grad_norm": 1.3931691646575928,
      "learning_rate": 0.00016713426853707416,
      "loss": 0.0796,
      "step": 166
    },
    {
      "epoch": 6.185185185185185,
      "grad_norm": 1.5277005434036255,
      "learning_rate": 0.00016693386773547096,
      "loss": 0.1377,
      "step": 167
    },
    {
      "epoch": 6.222222222222222,
      "grad_norm": 1.4653550386428833,
      "learning_rate": 0.00016673346693386773,
      "loss": 0.1085,
      "step": 168
    },
    {
      "epoch": 6.2592592592592595,
      "grad_norm": 0.8865818977355957,
      "learning_rate": 0.00016653306613226453,
      "loss": 0.0488,
      "step": 169
    },
    {
      "epoch": 6.296296296296296,
      "grad_norm": 1.2961333990097046,
      "learning_rate": 0.00016633266533066135,
      "loss": 0.2275,
      "step": 170
    },
    {
      "epoch": 6.333333333333333,
      "grad_norm": 1.33743155002594,
      "learning_rate": 0.00016613226452905812,
      "loss": 0.0769,
      "step": 171
    },
    {
      "epoch": 6.37037037037037,
      "grad_norm": 1.4374405145645142,
      "learning_rate": 0.00016593186372745492,
      "loss": 0.0932,
      "step": 172
    },
    {
      "epoch": 6.407407407407407,
      "grad_norm": 2.3198533058166504,
      "learning_rate": 0.00016573146292585172,
      "loss": 0.1345,
      "step": 173
    },
    {
      "epoch": 6.444444444444445,
      "grad_norm": 1.9653358459472656,
      "learning_rate": 0.00016553106212424852,
      "loss": 0.1728,
      "step": 174
    },
    {
      "epoch": 6.481481481481482,
      "grad_norm": 1.476054072380066,
      "learning_rate": 0.0001653306613226453,
      "loss": 0.108,
      "step": 175
    },
    {
      "epoch": 6.518518518518518,
      "grad_norm": 1.498436689376831,
      "learning_rate": 0.0001651302605210421,
      "loss": 0.0769,
      "step": 176
    },
    {
      "epoch": 6.555555555555555,
      "grad_norm": 1.681992530822754,
      "learning_rate": 0.0001649298597194389,
      "loss": 0.1011,
      "step": 177
    },
    {
      "epoch": 6.592592592592593,
      "grad_norm": 1.5885212421417236,
      "learning_rate": 0.0001647294589178357,
      "loss": 0.0883,
      "step": 178
    },
    {
      "epoch": 6.62962962962963,
      "grad_norm": 1.7328611612319946,
      "learning_rate": 0.00016452905811623246,
      "loss": 0.1182,
      "step": 179
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 1.6769355535507202,
      "learning_rate": 0.00016432865731462928,
      "loss": 0.123,
      "step": 180
    },
    {
      "epoch": 6.703703703703704,
      "grad_norm": 1.0566405057907104,
      "learning_rate": 0.00016412825651302606,
      "loss": 0.0598,
      "step": 181
    },
    {
      "epoch": 6.7407407407407405,
      "grad_norm": 1.501280426979065,
      "learning_rate": 0.00016392785571142285,
      "loss": 0.1109,
      "step": 182
    },
    {
      "epoch": 6.777777777777778,
      "grad_norm": 1.6482408046722412,
      "learning_rate": 0.00016372745490981965,
      "loss": 0.1358,
      "step": 183
    },
    {
      "epoch": 6.814814814814815,
      "grad_norm": 1.8301924467086792,
      "learning_rate": 0.00016352705410821645,
      "loss": 0.1438,
      "step": 184
    },
    {
      "epoch": 6.851851851851852,
      "grad_norm": 1.389725923538208,
      "learning_rate": 0.00016332665330661322,
      "loss": 0.092,
      "step": 185
    },
    {
      "epoch": 6.888888888888889,
      "grad_norm": 1.5607155561447144,
      "learning_rate": 0.00016312625250501005,
      "loss": 0.145,
      "step": 186
    },
    {
      "epoch": 6.925925925925926,
      "grad_norm": 2.067979335784912,
      "learning_rate": 0.00016292585170340682,
      "loss": 0.1966,
      "step": 187
    },
    {
      "epoch": 6.962962962962963,
      "grad_norm": 1.6432744264602661,
      "learning_rate": 0.00016272545090180362,
      "loss": 0.1502,
      "step": 188
    },
    {
      "epoch": 7.0,
      "grad_norm": 1.499188780784607,
      "learning_rate": 0.00016252505010020042,
      "loss": 0.1166,
      "step": 189
    },
    {
      "epoch": 7.037037037037037,
      "grad_norm": 1.0826411247253418,
      "learning_rate": 0.00016232464929859721,
      "loss": 0.0844,
      "step": 190
    },
    {
      "epoch": 7.074074074074074,
      "grad_norm": 0.9981607794761658,
      "learning_rate": 0.00016212424849699399,
      "loss": 0.0666,
      "step": 191
    },
    {
      "epoch": 7.111111111111111,
      "grad_norm": 1.3179012537002563,
      "learning_rate": 0.00016192384769539078,
      "loss": 0.0816,
      "step": 192
    },
    {
      "epoch": 7.148148148148148,
      "grad_norm": 1.179382085800171,
      "learning_rate": 0.00016172344689378758,
      "loss": 0.0615,
      "step": 193
    },
    {
      "epoch": 7.185185185185185,
      "grad_norm": 1.1645002365112305,
      "learning_rate": 0.00016152304609218438,
      "loss": 0.07,
      "step": 194
    },
    {
      "epoch": 7.222222222222222,
      "grad_norm": 1.334791660308838,
      "learning_rate": 0.00016132264529058115,
      "loss": 0.0684,
      "step": 195
    },
    {
      "epoch": 7.2592592592592595,
      "grad_norm": 1.970151662826538,
      "learning_rate": 0.00016112224448897798,
      "loss": 0.0802,
      "step": 196
    },
    {
      "epoch": 7.296296296296296,
      "grad_norm": 1.2020761966705322,
      "learning_rate": 0.00016092184368737475,
      "loss": 0.0645,
      "step": 197
    },
    {
      "epoch": 7.333333333333333,
      "grad_norm": 1.7836601734161377,
      "learning_rate": 0.00016072144288577155,
      "loss": 0.0923,
      "step": 198
    },
    {
      "epoch": 7.37037037037037,
      "grad_norm": 1.3115586042404175,
      "learning_rate": 0.00016052104208416835,
      "loss": 0.0564,
      "step": 199
    },
    {
      "epoch": 7.407407407407407,
      "grad_norm": 1.5026967525482178,
      "learning_rate": 0.00016032064128256515,
      "loss": 0.0715,
      "step": 200
    },
    {
      "epoch": 7.444444444444445,
      "grad_norm": 1.526752233505249,
      "learning_rate": 0.00016012024048096192,
      "loss": 0.0775,
      "step": 201
    },
    {
      "epoch": 7.481481481481482,
      "grad_norm": 1.7461658716201782,
      "learning_rate": 0.00015991983967935872,
      "loss": 0.1015,
      "step": 202
    },
    {
      "epoch": 7.518518518518518,
      "grad_norm": 1.3957600593566895,
      "learning_rate": 0.00015971943887775551,
      "loss": 0.0833,
      "step": 203
    },
    {
      "epoch": 7.555555555555555,
      "grad_norm": 1.141627311706543,
      "learning_rate": 0.0001595190380761523,
      "loss": 0.0606,
      "step": 204
    },
    {
      "epoch": 7.592592592592593,
      "grad_norm": 1.1171350479125977,
      "learning_rate": 0.0001593186372745491,
      "loss": 0.0601,
      "step": 205
    },
    {
      "epoch": 7.62962962962963,
      "grad_norm": 1.5876648426055908,
      "learning_rate": 0.0001591182364729459,
      "loss": 0.1,
      "step": 206
    },
    {
      "epoch": 7.666666666666667,
      "grad_norm": 1.1731330156326294,
      "learning_rate": 0.00015891783567134268,
      "loss": 0.0873,
      "step": 207
    },
    {
      "epoch": 7.703703703703704,
      "grad_norm": 1.0656827688217163,
      "learning_rate": 0.00015871743486973948,
      "loss": 0.0815,
      "step": 208
    },
    {
      "epoch": 7.7407407407407405,
      "grad_norm": 1.2962361574172974,
      "learning_rate": 0.00015851703406813628,
      "loss": 0.1177,
      "step": 209
    },
    {
      "epoch": 7.777777777777778,
      "grad_norm": 1.5042692422866821,
      "learning_rate": 0.00015831663326653308,
      "loss": 0.0831,
      "step": 210
    },
    {
      "epoch": 7.814814814814815,
      "grad_norm": 1.4140362739562988,
      "learning_rate": 0.00015811623246492985,
      "loss": 0.0788,
      "step": 211
    },
    {
      "epoch": 7.851851851851852,
      "grad_norm": 1.4694530963897705,
      "learning_rate": 0.00015791583166332667,
      "loss": 0.075,
      "step": 212
    },
    {
      "epoch": 7.888888888888889,
      "grad_norm": 1.6482559442520142,
      "learning_rate": 0.00015771543086172345,
      "loss": 0.105,
      "step": 213
    },
    {
      "epoch": 7.925925925925926,
      "grad_norm": 1.1907477378845215,
      "learning_rate": 0.00015751503006012024,
      "loss": 0.0901,
      "step": 214
    },
    {
      "epoch": 7.962962962962963,
      "grad_norm": 1.5348597764968872,
      "learning_rate": 0.00015731462925851704,
      "loss": 0.1931,
      "step": 215
    },
    {
      "epoch": 8.0,
      "grad_norm": 1.0043684244155884,
      "learning_rate": 0.00015711422845691384,
      "loss": 0.0669,
      "step": 216
    },
    {
      "epoch": 8.037037037037036,
      "grad_norm": 1.2372673749923706,
      "learning_rate": 0.0001569138276553106,
      "loss": 0.0463,
      "step": 217
    },
    {
      "epoch": 8.074074074074074,
      "grad_norm": 0.6536022424697876,
      "learning_rate": 0.0001567134268537074,
      "loss": 0.0421,
      "step": 218
    },
    {
      "epoch": 8.11111111111111,
      "grad_norm": 0.731320858001709,
      "learning_rate": 0.0001565130260521042,
      "loss": 0.0479,
      "step": 219
    },
    {
      "epoch": 8.148148148148149,
      "grad_norm": 0.8297260999679565,
      "learning_rate": 0.000156312625250501,
      "loss": 0.0624,
      "step": 220
    },
    {
      "epoch": 8.185185185185185,
      "grad_norm": 1.2675563097000122,
      "learning_rate": 0.00015611222444889778,
      "loss": 0.0581,
      "step": 221
    },
    {
      "epoch": 8.222222222222221,
      "grad_norm": NaN,
      "learning_rate": 0.00015611222444889778,
      "loss": 0.0726,
      "step": 222
    },
    {
      "epoch": 8.25925925925926,
      "grad_norm": 1.2273601293563843,
      "learning_rate": 0.0001559118236472946,
      "loss": 0.0611,
      "step": 223
    },
    {
      "epoch": 8.296296296296296,
      "grad_norm": 2.0294740200042725,
      "learning_rate": 0.00015571142284569138,
      "loss": 0.0636,
      "step": 224
    },
    {
      "epoch": 8.333333333333334,
      "grad_norm": 1.2760436534881592,
      "learning_rate": 0.00015551102204408818,
      "loss": 0.062,
      "step": 225
    },
    {
      "epoch": 8.37037037037037,
      "grad_norm": 1.4074667692184448,
      "learning_rate": 0.000155310621242485,
      "loss": 0.0588,
      "step": 226
    },
    {
      "epoch": 8.407407407407407,
      "grad_norm": 1.0478731393814087,
      "learning_rate": 0.00015511022044088177,
      "loss": 0.054,
      "step": 227
    },
    {
      "epoch": 8.444444444444445,
      "grad_norm": 1.3877822160720825,
      "learning_rate": 0.00015490981963927857,
      "loss": 0.0552,
      "step": 228
    },
    {
      "epoch": 8.481481481481481,
      "grad_norm": 1.2768754959106445,
      "learning_rate": 0.00015470941883767537,
      "loss": 0.0805,
      "step": 229
    },
    {
      "epoch": 8.518518518518519,
      "grad_norm": 1.024088740348816,
      "learning_rate": 0.00015450901803607217,
      "loss": 0.0514,
      "step": 230
    },
    {
      "epoch": 8.555555555555555,
      "grad_norm": 1.2485498189926147,
      "learning_rate": 0.00015430861723446894,
      "loss": 0.0534,
      "step": 231
    },
    {
      "epoch": 8.592592592592592,
      "grad_norm": 1.3300305604934692,
      "learning_rate": 0.00015410821643286574,
      "loss": 0.0792,
      "step": 232
    },
    {
      "epoch": 8.62962962962963,
      "grad_norm": 1.413541316986084,
      "learning_rate": 0.00015390781563126254,
      "loss": 0.0683,
      "step": 233
    },
    {
      "epoch": 8.666666666666666,
      "grad_norm": 1.3094735145568848,
      "learning_rate": 0.00015370741482965934,
      "loss": 0.0912,
      "step": 234
    },
    {
      "epoch": 8.703703703703704,
      "grad_norm": 1.6279065608978271,
      "learning_rate": 0.0001535070140280561,
      "loss": 0.087,
      "step": 235
    },
    {
      "epoch": 8.74074074074074,
      "grad_norm": 1.1990731954574585,
      "learning_rate": 0.00015330661322645293,
      "loss": 0.0628,
      "step": 236
    },
    {
      "epoch": 8.777777777777779,
      "grad_norm": 1.127059817314148,
      "learning_rate": 0.0001531062124248497,
      "loss": 0.0688,
      "step": 237
    },
    {
      "epoch": 8.814814814814815,
      "grad_norm": 1.2548563480377197,
      "learning_rate": 0.0001529058116232465,
      "loss": 0.0755,
      "step": 238
    },
    {
      "epoch": 8.851851851851851,
      "grad_norm": 1.5437253713607788,
      "learning_rate": 0.0001527054108216433,
      "loss": 0.1541,
      "step": 239
    },
    {
      "epoch": 8.88888888888889,
      "grad_norm": 0.9705492258071899,
      "learning_rate": 0.0001525050100200401,
      "loss": 0.064,
      "step": 240
    },
    {
      "epoch": 8.925925925925926,
      "grad_norm": 1.471771240234375,
      "learning_rate": 0.00015230460921843687,
      "loss": 0.0773,
      "step": 241
    },
    {
      "epoch": 8.962962962962964,
      "grad_norm": 1.4249458312988281,
      "learning_rate": 0.00015210420841683367,
      "loss": 0.0886,
      "step": 242
    },
    {
      "epoch": 9.0,
      "grad_norm": 0.9647703170776367,
      "learning_rate": 0.00015190380761523047,
      "loss": 0.0616,
      "step": 243
    },
    {
      "epoch": 9.037037037037036,
      "grad_norm": 0.7374787926673889,
      "learning_rate": 0.00015170340681362727,
      "loss": 0.0401,
      "step": 244
    },
    {
      "epoch": 9.074074074074074,
      "grad_norm": 0.8700003623962402,
      "learning_rate": 0.00015150300601202404,
      "loss": 0.0455,
      "step": 245
    },
    {
      "epoch": 9.11111111111111,
      "grad_norm": 0.7783188819885254,
      "learning_rate": 0.00015130260521042086,
      "loss": 0.0456,
      "step": 246
    },
    {
      "epoch": 9.148148148148149,
      "grad_norm": 0.852952241897583,
      "learning_rate": 0.00015110220440881764,
      "loss": 0.0507,
      "step": 247
    },
    {
      "epoch": 9.185185185185185,
      "grad_norm": 0.7725722789764404,
      "learning_rate": 0.00015090180360721443,
      "loss": 0.042,
      "step": 248
    },
    {
      "epoch": 9.222222222222221,
      "grad_norm": 0.9825606346130371,
      "learning_rate": 0.00015070140280561123,
      "loss": 0.0497,
      "step": 249
    },
    {
      "epoch": 9.25925925925926,
      "grad_norm": 1.2634708881378174,
      "learning_rate": 0.00015050100200400803,
      "loss": 0.0481,
      "step": 250
    },
    {
      "epoch": 9.296296296296296,
      "grad_norm": 1.727638840675354,
      "learning_rate": 0.0001503006012024048,
      "loss": 0.062,
      "step": 251
    },
    {
      "epoch": 9.333333333333334,
      "grad_norm": 1.1912624835968018,
      "learning_rate": 0.00015010020040080163,
      "loss": 0.0582,
      "step": 252
    },
    {
      "epoch": 9.37037037037037,
      "grad_norm": 1.4130210876464844,
      "learning_rate": 0.0001498997995991984,
      "loss": 0.0547,
      "step": 253
    },
    {
      "epoch": 9.407407407407407,
      "grad_norm": 0.9078881144523621,
      "learning_rate": 0.0001496993987975952,
      "loss": 0.048,
      "step": 254
    },
    {
      "epoch": 9.444444444444445,
      "grad_norm": 1.4111179113388062,
      "learning_rate": 0.000149498997995992,
      "loss": 0.0648,
      "step": 255
    },
    {
      "epoch": 9.481481481481481,
      "grad_norm": 1.466229796409607,
      "learning_rate": 0.0001492985971943888,
      "loss": 0.0603,
      "step": 256
    },
    {
      "epoch": 9.518518518518519,
      "grad_norm": 1.2059966325759888,
      "learning_rate": 0.00014909819639278557,
      "loss": 0.0603,
      "step": 257
    },
    {
      "epoch": 9.555555555555555,
      "grad_norm": 1.4421101808547974,
      "learning_rate": 0.00014889779559118237,
      "loss": 0.0525,
      "step": 258
    },
    {
      "epoch": 9.592592592592592,
      "grad_norm": 2.0155293941497803,
      "learning_rate": 0.00014869739478957916,
      "loss": 0.0786,
      "step": 259
    },
    {
      "epoch": 9.62962962962963,
      "grad_norm": 1.2687761783599854,
      "learning_rate": 0.00014849699398797596,
      "loss": 0.0505,
      "step": 260
    },
    {
      "epoch": 9.666666666666666,
      "grad_norm": 1.644395351409912,
      "learning_rate": 0.00014829659318637273,
      "loss": 0.0653,
      "step": 261
    },
    {
      "epoch": 9.703703703703704,
      "grad_norm": 1.050253987312317,
      "learning_rate": 0.00014809619238476956,
      "loss": 0.0636,
      "step": 262
    },
    {
      "epoch": 9.74074074074074,
      "grad_norm": 1.203935146331787,
      "learning_rate": 0.00014789579158316633,
      "loss": 0.0679,
      "step": 263
    },
    {
      "epoch": 9.777777777777779,
      "grad_norm": 1.0087735652923584,
      "learning_rate": 0.00014769539078156313,
      "loss": 0.0717,
      "step": 264
    },
    {
      "epoch": 9.814814814814815,
      "grad_norm": 1.0535528659820557,
      "learning_rate": 0.00014749498997995993,
      "loss": 0.0606,
      "step": 265
    },
    {
      "epoch": 9.851851851851851,
      "grad_norm": 1.590857744216919,
      "learning_rate": 0.00014729458917835673,
      "loss": 0.0762,
      "step": 266
    },
    {
      "epoch": 9.88888888888889,
      "grad_norm": 0.7801868319511414,
      "learning_rate": 0.0001470941883767535,
      "loss": 0.0493,
      "step": 267
    },
    {
      "epoch": 9.925925925925926,
      "grad_norm": 1.004595398902893,
      "learning_rate": 0.0001468937875751503,
      "loss": 0.0561,
      "step": 268
    },
    {
      "epoch": 9.962962962962964,
      "grad_norm": 0.9080763459205627,
      "learning_rate": 0.0001466933867735471,
      "loss": 0.0497,
      "step": 269
    },
    {
      "epoch": 10.0,
      "grad_norm": 1.0506818294525146,
      "learning_rate": 0.0001464929859719439,
      "loss": 0.0861,
      "step": 270
    },
    {
      "epoch": 10.037037037037036,
      "grad_norm": 0.5908365845680237,
      "learning_rate": 0.0001462925851703407,
      "loss": 0.0355,
      "step": 271
    },
    {
      "epoch": 10.074074074074074,
      "grad_norm": 0.8345270752906799,
      "learning_rate": 0.0001460921843687375,
      "loss": 0.0333,
      "step": 272
    },
    {
      "epoch": 10.11111111111111,
      "grad_norm": 1.819779634475708,
      "learning_rate": 0.00014589178356713426,
      "loss": 0.04,
      "step": 273
    },
    {
      "epoch": 10.148148148148149,
      "grad_norm": 1.1970969438552856,
      "learning_rate": 0.00014569138276553106,
      "loss": 0.0467,
      "step": 274
    },
    {
      "epoch": 10.185185185185185,
      "grad_norm": 0.9044147729873657,
      "learning_rate": 0.00014549098196392786,
      "loss": 0.0507,
      "step": 275
    },
    {
      "epoch": 10.222222222222221,
      "grad_norm": 0.7929803729057312,
      "learning_rate": 0.00014529058116232466,
      "loss": 0.0422,
      "step": 276
    },
    {
      "epoch": 10.25925925925926,
      "grad_norm": 0.8099430203437805,
      "learning_rate": 0.00014509018036072143,
      "loss": 0.0582,
      "step": 277
    },
    {
      "epoch": 10.296296296296296,
      "grad_norm": 0.6495085954666138,
      "learning_rate": 0.00014488977955911825,
      "loss": 0.0395,
      "step": 278
    },
    {
      "epoch": 10.333333333333334,
      "grad_norm": 1.1591905355453491,
      "learning_rate": 0.00014468937875751503,
      "loss": 0.0528,
      "step": 279
    },
    {
      "epoch": 10.37037037037037,
      "grad_norm": 0.7852423191070557,
      "learning_rate": 0.00014448897795591182,
      "loss": 0.0394,
      "step": 280
    },
    {
      "epoch": 10.407407407407407,
      "grad_norm": 0.873159646987915,
      "learning_rate": 0.00014428857715430862,
      "loss": 0.0362,
      "step": 281
    },
    {
      "epoch": 10.444444444444445,
      "grad_norm": 1.167210578918457,
      "learning_rate": 0.00014408817635270542,
      "loss": 0.0513,
      "step": 282
    },
    {
      "epoch": 10.481481481481481,
      "grad_norm": 1.4897819757461548,
      "learning_rate": 0.00014388777555110222,
      "loss": 0.0585,
      "step": 283
    },
    {
      "epoch": 10.518518518518519,
      "grad_norm": 1.4481068849563599,
      "learning_rate": 0.000143687374749499,
      "loss": 0.0645,
      "step": 284
    },
    {
      "epoch": 10.555555555555555,
      "grad_norm": 0.7212473750114441,
      "learning_rate": 0.00014348697394789582,
      "loss": 0.0382,
      "step": 285
    },
    {
      "epoch": 10.592592592592592,
      "grad_norm": 1.584017038345337,
      "learning_rate": 0.0001432865731462926,
      "loss": 0.0673,
      "step": 286
    },
    {
      "epoch": 10.62962962962963,
      "grad_norm": 1.2357816696166992,
      "learning_rate": 0.0001430861723446894,
      "loss": 0.066,
      "step": 287
    },
    {
      "epoch": 10.666666666666666,
      "grad_norm": 0.8960538506507874,
      "learning_rate": 0.00014288577154308619,
      "loss": 0.0547,
      "step": 288
    },
    {
      "epoch": 10.703703703703704,
      "grad_norm": 1.2063186168670654,
      "learning_rate": 0.00014268537074148298,
      "loss": 0.0511,
      "step": 289
    },
    {
      "epoch": 10.74074074074074,
      "grad_norm": 1.5664043426513672,
      "learning_rate": 0.00014248496993987976,
      "loss": 0.0809,
      "step": 290
    },
    {
      "epoch": 10.777777777777779,
      "grad_norm": 0.818841278553009,
      "learning_rate": 0.00014228456913827658,
      "loss": 0.0596,
      "step": 291
    },
    {
      "epoch": 10.814814814814815,
      "grad_norm": 1.2230114936828613,
      "learning_rate": 0.00014208416833667335,
      "loss": 0.0682,
      "step": 292
    },
    {
      "epoch": 10.851851851851851,
      "grad_norm": 0.9955494403839111,
      "learning_rate": 0.00014188376753507015,
      "loss": 0.0513,
      "step": 293
    },
    {
      "epoch": 10.88888888888889,
      "grad_norm": 1.518225073814392,
      "learning_rate": 0.00014168336673346695,
      "loss": 0.1048,
      "step": 294
    },
    {
      "epoch": 10.925925925925926,
      "grad_norm": 0.7626500129699707,
      "learning_rate": 0.00014148296593186375,
      "loss": 0.0541,
      "step": 295
    },
    {
      "epoch": 10.962962962962964,
      "grad_norm": 1.4206753969192505,
      "learning_rate": 0.00014128256513026052,
      "loss": 0.0861,
      "step": 296
    },
    {
      "epoch": 11.0,
      "grad_norm": 0.6359922885894775,
      "learning_rate": 0.00014108216432865732,
      "loss": 0.0486,
      "step": 297
    },
    {
      "epoch": 11.037037037037036,
      "grad_norm": 0.5922603607177734,
      "learning_rate": 0.00014088176352705412,
      "loss": 0.0356,
      "step": 298
    },
    {
      "epoch": 11.074074074074074,
      "grad_norm": 0.38280969858169556,
      "learning_rate": 0.00014068136272545092,
      "loss": 0.0322,
      "step": 299
    },
    {
      "epoch": 11.11111111111111,
      "grad_norm": 0.4959040582180023,
      "learning_rate": 0.0001404809619238477,
      "loss": 0.0423,
      "step": 300
    },
    {
      "epoch": 11.148148148148149,
      "grad_norm": 0.6353457570075989,
      "learning_rate": 0.0001402805611222445,
      "loss": 0.0435,
      "step": 301
    },
    {
      "epoch": 11.185185185185185,
      "grad_norm": 0.7576203346252441,
      "learning_rate": 0.00014008016032064128,
      "loss": 0.0321,
      "step": 302
    },
    {
      "epoch": 11.222222222222221,
      "grad_norm": 0.7252066731452942,
      "learning_rate": 0.00013987975951903808,
      "loss": 0.0394,
      "step": 303
    },
    {
      "epoch": 11.25925925925926,
      "grad_norm": 1.1327282190322876,
      "learning_rate": 0.00013967935871743488,
      "loss": 0.0386,
      "step": 304
    },
    {
      "epoch": 11.296296296296296,
      "grad_norm": 0.7345007061958313,
      "learning_rate": 0.00013947895791583168,
      "loss": 0.0433,
      "step": 305
    },
    {
      "epoch": 11.333333333333334,
      "grad_norm": 0.7733079195022583,
      "learning_rate": 0.00013927855711422845,
      "loss": 0.0333,
      "step": 306
    },
    {
      "epoch": 11.37037037037037,
      "grad_norm": 0.7940791845321655,
      "learning_rate": 0.00013907815631262525,
      "loss": 0.0376,
      "step": 307
    },
    {
      "epoch": 11.407407407407407,
      "grad_norm": 0.9100624918937683,
      "learning_rate": 0.00013887775551102205,
      "loss": 0.0444,
      "step": 308
    },
    {
      "epoch": 11.444444444444445,
      "grad_norm": 1.053331971168518,
      "learning_rate": 0.00013867735470941885,
      "loss": 0.0484,
      "step": 309
    },
    {
      "epoch": 11.481481481481481,
      "grad_norm": 0.6496856808662415,
      "learning_rate": 0.00013847695390781562,
      "loss": 0.0402,
      "step": 310
    },
    {
      "epoch": 11.518518518518519,
      "grad_norm": 1.3300055265426636,
      "learning_rate": 0.00013827655310621244,
      "loss": 0.0435,
      "step": 311
    },
    {
      "epoch": 11.555555555555555,
      "grad_norm": 1.1720458269119263,
      "learning_rate": 0.00013807615230460922,
      "loss": 0.0508,
      "step": 312
    },
    {
      "epoch": 11.592592592592592,
      "grad_norm": 0.4667465388774872,
      "learning_rate": 0.00013787575150300601,
      "loss": 0.0409,
      "step": 313
    },
    {
      "epoch": 11.62962962962963,
      "grad_norm": 0.8103139996528625,
      "learning_rate": 0.0001376753507014028,
      "loss": 0.0627,
      "step": 314
    },
    {
      "epoch": 11.666666666666666,
      "grad_norm": 0.9636147618293762,
      "learning_rate": 0.0001374749498997996,
      "loss": 0.0477,
      "step": 315
    },
    {
      "epoch": 11.703703703703704,
      "grad_norm": 1.0425158739089966,
      "learning_rate": 0.00013727454909819638,
      "loss": 0.0478,
      "step": 316
    },
    {
      "epoch": 11.74074074074074,
      "grad_norm": 0.804784893989563,
      "learning_rate": 0.0001370741482965932,
      "loss": 0.0466,
      "step": 317
    },
    {
      "epoch": 11.777777777777779,
      "grad_norm": 1.0412399768829346,
      "learning_rate": 0.00013687374749498998,
      "loss": 0.0553,
      "step": 318
    },
    {
      "epoch": 11.814814814814815,
      "grad_norm": 0.8987804055213928,
      "learning_rate": 0.00013667334669338678,
      "loss": 0.0632,
      "step": 319
    },
    {
      "epoch": 11.851851851851851,
      "grad_norm": 0.6414419412612915,
      "learning_rate": 0.00013647294589178358,
      "loss": 0.0503,
      "step": 320
    },
    {
      "epoch": 11.88888888888889,
      "grad_norm": 0.4002060890197754,
      "learning_rate": 0.00013627254509018038,
      "loss": 0.0377,
      "step": 321
    },
    {
      "epoch": 11.925925925925926,
      "grad_norm": 1.2560527324676514,
      "learning_rate": 0.00013607214428857715,
      "loss": 0.0543,
      "step": 322
    },
    {
      "epoch": 11.962962962962964,
      "grad_norm": 0.8130725622177124,
      "learning_rate": 0.00013587174348697395,
      "loss": 0.0467,
      "step": 323
    },
    {
      "epoch": 12.0,
      "grad_norm": 0.7042014002799988,
      "learning_rate": 0.00013567134268537074,
      "loss": 0.055,
      "step": 324
    },
    {
      "epoch": 12.037037037037036,
      "grad_norm": 0.30106601119041443,
      "learning_rate": 0.00013547094188376754,
      "loss": 0.0295,
      "step": 325
    },
    {
      "epoch": 12.074074074074074,
      "grad_norm": 0.619452714920044,
      "learning_rate": 0.00013527054108216431,
      "loss": 0.035,
      "step": 326
    },
    {
      "epoch": 12.11111111111111,
      "grad_norm": 0.7758803963661194,
      "learning_rate": 0.00013507014028056114,
      "loss": 0.0394,
      "step": 327
    },
    {
      "epoch": 12.148148148148149,
      "grad_norm": 0.2939555048942566,
      "learning_rate": 0.0001348697394789579,
      "loss": 0.0333,
      "step": 328
    },
    {
      "epoch": 12.185185185185185,
      "grad_norm": 0.4854755401611328,
      "learning_rate": 0.0001346693386773547,
      "loss": 0.0321,
      "step": 329
    },
    {
      "epoch": 12.222222222222221,
      "grad_norm": 0.47098100185394287,
      "learning_rate": 0.0001344689378757515,
      "loss": 0.0336,
      "step": 330
    },
    {
      "epoch": 12.25925925925926,
      "grad_norm": 0.9715535640716553,
      "learning_rate": 0.0001342685370741483,
      "loss": 0.0516,
      "step": 331
    },
    {
      "epoch": 12.296296296296296,
      "grad_norm": 0.8751097321510315,
      "learning_rate": 0.00013406813627254508,
      "loss": 0.0411,
      "step": 332
    },
    {
      "epoch": 12.333333333333334,
      "grad_norm": 0.7895125150680542,
      "learning_rate": 0.0001338677354709419,
      "loss": 0.0395,
      "step": 333
    },
    {
      "epoch": 12.37037037037037,
      "grad_norm": 0.6823554039001465,
      "learning_rate": 0.00013366733466933868,
      "loss": 0.0387,
      "step": 334
    },
    {
      "epoch": 12.407407407407407,
      "grad_norm": 0.26752057671546936,
      "learning_rate": 0.00013346693386773547,
      "loss": 0.028,
      "step": 335
    },
    {
      "epoch": 12.444444444444445,
      "grad_norm": 0.27481821179389954,
      "learning_rate": 0.00013326653306613227,
      "loss": 0.0268,
      "step": 336
    },
    {
      "epoch": 12.481481481481481,
      "grad_norm": 0.44282934069633484,
      "learning_rate": 0.00013306613226452907,
      "loss": 0.03,
      "step": 337
    },
    {
      "epoch": 12.518518518518519,
      "grad_norm": 0.5402517318725586,
      "learning_rate": 0.00013286573146292587,
      "loss": 0.0303,
      "step": 338
    },
    {
      "epoch": 12.555555555555555,
      "grad_norm": 0.4563344419002533,
      "learning_rate": 0.00013266533066132264,
      "loss": 0.0372,
      "step": 339
    },
    {
      "epoch": 12.592592592592592,
      "grad_norm": 1.1417568922042847,
      "learning_rate": 0.00013246492985971947,
      "loss": 0.0436,
      "step": 340
    },
    {
      "epoch": 12.62962962962963,
      "grad_norm": 0.7502787113189697,
      "learning_rate": 0.00013226452905811624,
      "loss": 0.0501,
      "step": 341
    },
    {
      "epoch": 12.666666666666666,
      "grad_norm": 0.6135110855102539,
      "learning_rate": 0.00013206412825651304,
      "loss": 0.037,
      "step": 342
    },
    {
      "epoch": 12.703703703703704,
      "grad_norm": 0.9214980602264404,
      "learning_rate": 0.00013186372745490984,
      "loss": 0.0526,
      "step": 343
    },
    {
      "epoch": 12.74074074074074,
      "grad_norm": 0.8492892384529114,
      "learning_rate": 0.00013166332665330663,
      "loss": 0.0455,
      "step": 344
    },
    {
      "epoch": 12.777777777777779,
      "grad_norm": 1.2739397287368774,
      "learning_rate": 0.0001314629258517034,
      "loss": 0.06,
      "step": 345
    },
    {
      "epoch": 12.814814814814815,
      "grad_norm": 0.654902458190918,
      "learning_rate": 0.0001312625250501002,
      "loss": 0.0353,
      "step": 346
    },
    {
      "epoch": 12.851851851851851,
      "grad_norm": 1.1886014938354492,
      "learning_rate": 0.000131062124248497,
      "loss": 0.0602,
      "step": 347
    },
    {
      "epoch": 12.88888888888889,
      "grad_norm": 0.44574496150016785,
      "learning_rate": 0.0001308617234468938,
      "loss": 0.0365,
      "step": 348
    },
    {
      "epoch": 12.925925925925926,
      "grad_norm": 0.7472143769264221,
      "learning_rate": 0.00013066132264529057,
      "loss": 0.0423,
      "step": 349
    },
    {
      "epoch": 12.962962962962964,
      "grad_norm": 0.4996347725391388,
      "learning_rate": 0.0001304609218436874,
      "loss": 0.0342,
      "step": 350
    },
    {
      "epoch": 13.0,
      "grad_norm": 0.8139229416847229,
      "learning_rate": 0.00013026052104208417,
      "loss": 0.0422,
      "step": 351
    },
    {
      "epoch": 13.037037037037036,
      "grad_norm": 0.17460402846336365,
      "learning_rate": 0.00013006012024048097,
      "loss": 0.0244,
      "step": 352
    },
    {
      "epoch": 13.074074074074074,
      "grad_norm": 1.0854288339614868,
      "learning_rate": 0.00012985971943887777,
      "loss": 0.0326,
      "step": 353
    },
    {
      "epoch": 13.11111111111111,
      "grad_norm": 0.8812730312347412,
      "learning_rate": 0.00012965931863727457,
      "loss": 0.0386,
      "step": 354
    },
    {
      "epoch": 13.148148148148149,
      "grad_norm": 0.6162160038948059,
      "learning_rate": 0.00012945891783567134,
      "loss": 0.0421,
      "step": 355
    },
    {
      "epoch": 13.185185185185185,
      "grad_norm": 0.7324422001838684,
      "learning_rate": 0.00012925851703406816,
      "loss": 0.0584,
      "step": 356
    },
    {
      "epoch": 13.222222222222221,
      "grad_norm": 0.35778895020484924,
      "learning_rate": 0.00012905811623246493,
      "loss": 0.031,
      "step": 357
    },
    {
      "epoch": 13.25925925925926,
      "grad_norm": 0.43536970019340515,
      "learning_rate": 0.00012885771543086173,
      "loss": 0.0288,
      "step": 358
    },
    {
      "epoch": 13.296296296296296,
      "grad_norm": 0.28832513093948364,
      "learning_rate": 0.00012865731462925853,
      "loss": 0.0267,
      "step": 359
    },
    {
      "epoch": 13.333333333333334,
      "grad_norm": 0.32554811239242554,
      "learning_rate": 0.00012845691382765533,
      "loss": 0.0292,
      "step": 360
    },
    {
      "epoch": 13.37037037037037,
      "grad_norm": 0.38349437713623047,
      "learning_rate": 0.0001282565130260521,
      "loss": 0.0267,
      "step": 361
    },
    {
      "epoch": 13.407407407407407,
      "grad_norm": 0.38657641410827637,
      "learning_rate": 0.0001280561122244489,
      "loss": 0.0295,
      "step": 362
    },
    {
      "epoch": 13.444444444444445,
      "grad_norm": 0.6064786314964294,
      "learning_rate": 0.0001278557114228457,
      "loss": 0.0334,
      "step": 363
    },
    {
      "epoch": 13.481481481481481,
      "grad_norm": 0.37524479627609253,
      "learning_rate": 0.0001276553106212425,
      "loss": 0.0294,
      "step": 364
    },
    {
      "epoch": 13.518518518518519,
      "grad_norm": 0.795975387096405,
      "learning_rate": 0.00012745490981963927,
      "loss": 0.0331,
      "step": 365
    },
    {
      "epoch": 13.555555555555555,
      "grad_norm": 0.453640878200531,
      "learning_rate": 0.0001272545090180361,
      "loss": 0.0369,
      "step": 366
    },
    {
      "epoch": 13.592592592592592,
      "grad_norm": 0.7161301970481873,
      "learning_rate": 0.00012705410821643286,
      "loss": 0.0387,
      "step": 367
    },
    {
      "epoch": 13.62962962962963,
      "grad_norm": 0.2860196530818939,
      "learning_rate": 0.00012685370741482966,
      "loss": 0.0292,
      "step": 368
    },
    {
      "epoch": 13.666666666666666,
      "grad_norm": 0.8654524087905884,
      "learning_rate": 0.00012665330661322646,
      "loss": 0.0337,
      "step": 369
    },
    {
      "epoch": 13.703703703703704,
      "grad_norm": 0.3767426908016205,
      "learning_rate": 0.00012645290581162326,
      "loss": 0.0312,
      "step": 370
    },
    {
      "epoch": 13.74074074074074,
      "grad_norm": 0.5137270092964172,
      "learning_rate": 0.00012625250501002003,
      "loss": 0.0352,
      "step": 371
    },
    {
      "epoch": 13.777777777777779,
      "grad_norm": 0.44900253415107727,
      "learning_rate": 0.00012605210420841683,
      "loss": 0.0274,
      "step": 372
    },
    {
      "epoch": 13.814814814814815,
      "grad_norm": 0.516342282295227,
      "learning_rate": 0.00012585170340681363,
      "loss": 0.0346,
      "step": 373
    },
    {
      "epoch": 13.851851851851851,
      "grad_norm": 0.5580303072929382,
      "learning_rate": 0.00012565130260521043,
      "loss": 0.0413,
      "step": 374
    },
    {
      "epoch": 13.88888888888889,
      "grad_norm": 0.5013909935951233,
      "learning_rate": 0.0001254509018036072,
      "loss": 0.0387,
      "step": 375
    },
    {
      "epoch": 13.925925925925926,
      "grad_norm": 0.3501083254814148,
      "learning_rate": 0.00012525050100200402,
      "loss": 0.0294,
      "step": 376
    },
    {
      "epoch": 13.962962962962964,
      "grad_norm": 0.8724470138549805,
      "learning_rate": 0.0001250501002004008,
      "loss": 0.0422,
      "step": 377
    },
    {
      "epoch": 14.0,
      "grad_norm": 1.4526540040969849,
      "learning_rate": 0.0001248496993987976,
      "loss": 0.0614,
      "step": 378
    },
    {
      "epoch": 14.037037037037036,
      "grad_norm": 0.19861972332000732,
      "learning_rate": 0.0001246492985971944,
      "loss": 0.0261,
      "step": 379
    },
    {
      "epoch": 14.074074074074074,
      "grad_norm": 0.20139285922050476,
      "learning_rate": 0.0001244488977955912,
      "loss": 0.0231,
      "step": 380
    },
    {
      "epoch": 14.11111111111111,
      "grad_norm": 0.15112949907779694,
      "learning_rate": 0.00012424849699398796,
      "loss": 0.025,
      "step": 381
    },
    {
      "epoch": 14.148148148148149,
      "grad_norm": 0.14301951229572296,
      "learning_rate": 0.0001240480961923848,
      "loss": 0.0216,
      "step": 382
    },
    {
      "epoch": 14.185185185185185,
      "grad_norm": 0.1523558795452118,
      "learning_rate": 0.00012384769539078156,
      "loss": 0.0233,
      "step": 383
    },
    {
      "epoch": 14.222222222222221,
      "grad_norm": 0.20586998760700226,
      "learning_rate": 0.00012364729458917836,
      "loss": 0.0229,
      "step": 384
    },
    {
      "epoch": 14.25925925925926,
      "grad_norm": 0.5122826099395752,
      "learning_rate": 0.00012344689378757516,
      "loss": 0.0403,
      "step": 385
    },
    {
      "epoch": 14.296296296296296,
      "grad_norm": 0.47157731652259827,
      "learning_rate": 0.00012324649298597196,
      "loss": 0.0333,
      "step": 386
    },
    {
      "epoch": 14.333333333333334,
      "grad_norm": 0.15661773085594177,
      "learning_rate": 0.00012304609218436873,
      "loss": 0.0277,
      "step": 387
    },
    {
      "epoch": 14.37037037037037,
      "grad_norm": 0.501438558101654,
      "learning_rate": 0.00012284569138276553,
      "loss": 0.0368,
      "step": 388
    },
    {
      "epoch": 14.407407407407407,
      "grad_norm": 0.7966614961624146,
      "learning_rate": 0.00012264529058116232,
      "loss": 0.0439,
      "step": 389
    },
    {
      "epoch": 14.444444444444445,
      "grad_norm": 0.6421314477920532,
      "learning_rate": 0.00012244488977955912,
      "loss": 0.0402,
      "step": 390
    },
    {
      "epoch": 14.481481481481481,
      "grad_norm": 0.2019364833831787,
      "learning_rate": 0.0001222444889779559,
      "loss": 0.0267,
      "step": 391
    },
    {
      "epoch": 14.518518518518519,
      "grad_norm": 0.3437703549861908,
      "learning_rate": 0.0001220440881763527,
      "loss": 0.0266,
      "step": 392
    },
    {
      "epoch": 14.555555555555555,
      "grad_norm": 0.7435939311981201,
      "learning_rate": 0.00012184368737474952,
      "loss": 0.0351,
      "step": 393
    },
    {
      "epoch": 14.592592592592592,
      "grad_norm": 0.19059042632579803,
      "learning_rate": 0.0001216432865731463,
      "loss": 0.023,
      "step": 394
    },
    {
      "epoch": 14.62962962962963,
      "grad_norm": 0.45882317423820496,
      "learning_rate": 0.0001214428857715431,
      "loss": 0.0278,
      "step": 395
    },
    {
      "epoch": 14.666666666666666,
      "grad_norm": 0.3508089780807495,
      "learning_rate": 0.00012124248496993989,
      "loss": 0.0325,
      "step": 396
    },
    {
      "epoch": 14.703703703703704,
      "grad_norm": 0.8747490048408508,
      "learning_rate": 0.00012104208416833669,
      "loss": 0.0361,
      "step": 397
    },
    {
      "epoch": 14.74074074074074,
      "grad_norm": 0.5853513479232788,
      "learning_rate": 0.00012084168336673347,
      "loss": 0.0377,
      "step": 398
    },
    {
      "epoch": 14.777777777777779,
      "grad_norm": 0.534807562828064,
      "learning_rate": 0.00012064128256513027,
      "loss": 0.0283,
      "step": 399
    },
    {
      "epoch": 14.814814814814815,
      "grad_norm": 0.7036003470420837,
      "learning_rate": 0.00012044088176352705,
      "loss": 0.0463,
      "step": 400
    },
    {
      "epoch": 14.851851851851851,
      "grad_norm": 0.5275050401687622,
      "learning_rate": 0.00012024048096192387,
      "loss": 0.0377,
      "step": 401
    },
    {
      "epoch": 14.88888888888889,
      "grad_norm": 0.35144442319869995,
      "learning_rate": 0.00012004008016032065,
      "loss": 0.0298,
      "step": 402
    },
    {
      "epoch": 14.925925925925926,
      "grad_norm": 0.42558711767196655,
      "learning_rate": 0.00011983967935871745,
      "loss": 0.035,
      "step": 403
    },
    {
      "epoch": 14.962962962962964,
      "grad_norm": 0.1957201212644577,
      "learning_rate": 0.00011963927855711424,
      "loss": 0.0287,
      "step": 404
    },
    {
      "epoch": 15.0,
      "grad_norm": 0.3119232654571533,
      "learning_rate": 0.00011943887775551103,
      "loss": 0.0361,
      "step": 405
    },
    {
      "epoch": 15.037037037037036,
      "grad_norm": 0.29238471388816833,
      "learning_rate": 0.00011923847695390782,
      "loss": 0.028,
      "step": 406
    },
    {
      "epoch": 15.074074074074074,
      "grad_norm": 0.21786649525165558,
      "learning_rate": 0.00011903807615230462,
      "loss": 0.0262,
      "step": 407
    },
    {
      "epoch": 15.11111111111111,
      "grad_norm": 0.17184920608997345,
      "learning_rate": 0.0001188376753507014,
      "loss": 0.0246,
      "step": 408
    },
    {
      "epoch": 15.148148148148149,
      "grad_norm": 0.17216891050338745,
      "learning_rate": 0.00011863727454909821,
      "loss": 0.0222,
      "step": 409
    },
    {
      "epoch": 15.185185185185185,
      "grad_norm": 0.6524999737739563,
      "learning_rate": 0.00011843687374749499,
      "loss": 0.0292,
      "step": 410
    },
    {
      "epoch": 15.222222222222221,
      "grad_norm": 0.7498574256896973,
      "learning_rate": 0.0001182364729458918,
      "loss": 0.0274,
      "step": 411
    },
    {
      "epoch": 15.25925925925926,
      "grad_norm": 0.13467781245708466,
      "learning_rate": 0.00011803607214428858,
      "loss": 0.0207,
      "step": 412
    },
    {
      "epoch": 15.296296296296296,
      "grad_norm": 0.663286030292511,
      "learning_rate": 0.00011783567134268538,
      "loss": 0.0409,
      "step": 413
    },
    {
      "epoch": 15.333333333333334,
      "grad_norm": 0.35175031423568726,
      "learning_rate": 0.00011763527054108217,
      "loss": 0.0256,
      "step": 414
    },
    {
      "epoch": 15.37037037037037,
      "grad_norm": 0.34167206287384033,
      "learning_rate": 0.00011743486973947896,
      "loss": 0.0371,
      "step": 415
    },
    {
      "epoch": 15.407407407407407,
      "grad_norm": 0.5397125482559204,
      "learning_rate": 0.00011723446893787575,
      "loss": 0.0393,
      "step": 416
    },
    {
      "epoch": 15.444444444444445,
      "grad_norm": 0.20127902925014496,
      "learning_rate": 0.00011703406813627256,
      "loss": 0.0251,
      "step": 417
    },
    {
      "epoch": 15.481481481481481,
      "grad_norm": 0.7514338493347168,
      "learning_rate": 0.00011683366733466933,
      "loss": 0.0367,
      "step": 418
    },
    {
      "epoch": 15.518518518518519,
      "grad_norm": 0.22322623431682587,
      "learning_rate": 0.00011663326653306615,
      "loss": 0.0212,
      "step": 419
    },
    {
      "epoch": 15.555555555555555,
      "grad_norm": 0.3187597095966339,
      "learning_rate": 0.00011643286573146293,
      "loss": 0.0269,
      "step": 420
    },
    {
      "epoch": 15.592592592592592,
      "grad_norm": 0.2760290503501892,
      "learning_rate": 0.00011623246492985973,
      "loss": 0.0254,
      "step": 421
    },
    {
      "epoch": 15.62962962962963,
      "grad_norm": 0.35340186953544617,
      "learning_rate": 0.00011603206412825651,
      "loss": 0.0289,
      "step": 422
    },
    {
      "epoch": 15.666666666666666,
      "grad_norm": 0.4748540222644806,
      "learning_rate": 0.00011583166332665331,
      "loss": 0.0263,
      "step": 423
    },
    {
      "epoch": 15.703703703703704,
      "grad_norm": 0.20941664278507233,
      "learning_rate": 0.0001156312625250501,
      "loss": 0.0281,
      "step": 424
    },
    {
      "epoch": 15.74074074074074,
      "grad_norm": 0.6618986129760742,
      "learning_rate": 0.00011543086172344691,
      "loss": 0.0359,
      "step": 425
    },
    {
      "epoch": 15.777777777777779,
      "grad_norm": 0.26745250821113586,
      "learning_rate": 0.00011523046092184368,
      "loss": 0.0333,
      "step": 426
    },
    {
      "epoch": 15.814814814814815,
      "grad_norm": 0.3841153681278229,
      "learning_rate": 0.00011503006012024049,
      "loss": 0.0328,
      "step": 427
    },
    {
      "epoch": 15.851851851851851,
      "grad_norm": 0.5261085033416748,
      "learning_rate": 0.00011482965931863728,
      "loss": 0.0293,
      "step": 428
    },
    {
      "epoch": 15.88888888888889,
      "grad_norm": 0.37217646837234497,
      "learning_rate": 0.00011462925851703408,
      "loss": 0.0279,
      "step": 429
    },
    {
      "epoch": 15.925925925925926,
      "grad_norm": 0.24486394226551056,
      "learning_rate": 0.00011442885771543086,
      "loss": 0.0276,
      "step": 430
    },
    {
      "epoch": 15.962962962962964,
      "grad_norm": 0.36117318272590637,
      "learning_rate": 0.00011422845691382766,
      "loss": 0.0264,
      "step": 431
    },
    {
      "epoch": 16.0,
      "grad_norm": 0.20838835835456848,
      "learning_rate": 0.00011402805611222445,
      "loss": 0.0321,
      "step": 432
    },
    {
      "epoch": 16.037037037037038,
      "grad_norm": 0.12924319505691528,
      "learning_rate": 0.00011382765531062126,
      "loss": 0.0219,
      "step": 433
    },
    {
      "epoch": 16.074074074074073,
      "grad_norm": 0.47885775566101074,
      "learning_rate": 0.00011362725450901803,
      "loss": 0.0275,
      "step": 434
    },
    {
      "epoch": 16.11111111111111,
      "grad_norm": 0.10595516860485077,
      "learning_rate": 0.00011342685370741484,
      "loss": 0.0211,
      "step": 435
    },
    {
      "epoch": 16.14814814814815,
      "grad_norm": 0.12245005369186401,
      "learning_rate": 0.00011322645290581163,
      "loss": 0.0249,
      "step": 436
    },
    {
      "epoch": 16.185185185185187,
      "grad_norm": 0.1357812136411667,
      "learning_rate": 0.00011302605210420842,
      "loss": 0.0235,
      "step": 437
    },
    {
      "epoch": 16.22222222222222,
      "grad_norm": 0.16972768306732178,
      "learning_rate": 0.00011282565130260521,
      "loss": 0.0231,
      "step": 438
    },
    {
      "epoch": 16.25925925925926,
      "grad_norm": 0.2073618620634079,
      "learning_rate": 0.00011262525050100201,
      "loss": 0.0287,
      "step": 439
    },
    {
      "epoch": 16.296296296296298,
      "grad_norm": 0.37559667229652405,
      "learning_rate": 0.00011242484969939879,
      "loss": 0.0279,
      "step": 440
    },
    {
      "epoch": 16.333333333333332,
      "grad_norm": 0.1591014713048935,
      "learning_rate": 0.00011222444889779559,
      "loss": 0.0256,
      "step": 441
    },
    {
      "epoch": 16.37037037037037,
      "grad_norm": 0.2811826169490814,
      "learning_rate": 0.00011202404809619238,
      "loss": 0.0223,
      "step": 442
    },
    {
      "epoch": 16.40740740740741,
      "grad_norm": 0.33672210574150085,
      "learning_rate": 0.00011182364729458919,
      "loss": 0.0263,
      "step": 443
    },
    {
      "epoch": 16.444444444444443,
      "grad_norm": 0.474674254655838,
      "learning_rate": 0.00011162324649298596,
      "loss": 0.025,
      "step": 444
    },
    {
      "epoch": 16.48148148148148,
      "grad_norm": 0.40313687920570374,
      "learning_rate": 0.00011142284569138277,
      "loss": 0.0358,
      "step": 445
    },
    {
      "epoch": 16.51851851851852,
      "grad_norm": 0.6080648303031921,
      "learning_rate": 0.00011122244488977956,
      "loss": 0.0386,
      "step": 446
    },
    {
      "epoch": 16.555555555555557,
      "grad_norm": 0.2893613576889038,
      "learning_rate": 0.00011102204408817636,
      "loss": 0.0288,
      "step": 447
    },
    {
      "epoch": 16.59259259259259,
      "grad_norm": 0.17220960557460785,
      "learning_rate": 0.00011082164328657317,
      "loss": 0.0268,
      "step": 448
    },
    {
      "epoch": 16.62962962962963,
      "grad_norm": 0.2868034839630127,
      "learning_rate": 0.00011062124248496994,
      "loss": 0.0295,
      "step": 449
    },
    {
      "epoch": 16.666666666666668,
      "grad_norm": 0.18853825330734253,
      "learning_rate": 0.00011042084168336675,
      "loss": 0.0285,
      "step": 450
    },
    {
      "epoch": 16.703703703703702,
      "grad_norm": 0.21354185044765472,
      "learning_rate": 0.00011022044088176354,
      "loss": 0.0272,
      "step": 451
    },
    {
      "epoch": 16.74074074074074,
      "grad_norm": 0.1482076793909073,
      "learning_rate": 0.00011002004008016033,
      "loss": 0.0259,
      "step": 452
    },
    {
      "epoch": 16.77777777777778,
      "grad_norm": 0.35347655415534973,
      "learning_rate": 0.00010981963927855712,
      "loss": 0.0235,
      "step": 453
    },
    {
      "epoch": 16.814814814814813,
      "grad_norm": 0.7997738122940063,
      "learning_rate": 0.00010961923847695392,
      "loss": 0.0303,
      "step": 454
    },
    {
      "epoch": 16.85185185185185,
      "grad_norm": 0.1521868258714676,
      "learning_rate": 0.0001094188376753507,
      "loss": 0.0242,
      "step": 455
    },
    {
      "epoch": 16.88888888888889,
      "grad_norm": 0.5698783993721008,
      "learning_rate": 0.00010921843687374752,
      "loss": 0.0385,
      "step": 456
    },
    {
      "epoch": 16.925925925925927,
      "grad_norm": 0.2126310020685196,
      "learning_rate": 0.00010901803607214429,
      "loss": 0.0282,
      "step": 457
    },
    {
      "epoch": 16.962962962962962,
      "grad_norm": 0.16841605305671692,
      "learning_rate": 0.0001088176352705411,
      "loss": 0.0192,
      "step": 458
    },
    {
      "epoch": 17.0,
      "grad_norm": 0.2548891007900238,
      "learning_rate": 0.00010861723446893788,
      "loss": 0.0248,
      "step": 459
    },
    {
      "epoch": 17.037037037037038,
      "grad_norm": 0.12552155554294586,
      "learning_rate": 0.00010841683366733468,
      "loss": 0.0242,
      "step": 460
    },
    {
      "epoch": 17.074074074074073,
      "grad_norm": 0.10076369345188141,
      "learning_rate": 0.00010821643286573147,
      "loss": 0.0183,
      "step": 461
    },
    {
      "epoch": 17.11111111111111,
      "grad_norm": 0.4432346522808075,
      "learning_rate": 0.00010801603206412827,
      "loss": 0.0269,
      "step": 462
    },
    {
      "epoch": 17.14814814814815,
      "grad_norm": 0.5575934648513794,
      "learning_rate": 0.00010781563126252505,
      "loss": 0.0289,
      "step": 463
    },
    {
      "epoch": 17.185185185185187,
      "grad_norm": 0.12666645646095276,
      "learning_rate": 0.00010761523046092185,
      "loss": 0.0177,
      "step": 464
    },
    {
      "epoch": 17.22222222222222,
      "grad_norm": 0.1326364427804947,
      "learning_rate": 0.00010741482965931863,
      "loss": 0.0231,
      "step": 465
    },
    {
      "epoch": 17.25925925925926,
      "grad_norm": 0.16062404215335846,
      "learning_rate": 0.00010721442885771545,
      "loss": 0.0276,
      "step": 466
    },
    {
      "epoch": 17.296296296296298,
      "grad_norm": 0.7143458127975464,
      "learning_rate": 0.00010701402805611223,
      "loss": 0.027,
      "step": 467
    },
    {
      "epoch": 17.333333333333332,
      "grad_norm": 0.2168736606836319,
      "learning_rate": 0.00010681362725450903,
      "loss": 0.0269,
      "step": 468
    },
    {
      "epoch": 17.37037037037037,
      "grad_norm": 0.1578088104724884,
      "learning_rate": 0.00010661322645290582,
      "loss": 0.0285,
      "step": 469
    },
    {
      "epoch": 17.40740740740741,
      "grad_norm": 0.13782066106796265,
      "learning_rate": 0.00010641282565130261,
      "loss": 0.0257,
      "step": 470
    },
    {
      "epoch": 17.444444444444443,
      "grad_norm": 0.25650638341903687,
      "learning_rate": 0.0001062124248496994,
      "loss": 0.0251,
      "step": 471
    },
    {
      "epoch": 17.48148148148148,
      "grad_norm": 0.27481648325920105,
      "learning_rate": 0.0001060120240480962,
      "loss": 0.0275,
      "step": 472
    },
    {
      "epoch": 17.51851851851852,
      "grad_norm": 0.20846951007843018,
      "learning_rate": 0.00010581162324649298,
      "loss": 0.0236,
      "step": 473
    },
    {
      "epoch": 17.555555555555557,
      "grad_norm": 0.19846048951148987,
      "learning_rate": 0.0001056112224448898,
      "loss": 0.0245,
      "step": 474
    },
    {
      "epoch": 17.59259259259259,
      "grad_norm": 0.12382128089666367,
      "learning_rate": 0.00010541082164328657,
      "loss": 0.0237,
      "step": 475
    },
    {
      "epoch": 17.62962962962963,
      "grad_norm": 0.24272292852401733,
      "learning_rate": 0.00010521042084168338,
      "loss": 0.0204,
      "step": 476
    },
    {
      "epoch": 17.666666666666668,
      "grad_norm": 0.1624356359243393,
      "learning_rate": 0.00010501002004008016,
      "loss": 0.0203,
      "step": 477
    },
    {
      "epoch": 17.703703703703702,
      "grad_norm": 0.15605877339839935,
      "learning_rate": 0.00010480961923847696,
      "loss": 0.0245,
      "step": 478
    },
    {
      "epoch": 17.74074074074074,
      "grad_norm": 0.4659874141216278,
      "learning_rate": 0.00010460921843687375,
      "loss": 0.0339,
      "step": 479
    },
    {
      "epoch": 17.77777777777778,
      "grad_norm": 0.15632987022399902,
      "learning_rate": 0.00010440881763527055,
      "loss": 0.028,
      "step": 480
    },
    {
      "epoch": 17.814814814814813,
      "grad_norm": 0.21622446179389954,
      "learning_rate": 0.00010420841683366733,
      "loss": 0.0285,
      "step": 481
    },
    {
      "epoch": 17.85185185185185,
      "grad_norm": 0.2627243399620056,
      "learning_rate": 0.00010400801603206414,
      "loss": 0.0254,
      "step": 482
    },
    {
      "epoch": 17.88888888888889,
      "grad_norm": 0.19141949713230133,
      "learning_rate": 0.00010380761523046091,
      "loss": 0.0273,
      "step": 483
    },
    {
      "epoch": 17.925925925925927,
      "grad_norm": 0.25379911065101624,
      "learning_rate": 0.00010360721442885773,
      "loss": 0.0318,
      "step": 484
    },
    {
      "epoch": 17.962962962962962,
      "grad_norm": 0.42825236916542053,
      "learning_rate": 0.00010340681362725451,
      "loss": 0.0301,
      "step": 485
    },
    {
      "epoch": 18.0,
      "grad_norm": 0.1462051421403885,
      "learning_rate": 0.00010320641282565131,
      "loss": 0.0258,
      "step": 486
    },
    {
      "epoch": 18.037037037037038,
      "grad_norm": 0.12072025239467621,
      "learning_rate": 0.0001030060120240481,
      "loss": 0.0176,
      "step": 487
    },
    {
      "epoch": 18.074074074074073,
      "grad_norm": 0.141004741191864,
      "learning_rate": 0.00010280561122244489,
      "loss": 0.0235,
      "step": 488
    },
    {
      "epoch": 18.11111111111111,
      "grad_norm": 0.12237051874399185,
      "learning_rate": 0.00010260521042084168,
      "loss": 0.0198,
      "step": 489
    },
    {
      "epoch": 18.14814814814815,
      "grad_norm": 0.19282926619052887,
      "learning_rate": 0.00010240480961923849,
      "loss": 0.0239,
      "step": 490
    },
    {
      "epoch": 18.185185185185187,
      "grad_norm": 0.14472904801368713,
      "learning_rate": 0.00010220440881763526,
      "loss": 0.0237,
      "step": 491
    },
    {
      "epoch": 18.22222222222222,
      "grad_norm": 0.17054502665996552,
      "learning_rate": 0.00010200400801603207,
      "loss": 0.0233,
      "step": 492
    },
    {
      "epoch": 18.25925925925926,
      "grad_norm": 0.5415389537811279,
      "learning_rate": 0.00010180360721442886,
      "loss": 0.0224,
      "step": 493
    },
    {
      "epoch": 18.296296296296298,
      "grad_norm": 0.514498770236969,
      "learning_rate": 0.00010160320641282566,
      "loss": 0.0264,
      "step": 494
    },
    {
      "epoch": 18.333333333333332,
      "grad_norm": 0.1169576495885849,
      "learning_rate": 0.00010140280561122244,
      "loss": 0.0195,
      "step": 495
    },
    {
      "epoch": 18.37037037037037,
      "grad_norm": 0.11642298847436905,
      "learning_rate": 0.00010120240480961924,
      "loss": 0.0195,
      "step": 496
    },
    {
      "epoch": 18.40740740740741,
      "grad_norm": 0.13670563697814941,
      "learning_rate": 0.00010100200400801603,
      "loss": 0.0241,
      "step": 497
    },
    {
      "epoch": 18.444444444444443,
      "grad_norm": 0.1577383279800415,
      "learning_rate": 0.00010080160320641284,
      "loss": 0.0261,
      "step": 498
    },
    {
      "epoch": 18.48148148148148,
      "grad_norm": 0.1338013857603073,
      "learning_rate": 0.00010060120240480961,
      "loss": 0.0249,
      "step": 499
    },
    {
      "epoch": 18.51851851851852,
      "grad_norm": 0.1962672770023346,
      "learning_rate": 0.00010040080160320642,
      "loss": 0.0237,
      "step": 500
    },
    {
      "epoch": 18.555555555555557,
      "grad_norm": 0.16365988552570343,
      "learning_rate": 0.0001002004008016032,
      "loss": 0.0261,
      "step": 501
    },
    {
      "epoch": 18.59259259259259,
      "grad_norm": 0.12032685428857803,
      "learning_rate": 0.0001,
      "loss": 0.0184,
      "step": 502
    },
    {
      "epoch": 18.62962962962963,
      "grad_norm": 0.14418888092041016,
      "learning_rate": 9.97995991983968e-05,
      "loss": 0.0233,
      "step": 503
    },
    {
      "epoch": 18.666666666666668,
      "grad_norm": 0.1619766652584076,
      "learning_rate": 9.959919839679359e-05,
      "loss": 0.0257,
      "step": 504
    },
    {
      "epoch": 18.703703703703702,
      "grad_norm": 0.5052146315574646,
      "learning_rate": 9.939879759519039e-05,
      "loss": 0.0333,
      "step": 505
    },
    {
      "epoch": 18.74074074074074,
      "grad_norm": 0.5924764275550842,
      "learning_rate": 9.919839679358717e-05,
      "loss": 0.0354,
      "step": 506
    },
    {
      "epoch": 18.77777777777778,
      "grad_norm": 0.1407831609249115,
      "learning_rate": 9.899799599198397e-05,
      "loss": 0.0255,
      "step": 507
    },
    {
      "epoch": 18.814814814814813,
      "grad_norm": 0.4752911329269409,
      "learning_rate": 9.879759519038077e-05,
      "loss": 0.0303,
      "step": 508
    },
    {
      "epoch": 18.85185185185185,
      "grad_norm": 0.18908542394638062,
      "learning_rate": 9.859719438877755e-05,
      "loss": 0.0333,
      "step": 509
    },
    {
      "epoch": 18.88888888888889,
      "grad_norm": 0.14465676248073578,
      "learning_rate": 9.839679358717435e-05,
      "loss": 0.0275,
      "step": 510
    },
    {
      "epoch": 18.925925925925927,
      "grad_norm": 0.28505051136016846,
      "learning_rate": 9.819639278557115e-05,
      "loss": 0.0256,
      "step": 511
    },
    {
      "epoch": 18.962962962962962,
      "grad_norm": 0.14673714339733124,
      "learning_rate": 9.799599198396794e-05,
      "loss": 0.0265,
      "step": 512
    },
    {
      "epoch": 19.0,
      "grad_norm": 0.5833420157432556,
      "learning_rate": 9.779559118236473e-05,
      "loss": 0.0336,
      "step": 513
    },
    {
      "epoch": 19.037037037037038,
      "grad_norm": 0.11886291950941086,
      "learning_rate": 9.759519038076152e-05,
      "loss": 0.0224,
      "step": 514
    },
    {
      "epoch": 19.074074074074073,
      "grad_norm": 0.15218456089496613,
      "learning_rate": 9.739478957915832e-05,
      "loss": 0.0218,
      "step": 515
    },
    {
      "epoch": 19.11111111111111,
      "grad_norm": 0.10536502301692963,
      "learning_rate": 9.719438877755512e-05,
      "loss": 0.0198,
      "step": 516
    },
    {
      "epoch": 19.14814814814815,
      "grad_norm": 0.17798732221126556,
      "learning_rate": 9.69939879759519e-05,
      "loss": 0.0241,
      "step": 517
    },
    {
      "epoch": 19.185185185185187,
      "grad_norm": 0.12116431444883347,
      "learning_rate": 9.67935871743487e-05,
      "loss": 0.0217,
      "step": 518
    },
    {
      "epoch": 19.22222222222222,
      "grad_norm": 0.18557237088680267,
      "learning_rate": 9.65931863727455e-05,
      "loss": 0.0234,
      "step": 519
    },
    {
      "epoch": 19.25925925925926,
      "grad_norm": 0.13223165273666382,
      "learning_rate": 9.639278557114228e-05,
      "loss": 0.0241,
      "step": 520
    },
    {
      "epoch": 19.296296296296298,
      "grad_norm": 0.14592395722866058,
      "learning_rate": 9.619238476953908e-05,
      "loss": 0.0243,
      "step": 521
    },
    {
      "epoch": 19.333333333333332,
      "grad_norm": 0.11865517497062683,
      "learning_rate": 9.599198396793587e-05,
      "loss": 0.0198,
      "step": 522
    },
    {
      "epoch": 19.37037037037037,
      "grad_norm": 0.1662830263376236,
      "learning_rate": 9.579158316633267e-05,
      "loss": 0.0224,
      "step": 523
    },
    {
      "epoch": 19.40740740740741,
      "grad_norm": 0.20868563652038574,
      "learning_rate": 9.559118236472946e-05,
      "loss": 0.0235,
      "step": 524
    },
    {
      "epoch": 19.444444444444443,
      "grad_norm": 0.15034441649913788,
      "learning_rate": 9.539078156312625e-05,
      "loss": 0.0276,
      "step": 525
    },
    {
      "epoch": 19.48148148148148,
      "grad_norm": 0.13293281197547913,
      "learning_rate": 9.519038076152305e-05,
      "loss": 0.023,
      "step": 526
    },
    {
      "epoch": 19.51851851851852,
      "grad_norm": 0.1815921813249588,
      "learning_rate": 9.498997995991983e-05,
      "loss": 0.0257,
      "step": 527
    },
    {
      "epoch": 19.555555555555557,
      "grad_norm": 0.11606353521347046,
      "learning_rate": 9.478957915831663e-05,
      "loss": 0.0226,
      "step": 528
    },
    {
      "epoch": 19.59259259259259,
      "grad_norm": 0.23613221943378448,
      "learning_rate": 9.458917835671343e-05,
      "loss": 0.03,
      "step": 529
    },
    {
      "epoch": 19.62962962962963,
      "grad_norm": 0.13635443150997162,
      "learning_rate": 9.438877755511023e-05,
      "loss": 0.017,
      "step": 530
    },
    {
      "epoch": 19.666666666666668,
      "grad_norm": 0.5961220264434814,
      "learning_rate": 9.418837675350703e-05,
      "loss": 0.0404,
      "step": 531
    },
    {
      "epoch": 19.703703703703702,
      "grad_norm": 0.1354340761899948,
      "learning_rate": 9.398797595190381e-05,
      "loss": 0.0228,
      "step": 532
    },
    {
      "epoch": 19.74074074074074,
      "grad_norm": 0.1471337527036667,
      "learning_rate": 9.378757515030061e-05,
      "loss": 0.0257,
      "step": 533
    },
    {
      "epoch": 19.77777777777778,
      "grad_norm": 0.1503000110387802,
      "learning_rate": 9.358717434869741e-05,
      "loss": 0.0277,
      "step": 534
    },
    {
      "epoch": 19.814814814814813,
      "grad_norm": 0.32888326048851013,
      "learning_rate": 9.33867735470942e-05,
      "loss": 0.0267,
      "step": 535
    },
    {
      "epoch": 19.85185185185185,
      "grad_norm": 0.1313401311635971,
      "learning_rate": 9.318637274549099e-05,
      "loss": 0.0231,
      "step": 536
    },
    {
      "epoch": 19.88888888888889,
      "grad_norm": 0.16861896216869354,
      "learning_rate": 9.298597194388778e-05,
      "loss": 0.0258,
      "step": 537
    },
    {
      "epoch": 19.925925925925927,
      "grad_norm": 0.5906833410263062,
      "learning_rate": 9.278557114228458e-05,
      "loss": 0.0359,
      "step": 538
    },
    {
      "epoch": 19.962962962962962,
      "grad_norm": 0.25916722416877747,
      "learning_rate": 9.258517034068137e-05,
      "loss": 0.019,
      "step": 539
    },
    {
      "epoch": 20.0,
      "grad_norm": 0.11940132081508636,
      "learning_rate": 9.238476953907816e-05,
      "loss": 0.02,
      "step": 540
    },
    {
      "epoch": 20.037037037037038,
      "grad_norm": 0.1199263408780098,
      "learning_rate": 9.218436873747496e-05,
      "loss": 0.0174,
      "step": 541
    },
    {
      "epoch": 20.074074074074073,
      "grad_norm": 0.11700902134180069,
      "learning_rate": 9.198396793587176e-05,
      "loss": 0.0223,
      "step": 542
    },
    {
      "epoch": 20.11111111111111,
      "grad_norm": 0.11632819473743439,
      "learning_rate": 9.178356713426854e-05,
      "loss": 0.0204,
      "step": 543
    },
    {
      "epoch": 20.14814814814815,
      "grad_norm": 0.11766180396080017,
      "learning_rate": 9.158316633266534e-05,
      "loss": 0.0202,
      "step": 544
    },
    {
      "epoch": 20.185185185185187,
      "grad_norm": 0.12762971222400665,
      "learning_rate": 9.138276553106213e-05,
      "loss": 0.0244,
      "step": 545
    },
    {
      "epoch": 20.22222222222222,
      "grad_norm": 0.13239620625972748,
      "learning_rate": 9.118236472945892e-05,
      "loss": 0.0235,
      "step": 546
    },
    {
      "epoch": 20.25925925925926,
      "grad_norm": 0.10459516197443008,
      "learning_rate": 9.098196392785572e-05,
      "loss": 0.0203,
      "step": 547
    },
    {
      "epoch": 20.296296296296298,
      "grad_norm": 0.18712547421455383,
      "learning_rate": 9.078156312625251e-05,
      "loss": 0.0282,
      "step": 548
    },
    {
      "epoch": 20.333333333333332,
      "grad_norm": 0.3102279007434845,
      "learning_rate": 9.05811623246493e-05,
      "loss": 0.0271,
      "step": 549
    },
    {
      "epoch": 20.37037037037037,
      "grad_norm": 0.22944460809230804,
      "learning_rate": 9.03807615230461e-05,
      "loss": 0.0205,
      "step": 550
    },
    {
      "epoch": 20.40740740740741,
      "grad_norm": 0.17860883474349976,
      "learning_rate": 9.018036072144289e-05,
      "loss": 0.0219,
      "step": 551
    },
    {
      "epoch": 20.444444444444443,
      "grad_norm": 0.1343812644481659,
      "learning_rate": 8.997995991983969e-05,
      "loss": 0.0251,
      "step": 552
    },
    {
      "epoch": 20.48148148148148,
      "grad_norm": 0.1494176834821701,
      "learning_rate": 8.977955911823647e-05,
      "loss": 0.0275,
      "step": 553
    },
    {
      "epoch": 20.51851851851852,
      "grad_norm": 0.17362435162067413,
      "learning_rate": 8.957915831663327e-05,
      "loss": 0.0276,
      "step": 554
    },
    {
      "epoch": 20.555555555555557,
      "grad_norm": 0.1589856594800949,
      "learning_rate": 8.937875751503007e-05,
      "loss": 0.0225,
      "step": 555
    },
    {
      "epoch": 20.59259259259259,
      "grad_norm": 0.14996469020843506,
      "learning_rate": 8.917835671342686e-05,
      "loss": 0.0227,
      "step": 556
    },
    {
      "epoch": 20.62962962962963,
      "grad_norm": 0.35750651359558105,
      "learning_rate": 8.897795591182365e-05,
      "loss": 0.0256,
      "step": 557
    },
    {
      "epoch": 20.666666666666668,
      "grad_norm": 0.14026084542274475,
      "learning_rate": 8.877755511022044e-05,
      "loss": 0.0229,
      "step": 558
    },
    {
      "epoch": 20.703703703703702,
      "grad_norm": 0.11125456541776657,
      "learning_rate": 8.857715430861724e-05,
      "loss": 0.0201,
      "step": 559
    },
    {
      "epoch": 20.74074074074074,
      "grad_norm": 0.16254356503486633,
      "learning_rate": 8.837675350701404e-05,
      "loss": 0.0256,
      "step": 560
    },
    {
      "epoch": 20.77777777777778,
      "grad_norm": 0.13874681293964386,
      "learning_rate": 8.817635270541082e-05,
      "loss": 0.0235,
      "step": 561
    },
    {
      "epoch": 20.814814814814813,
      "grad_norm": 0.24964991211891174,
      "learning_rate": 8.797595190380762e-05,
      "loss": 0.0244,
      "step": 562
    },
    {
      "epoch": 20.85185185185185,
      "grad_norm": 0.12341505289077759,
      "learning_rate": 8.777555110220442e-05,
      "loss": 0.0248,
      "step": 563
    },
    {
      "epoch": 20.88888888888889,
      "grad_norm": 0.13993579149246216,
      "learning_rate": 8.75751503006012e-05,
      "loss": 0.0237,
      "step": 564
    },
    {
      "epoch": 20.925925925925927,
      "grad_norm": 0.16012033820152283,
      "learning_rate": 8.7374749498998e-05,
      "loss": 0.0286,
      "step": 565
    },
    {
      "epoch": 20.962962962962962,
      "grad_norm": 0.12359195947647095,
      "learning_rate": 8.717434869739479e-05,
      "loss": 0.0226,
      "step": 566
    },
    {
      "epoch": 21.0,
      "grad_norm": 0.17650088667869568,
      "learning_rate": 8.697394789579159e-05,
      "loss": 0.0307,
      "step": 567
    },
    {
      "epoch": 21.037037037037038,
      "grad_norm": 0.13578207790851593,
      "learning_rate": 8.677354709418838e-05,
      "loss": 0.0211,
      "step": 568
    },
    {
      "epoch": 21.074074074074073,
      "grad_norm": 0.14754505455493927,
      "learning_rate": 8.657314629258517e-05,
      "loss": 0.0232,
      "step": 569
    },
    {
      "epoch": 21.11111111111111,
      "grad_norm": 0.16151496767997742,
      "learning_rate": 8.637274549098197e-05,
      "loss": 0.0247,
      "step": 570
    },
    {
      "epoch": 21.14814814814815,
      "grad_norm": 0.17608730494976044,
      "learning_rate": 8.617234468937875e-05,
      "loss": 0.0254,
      "step": 571
    },
    {
      "epoch": 21.185185185185187,
      "grad_norm": 0.11415517330169678,
      "learning_rate": 8.597194388777555e-05,
      "loss": 0.0206,
      "step": 572
    },
    {
      "epoch": 21.22222222222222,
      "grad_norm": 0.10621779412031174,
      "learning_rate": 8.577154308617235e-05,
      "loss": 0.0212,
      "step": 573
    },
    {
      "epoch": 21.25925925925926,
      "grad_norm": 0.10087963938713074,
      "learning_rate": 8.557114228456913e-05,
      "loss": 0.019,
      "step": 574
    },
    {
      "epoch": 21.296296296296298,
      "grad_norm": 0.10898352414369583,
      "learning_rate": 8.537074148296593e-05,
      "loss": 0.0219,
      "step": 575
    },
    {
      "epoch": 21.333333333333332,
      "grad_norm": 0.13257169723510742,
      "learning_rate": 8.517034068136273e-05,
      "loss": 0.0215,
      "step": 576
    },
    {
      "epoch": 21.37037037037037,
      "grad_norm": 0.1218666136264801,
      "learning_rate": 8.496993987975952e-05,
      "loss": 0.0209,
      "step": 577
    },
    {
      "epoch": 21.40740740740741,
      "grad_norm": 0.11693620681762695,
      "learning_rate": 8.476953907815631e-05,
      "loss": 0.0218,
      "step": 578
    },
    {
      "epoch": 21.444444444444443,
      "grad_norm": 0.1073979064822197,
      "learning_rate": 8.45691382765531e-05,
      "loss": 0.0216,
      "step": 579
    },
    {
      "epoch": 21.48148148148148,
      "grad_norm": 0.11156482994556427,
      "learning_rate": 8.43687374749499e-05,
      "loss": 0.0209,
      "step": 580
    },
    {
      "epoch": 21.51851851851852,
      "grad_norm": 0.14311206340789795,
      "learning_rate": 8.41683366733467e-05,
      "loss": 0.025,
      "step": 581
    },
    {
      "epoch": 21.555555555555557,
      "grad_norm": 0.17928041517734528,
      "learning_rate": 8.396793587174348e-05,
      "loss": 0.028,
      "step": 582
    },
    {
      "epoch": 21.59259259259259,
      "grad_norm": 0.13132858276367188,
      "learning_rate": 8.376753507014028e-05,
      "loss": 0.0214,
      "step": 583
    },
    {
      "epoch": 21.62962962962963,
      "grad_norm": 0.12936438620090485,
      "learning_rate": 8.356713426853708e-05,
      "loss": 0.0216,
      "step": 584
    },
    {
      "epoch": 21.666666666666668,
      "grad_norm": 0.16591300070285797,
      "learning_rate": 8.336673346693386e-05,
      "loss": 0.029,
      "step": 585
    },
    {
      "epoch": 21.703703703703702,
      "grad_norm": 0.1182081326842308,
      "learning_rate": 8.316633266533068e-05,
      "loss": 0.0234,
      "step": 586
    },
    {
      "epoch": 21.74074074074074,
      "grad_norm": 0.1367853730916977,
      "learning_rate": 8.296593186372746e-05,
      "loss": 0.0256,
      "step": 587
    },
    {
      "epoch": 21.77777777777778,
      "grad_norm": 0.13238440454006195,
      "learning_rate": 8.276553106212426e-05,
      "loss": 0.0245,
      "step": 588
    },
    {
      "epoch": 21.814814814814813,
      "grad_norm": 0.09679504483938217,
      "learning_rate": 8.256513026052104e-05,
      "loss": 0.0184,
      "step": 589
    },
    {
      "epoch": 21.85185185185185,
      "grad_norm": 0.13203255832195282,
      "learning_rate": 8.236472945891784e-05,
      "loss": 0.0216,
      "step": 590
    },
    {
      "epoch": 21.88888888888889,
      "grad_norm": 0.151102676987648,
      "learning_rate": 8.216432865731464e-05,
      "loss": 0.0253,
      "step": 591
    },
    {
      "epoch": 21.925925925925927,
      "grad_norm": 0.13960985839366913,
      "learning_rate": 8.196392785571143e-05,
      "loss": 0.0271,
      "step": 592
    },
    {
      "epoch": 21.962962962962962,
      "grad_norm": 0.1435489058494568,
      "learning_rate": 8.176352705410823e-05,
      "loss": 0.0248,
      "step": 593
    },
    {
      "epoch": 22.0,
      "grad_norm": 0.1534138321876526,
      "learning_rate": 8.156312625250502e-05,
      "loss": 0.024,
      "step": 594
    },
    {
      "epoch": 22.037037037037038,
      "grad_norm": 0.11863797158002853,
      "learning_rate": 8.136272545090181e-05,
      "loss": 0.0206,
      "step": 595
    },
    {
      "epoch": 22.074074074074073,
      "grad_norm": 0.11639945954084396,
      "learning_rate": 8.116232464929861e-05,
      "loss": 0.0214,
      "step": 596
    },
    {
      "epoch": 22.11111111111111,
      "grad_norm": 0.13061970472335815,
      "learning_rate": 8.096192384769539e-05,
      "loss": 0.0237,
      "step": 597
    },
    {
      "epoch": 22.14814814814815,
      "grad_norm": 0.12110886722803116,
      "learning_rate": 8.076152304609219e-05,
      "loss": 0.0233,
      "step": 598
    },
    {
      "epoch": 22.185185185185187,
      "grad_norm": 0.1109849140048027,
      "learning_rate": 8.056112224448899e-05,
      "loss": 0.0213,
      "step": 599
    },
    {
      "epoch": 22.22222222222222,
      "grad_norm": 0.10948534309864044,
      "learning_rate": 8.036072144288577e-05,
      "loss": 0.0201,
      "step": 600
    },
    {
      "epoch": 22.25925925925926,
      "grad_norm": 0.11646151542663574,
      "learning_rate": 8.016032064128257e-05,
      "loss": 0.0194,
      "step": 601
    },
    {
      "epoch": 22.296296296296298,
      "grad_norm": 0.1193922609090805,
      "learning_rate": 7.995991983967936e-05,
      "loss": 0.0214,
      "step": 602
    },
    {
      "epoch": 22.333333333333332,
      "grad_norm": 0.11526831984519958,
      "learning_rate": 7.975951903807616e-05,
      "loss": 0.0226,
      "step": 603
    },
    {
      "epoch": 22.37037037037037,
      "grad_norm": 0.13005131483078003,
      "learning_rate": 7.955911823647296e-05,
      "loss": 0.0233,
      "step": 604
    },
    {
      "epoch": 22.40740740740741,
      "grad_norm": 0.12382899969816208,
      "learning_rate": 7.935871743486974e-05,
      "loss": 0.0235,
      "step": 605
    },
    {
      "epoch": 22.444444444444443,
      "grad_norm": 0.15598204731941223,
      "learning_rate": 7.915831663326654e-05,
      "loss": 0.0275,
      "step": 606
    },
    {
      "epoch": 22.48148148148148,
      "grad_norm": 0.1374807208776474,
      "learning_rate": 7.895791583166334e-05,
      "loss": 0.0234,
      "step": 607
    },
    {
      "epoch": 22.51851851851852,
      "grad_norm": 0.14161764085292816,
      "learning_rate": 7.875751503006012e-05,
      "loss": 0.0211,
      "step": 608
    },
    {
      "epoch": 22.555555555555557,
      "grad_norm": 0.1163550317287445,
      "learning_rate": 7.855711422845692e-05,
      "loss": 0.0193,
      "step": 609
    },
    {
      "epoch": 22.59259259259259,
      "grad_norm": 0.11394396424293518,
      "learning_rate": 7.83567134268537e-05,
      "loss": 0.0218,
      "step": 610
    },
    {
      "epoch": 22.62962962962963,
      "grad_norm": 0.15437938272953033,
      "learning_rate": 7.81563126252505e-05,
      "loss": 0.0261,
      "step": 611
    },
    {
      "epoch": 22.666666666666668,
      "grad_norm": 0.1556982845067978,
      "learning_rate": 7.79559118236473e-05,
      "loss": 0.0244,
      "step": 612
    },
    {
      "epoch": 22.703703703703702,
      "grad_norm": 0.11684628576040268,
      "learning_rate": 7.775551102204409e-05,
      "loss": 0.023,
      "step": 613
    },
    {
      "epoch": 22.74074074074074,
      "grad_norm": 0.12265589833259583,
      "learning_rate": 7.755511022044089e-05,
      "loss": 0.0242,
      "step": 614
    },
    {
      "epoch": 22.77777777777778,
      "grad_norm": 0.1350424885749817,
      "learning_rate": 7.735470941883769e-05,
      "loss": 0.0238,
      "step": 615
    },
    {
      "epoch": 22.814814814814813,
      "grad_norm": 0.1346915364265442,
      "learning_rate": 7.715430861723447e-05,
      "loss": 0.0238,
      "step": 616
    },
    {
      "epoch": 22.85185185185185,
      "grad_norm": 0.141080841422081,
      "learning_rate": 7.695390781563127e-05,
      "loss": 0.0234,
      "step": 617
    },
    {
      "epoch": 22.88888888888889,
      "grad_norm": 0.18990372121334076,
      "learning_rate": 7.675350701402805e-05,
      "loss": 0.0282,
      "step": 618
    },
    {
      "epoch": 22.925925925925927,
      "grad_norm": 0.11932280659675598,
      "learning_rate": 7.655310621242485e-05,
      "loss": 0.0203,
      "step": 619
    },
    {
      "epoch": 22.962962962962962,
      "grad_norm": 0.15157465636730194,
      "learning_rate": 7.635270541082165e-05,
      "loss": 0.0278,
      "step": 620
    },
    {
      "epoch": 23.0,
      "grad_norm": 0.13585489988327026,
      "learning_rate": 7.615230460921844e-05,
      "loss": 0.0231,
      "step": 621
    },
    {
      "epoch": 23.037037037037038,
      "grad_norm": 0.11118059605360031,
      "learning_rate": 7.595190380761523e-05,
      "loss": 0.0233,
      "step": 622
    },
    {
      "epoch": 23.074074074074073,
      "grad_norm": 0.13826662302017212,
      "learning_rate": 7.575150300601202e-05,
      "loss": 0.0226,
      "step": 623
    },
    {
      "epoch": 23.11111111111111,
      "grad_norm": 0.11165419965982437,
      "learning_rate": 7.555110220440882e-05,
      "loss": 0.0199,
      "step": 624
    },
    {
      "epoch": 23.14814814814815,
      "grad_norm": 0.09581878036260605,
      "learning_rate": 7.535070140280562e-05,
      "loss": 0.0168,
      "step": 625
    },
    {
      "epoch": 23.185185185185187,
      "grad_norm": 0.11209579557180405,
      "learning_rate": 7.51503006012024e-05,
      "loss": 0.0225,
      "step": 626
    },
    {
      "epoch": 23.22222222222222,
      "grad_norm": 0.11494775116443634,
      "learning_rate": 7.49498997995992e-05,
      "loss": 0.0228,
      "step": 627
    },
    {
      "epoch": 23.25925925925926,
      "grad_norm": 0.10430023074150085,
      "learning_rate": 7.4749498997996e-05,
      "loss": 0.0154,
      "step": 628
    },
    {
      "epoch": 23.296296296296298,
      "grad_norm": 0.09285532683134079,
      "learning_rate": 7.454909819639278e-05,
      "loss": 0.0179,
      "step": 629
    },
    {
      "epoch": 23.333333333333332,
      "grad_norm": 0.13687753677368164,
      "learning_rate": 7.434869739478958e-05,
      "loss": 0.0221,
      "step": 630
    },
    {
      "epoch": 23.37037037037037,
      "grad_norm": 0.14837731420993805,
      "learning_rate": 7.414829659318637e-05,
      "loss": 0.0248,
      "step": 631
    },
    {
      "epoch": 23.40740740740741,
      "grad_norm": 0.17128004133701324,
      "learning_rate": 7.394789579158317e-05,
      "loss": 0.0262,
      "step": 632
    },
    {
      "epoch": 23.444444444444443,
      "grad_norm": 0.13440854847431183,
      "learning_rate": 7.374749498997996e-05,
      "loss": 0.0246,
      "step": 633
    },
    {
      "epoch": 23.48148148148148,
      "grad_norm": 0.14474356174468994,
      "learning_rate": 7.354709418837675e-05,
      "loss": 0.0258,
      "step": 634
    },
    {
      "epoch": 23.51851851851852,
      "grad_norm": 0.11311326920986176,
      "learning_rate": 7.334669338677355e-05,
      "loss": 0.0227,
      "step": 635
    },
    {
      "epoch": 23.555555555555557,
      "grad_norm": 0.15368089079856873,
      "learning_rate": 7.314629258517035e-05,
      "loss": 0.024,
      "step": 636
    },
    {
      "epoch": 23.59259259259259,
      "grad_norm": 0.13051241636276245,
      "learning_rate": 7.294589178356713e-05,
      "loss": 0.0241,
      "step": 637
    },
    {
      "epoch": 23.62962962962963,
      "grad_norm": 0.10851992666721344,
      "learning_rate": 7.274549098196393e-05,
      "loss": 0.0188,
      "step": 638
    },
    {
      "epoch": 23.666666666666668,
      "grad_norm": 0.14317022264003754,
      "learning_rate": 7.254509018036071e-05,
      "loss": 0.0226,
      "step": 639
    },
    {
      "epoch": 23.703703703703702,
      "grad_norm": 0.14672763645648956,
      "learning_rate": 7.234468937875751e-05,
      "loss": 0.0248,
      "step": 640
    },
    {
      "epoch": 23.74074074074074,
      "grad_norm": 0.1308847814798355,
      "learning_rate": 7.214428857715431e-05,
      "loss": 0.0234,
      "step": 641
    },
    {
      "epoch": 23.77777777777778,
      "grad_norm": 0.10970897972583771,
      "learning_rate": 7.194388777555111e-05,
      "loss": 0.0213,
      "step": 642
    },
    {
      "epoch": 23.814814814814813,
      "grad_norm": 0.16006821393966675,
      "learning_rate": 7.174348697394791e-05,
      "loss": 0.0259,
      "step": 643
    },
    {
      "epoch": 23.85185185185185,
      "grad_norm": 0.16374357044696808,
      "learning_rate": 7.15430861723447e-05,
      "loss": 0.0281,
      "step": 644
    },
    {
      "epoch": 23.88888888888889,
      "grad_norm": 0.14340266585350037,
      "learning_rate": 7.134268537074149e-05,
      "loss": 0.0256,
      "step": 645
    },
    {
      "epoch": 23.925925925925927,
      "grad_norm": 0.13317134976387024,
      "learning_rate": 7.114228456913829e-05,
      "loss": 0.0244,
      "step": 646
    },
    {
      "epoch": 23.962962962962962,
      "grad_norm": 0.12370257824659348,
      "learning_rate": 7.094188376753508e-05,
      "loss": 0.0221,
      "step": 647
    },
    {
      "epoch": 24.0,
      "grad_norm": 0.18850912153720856,
      "learning_rate": 7.074148296593187e-05,
      "loss": 0.0265,
      "step": 648
    },
    {
      "epoch": 24.037037037037038,
      "grad_norm": 0.10530590265989304,
      "learning_rate": 7.054108216432866e-05,
      "loss": 0.0194,
      "step": 649
    },
    {
      "epoch": 24.074074074074073,
      "grad_norm": 0.0912913829088211,
      "learning_rate": 7.034068136272546e-05,
      "loss": 0.0201,
      "step": 650
    },
    {
      "epoch": 24.11111111111111,
      "grad_norm": 0.11250148713588715,
      "learning_rate": 7.014028056112226e-05,
      "loss": 0.0179,
      "step": 651
    },
    {
      "epoch": 24.14814814814815,
      "grad_norm": 0.1916513293981552,
      "learning_rate": 6.993987975951904e-05,
      "loss": 0.0267,
      "step": 652
    },
    {
      "epoch": 24.185185185185187,
      "grad_norm": 0.12128588557243347,
      "learning_rate": 6.973947895791584e-05,
      "loss": 0.0246,
      "step": 653
    },
    {
      "epoch": 24.22222222222222,
      "grad_norm": 0.12690936028957367,
      "learning_rate": 6.953907815631263e-05,
      "loss": 0.0217,
      "step": 654
    },
    {
      "epoch": 24.25925925925926,
      "grad_norm": 0.11542848497629166,
      "learning_rate": 6.933867735470942e-05,
      "loss": 0.0214,
      "step": 655
    },
    {
      "epoch": 24.296296296296298,
      "grad_norm": 0.11881420016288757,
      "learning_rate": 6.913827655310622e-05,
      "loss": 0.0223,
      "step": 656
    },
    {
      "epoch": 24.333333333333332,
      "grad_norm": 0.10533758252859116,
      "learning_rate": 6.893787575150301e-05,
      "loss": 0.0192,
      "step": 657
    },
    {
      "epoch": 24.37037037037037,
      "grad_norm": 0.11569146811962128,
      "learning_rate": 6.87374749498998e-05,
      "loss": 0.0207,
      "step": 658
    },
    {
      "epoch": 24.40740740740741,
      "grad_norm": 0.09750695526599884,
      "learning_rate": 6.85370741482966e-05,
      "loss": 0.0173,
      "step": 659
    },
    {
      "epoch": 24.444444444444443,
      "grad_norm": 0.12573349475860596,
      "learning_rate": 6.833667334669339e-05,
      "loss": 0.025,
      "step": 660
    },
    {
      "epoch": 24.48148148148148,
      "grad_norm": 0.12613800168037415,
      "learning_rate": 6.813627254509019e-05,
      "loss": 0.0232,
      "step": 661
    },
    {
      "epoch": 24.51851851851852,
      "grad_norm": 0.12137577682733536,
      "learning_rate": 6.793587174348697e-05,
      "loss": 0.0218,
      "step": 662
    },
    {
      "epoch": 24.555555555555557,
      "grad_norm": 0.1549111157655716,
      "learning_rate": 6.773547094188377e-05,
      "loss": 0.0247,
      "step": 663
    },
    {
      "epoch": 24.59259259259259,
      "grad_norm": 0.13752800226211548,
      "learning_rate": 6.753507014028057e-05,
      "loss": 0.0243,
      "step": 664
    },
    {
      "epoch": 24.62962962962963,
      "grad_norm": 0.18772989511489868,
      "learning_rate": 6.733466933867735e-05,
      "loss": 0.0278,
      "step": 665
    },
    {
      "epoch": 24.666666666666668,
      "grad_norm": 0.13678428530693054,
      "learning_rate": 6.713426853707415e-05,
      "loss": 0.024,
      "step": 666
    },
    {
      "epoch": 24.703703703703702,
      "grad_norm": 0.11719456315040588,
      "learning_rate": 6.693386773547095e-05,
      "loss": 0.0233,
      "step": 667
    },
    {
      "epoch": 24.74074074074074,
      "grad_norm": 0.10850589722394943,
      "learning_rate": 6.673346693386774e-05,
      "loss": 0.0176,
      "step": 668
    },
    {
      "epoch": 24.77777777777778,
      "grad_norm": 0.13086625933647156,
      "learning_rate": 6.653306613226454e-05,
      "loss": 0.024,
      "step": 669
    },
    {
      "epoch": 24.814814814814813,
      "grad_norm": 0.11695574223995209,
      "learning_rate": 6.633266533066132e-05,
      "loss": 0.0206,
      "step": 670
    },
    {
      "epoch": 24.85185185185185,
      "grad_norm": 0.1452474594116211,
      "learning_rate": 6.613226452905812e-05,
      "loss": 0.025,
      "step": 671
    },
    {
      "epoch": 24.88888888888889,
      "grad_norm": 0.15374472737312317,
      "learning_rate": 6.593186372745492e-05,
      "loss": 0.0284,
      "step": 672
    },
    {
      "epoch": 24.925925925925927,
      "grad_norm": 0.12292016297578812,
      "learning_rate": 6.57314629258517e-05,
      "loss": 0.0218,
      "step": 673
    },
    {
      "epoch": 24.962962962962962,
      "grad_norm": 0.13724680244922638,
      "learning_rate": 6.55310621242485e-05,
      "loss": 0.0242,
      "step": 674
    },
    {
      "epoch": 25.0,
      "grad_norm": 0.14408130943775177,
      "learning_rate": 6.533066132264529e-05,
      "loss": 0.0253,
      "step": 675
    },
    {
      "epoch": 25.037037037037038,
      "grad_norm": 0.10942663997411728,
      "learning_rate": 6.513026052104208e-05,
      "loss": 0.0191,
      "step": 676
    },
    {
      "epoch": 25.074074074074073,
      "grad_norm": 0.14202873408794403,
      "learning_rate": 6.492985971943888e-05,
      "loss": 0.0226,
      "step": 677
    },
    {
      "epoch": 25.11111111111111,
      "grad_norm": 0.12620551884174347,
      "learning_rate": 6.472945891783567e-05,
      "loss": 0.0202,
      "step": 678
    },
    {
      "epoch": 25.14814814814815,
      "grad_norm": 0.12622204422950745,
      "learning_rate": 6.452905811623247e-05,
      "loss": 0.0219,
      "step": 679
    },
    {
      "epoch": 25.185185185185187,
      "grad_norm": 0.14677803218364716,
      "learning_rate": 6.432865731462927e-05,
      "loss": 0.0259,
      "step": 680
    },
    {
      "epoch": 25.22222222222222,
      "grad_norm": 0.10894370824098587,
      "learning_rate": 6.412825651302605e-05,
      "loss": 0.0218,
      "step": 681
    },
    {
      "epoch": 25.25925925925926,
      "grad_norm": 0.1303981989622116,
      "learning_rate": 6.392785571142285e-05,
      "loss": 0.0225,
      "step": 682
    },
    {
      "epoch": 25.296296296296298,
      "grad_norm": 0.11140606552362442,
      "learning_rate": 6.372745490981963e-05,
      "loss": 0.0199,
      "step": 683
    },
    {
      "epoch": 25.333333333333332,
      "grad_norm": 0.12668538093566895,
      "learning_rate": 6.352705410821643e-05,
      "loss": 0.0262,
      "step": 684
    },
    {
      "epoch": 25.37037037037037,
      "grad_norm": 0.10957913100719452,
      "learning_rate": 6.332665330661323e-05,
      "loss": 0.0217,
      "step": 685
    },
    {
      "epoch": 25.40740740740741,
      "grad_norm": 0.1425139158964157,
      "learning_rate": 6.312625250501002e-05,
      "loss": 0.025,
      "step": 686
    },
    {
      "epoch": 25.444444444444443,
      "grad_norm": 0.10314993560314178,
      "learning_rate": 6.292585170340681e-05,
      "loss": 0.0196,
      "step": 687
    },
    {
      "epoch": 25.48148148148148,
      "grad_norm": 0.13583821058273315,
      "learning_rate": 6.27254509018036e-05,
      "loss": 0.0238,
      "step": 688
    },
    {
      "epoch": 25.51851851851852,
      "grad_norm": 0.10838653147220612,
      "learning_rate": 6.25250501002004e-05,
      "loss": 0.0214,
      "step": 689
    },
    {
      "epoch": 25.555555555555557,
      "grad_norm": 0.10499076545238495,
      "learning_rate": 6.23246492985972e-05,
      "loss": 0.019,
      "step": 690
    },
    {
      "epoch": 25.59259259259259,
      "grad_norm": 0.10719458013772964,
      "learning_rate": 6.212424849699398e-05,
      "loss": 0.022,
      "step": 691
    },
    {
      "epoch": 25.62962962962963,
      "grad_norm": 0.14451928436756134,
      "learning_rate": 6.192384769539078e-05,
      "loss": 0.0227,
      "step": 692
    },
    {
      "epoch": 25.666666666666668,
      "grad_norm": 0.10402344912290573,
      "learning_rate": 6.172344689378758e-05,
      "loss": 0.0202,
      "step": 693
    },
    {
      "epoch": 25.703703703703702,
      "grad_norm": 0.21030086278915405,
      "learning_rate": 6.152304609218436e-05,
      "loss": 0.0262,
      "step": 694
    },
    {
      "epoch": 25.74074074074074,
      "grad_norm": 0.10798510164022446,
      "learning_rate": 6.132264529058116e-05,
      "loss": 0.0188,
      "step": 695
    },
    {
      "epoch": 25.77777777777778,
      "grad_norm": 0.13429920375347137,
      "learning_rate": 6.112224448897795e-05,
      "loss": 0.0253,
      "step": 696
    },
    {
      "epoch": 25.814814814814813,
      "grad_norm": 0.1226969063282013,
      "learning_rate": 6.092184368737476e-05,
      "loss": 0.0214,
      "step": 697
    },
    {
      "epoch": 25.85185185185185,
      "grad_norm": 0.1374455839395523,
      "learning_rate": 6.072144288577155e-05,
      "loss": 0.0269,
      "step": 698
    },
    {
      "epoch": 25.88888888888889,
      "grad_norm": 0.14624741673469543,
      "learning_rate": 6.052104208416834e-05,
      "loss": 0.0235,
      "step": 699
    },
    {
      "epoch": 25.925925925925927,
      "grad_norm": 0.14635340869426727,
      "learning_rate": 6.0320641282565135e-05,
      "loss": 0.0231,
      "step": 700
    },
    {
      "epoch": 25.962962962962962,
      "grad_norm": 0.12397536635398865,
      "learning_rate": 6.012024048096193e-05,
      "loss": 0.0217,
      "step": 701
    },
    {
      "epoch": 26.0,
      "grad_norm": 0.14542090892791748,
      "learning_rate": 5.9919839679358725e-05,
      "loss": 0.025,
      "step": 702
    },
    {
      "epoch": 26.037037037037038,
      "grad_norm": 0.11637924611568451,
      "learning_rate": 5.971943887775552e-05,
      "loss": 0.0234,
      "step": 703
    },
    {
      "epoch": 26.074074074074073,
      "grad_norm": 0.12965011596679688,
      "learning_rate": 5.951903807615231e-05,
      "loss": 0.0218,
      "step": 704
    },
    {
      "epoch": 26.11111111111111,
      "grad_norm": 0.16893370449543,
      "learning_rate": 5.931863727454911e-05,
      "loss": 0.0251,
      "step": 705
    },
    {
      "epoch": 26.14814814814815,
      "grad_norm": 0.11603313684463501,
      "learning_rate": 5.91182364729459e-05,
      "loss": 0.019,
      "step": 706
    },
    {
      "epoch": 26.185185185185187,
      "grad_norm": 0.11152876913547516,
      "learning_rate": 5.891783567134269e-05,
      "loss": 0.0219,
      "step": 707
    },
    {
      "epoch": 26.22222222222222,
      "grad_norm": 0.13950496912002563,
      "learning_rate": 5.871743486973948e-05,
      "loss": 0.0211,
      "step": 708
    },
    {
      "epoch": 26.25925925925926,
      "grad_norm": 0.12337025254964828,
      "learning_rate": 5.851703406813628e-05,
      "loss": 0.0234,
      "step": 709
    },
    {
      "epoch": 26.296296296296298,
      "grad_norm": 0.1494603157043457,
      "learning_rate": 5.831663326653307e-05,
      "loss": 0.0235,
      "step": 710
    },
    {
      "epoch": 26.333333333333332,
      "grad_norm": 0.1211848184466362,
      "learning_rate": 5.8116232464929865e-05,
      "loss": 0.0219,
      "step": 711
    },
    {
      "epoch": 26.37037037037037,
      "grad_norm": 0.12664096057415009,
      "learning_rate": 5.7915831663326656e-05,
      "loss": 0.0236,
      "step": 712
    },
    {
      "epoch": 26.40740740740741,
      "grad_norm": 0.11549990624189377,
      "learning_rate": 5.7715430861723455e-05,
      "loss": 0.0214,
      "step": 713
    },
    {
      "epoch": 26.444444444444443,
      "grad_norm": 0.11431502550840378,
      "learning_rate": 5.7515030060120247e-05,
      "loss": 0.0202,
      "step": 714
    },
    {
      "epoch": 26.48148148148148,
      "grad_norm": 0.11189300566911697,
      "learning_rate": 5.731462925851704e-05,
      "loss": 0.0194,
      "step": 715
    },
    {
      "epoch": 26.51851851851852,
      "grad_norm": 0.1188134253025055,
      "learning_rate": 5.711422845691383e-05,
      "loss": 0.0208,
      "step": 716
    },
    {
      "epoch": 26.555555555555557,
      "grad_norm": 0.17237305641174316,
      "learning_rate": 5.691382765531063e-05,
      "loss": 0.0275,
      "step": 717
    },
    {
      "epoch": 26.59259259259259,
      "grad_norm": 0.09847073256969452,
      "learning_rate": 5.671342685370742e-05,
      "loss": 0.0176,
      "step": 718
    },
    {
      "epoch": 26.62962962962963,
      "grad_norm": 0.12062685936689377,
      "learning_rate": 5.651302605210421e-05,
      "loss": 0.0202,
      "step": 719
    },
    {
      "epoch": 26.666666666666668,
      "grad_norm": 0.1216929703950882,
      "learning_rate": 5.6312625250501004e-05,
      "loss": 0.0236,
      "step": 720
    },
    {
      "epoch": 26.703703703703702,
      "grad_norm": 0.1180981993675232,
      "learning_rate": 5.6112224448897796e-05,
      "loss": 0.0212,
      "step": 721
    },
    {
      "epoch": 26.74074074074074,
      "grad_norm": 0.1312122493982315,
      "learning_rate": 5.5911823647294594e-05,
      "loss": 0.0266,
      "step": 722
    },
    {
      "epoch": 26.77777777777778,
      "grad_norm": 0.10800334811210632,
      "learning_rate": 5.5711422845691386e-05,
      "loss": 0.0201,
      "step": 723
    },
    {
      "epoch": 26.814814814814813,
      "grad_norm": 0.13821972906589508,
      "learning_rate": 5.551102204408818e-05,
      "loss": 0.021,
      "step": 724
    },
    {
      "epoch": 26.85185185185185,
      "grad_norm": 0.11936486512422562,
      "learning_rate": 5.531062124248497e-05,
      "loss": 0.0216,
      "step": 725
    },
    {
      "epoch": 26.88888888888889,
      "grad_norm": 0.1576913744211197,
      "learning_rate": 5.511022044088177e-05,
      "loss": 0.027,
      "step": 726
    },
    {
      "epoch": 26.925925925925927,
      "grad_norm": 0.1251457929611206,
      "learning_rate": 5.490981963927856e-05,
      "loss": 0.0243,
      "step": 727
    },
    {
      "epoch": 26.962962962962962,
      "grad_norm": 0.13425050675868988,
      "learning_rate": 5.470941883767535e-05,
      "loss": 0.0255,
      "step": 728
    },
    {
      "epoch": 27.0,
      "grad_norm": 0.12178883701562881,
      "learning_rate": 5.4509018036072143e-05,
      "loss": 0.0249,
      "step": 729
    },
    {
      "epoch": 27.037037037037038,
      "grad_norm": 0.14490807056427002,
      "learning_rate": 5.430861723446894e-05,
      "loss": 0.0234,
      "step": 730
    },
    {
      "epoch": 27.074074074074073,
      "grad_norm": 0.12491891533136368,
      "learning_rate": 5.4108216432865734e-05,
      "loss": 0.0211,
      "step": 731
    },
    {
      "epoch": 27.11111111111111,
      "grad_norm": 0.1077302098274231,
      "learning_rate": 5.3907815631262526e-05,
      "loss": 0.0197,
      "step": 732
    },
    {
      "epoch": 27.14814814814815,
      "grad_norm": 0.14077065885066986,
      "learning_rate": 5.370741482965932e-05,
      "loss": 0.0253,
      "step": 733
    },
    {
      "epoch": 27.185185185185187,
      "grad_norm": 0.14127109944820404,
      "learning_rate": 5.3507014028056116e-05,
      "loss": 0.0247,
      "step": 734
    },
    {
      "epoch": 27.22222222222222,
      "grad_norm": 0.12198808789253235,
      "learning_rate": 5.330661322645291e-05,
      "loss": 0.0236,
      "step": 735
    },
    {
      "epoch": 27.25925925925926,
      "grad_norm": 0.09612884372472763,
      "learning_rate": 5.31062124248497e-05,
      "loss": 0.0172,
      "step": 736
    },
    {
      "epoch": 27.296296296296298,
      "grad_norm": 0.10322828590869904,
      "learning_rate": 5.290581162324649e-05,
      "loss": 0.0205,
      "step": 737
    },
    {
      "epoch": 27.333333333333332,
      "grad_norm": 0.12926390767097473,
      "learning_rate": 5.270541082164328e-05,
      "loss": 0.0223,
      "step": 738
    },
    {
      "epoch": 27.37037037037037,
      "grad_norm": 0.1559116095304489,
      "learning_rate": 5.250501002004008e-05,
      "loss": 0.0251,
      "step": 739
    },
    {
      "epoch": 27.40740740740741,
      "grad_norm": 0.11306753754615784,
      "learning_rate": 5.230460921843687e-05,
      "loss": 0.0189,
      "step": 740
    },
    {
      "epoch": 27.444444444444443,
      "grad_norm": 0.1310882717370987,
      "learning_rate": 5.2104208416833665e-05,
      "loss": 0.0237,
      "step": 741
    },
    {
      "epoch": 27.48148148148148,
      "grad_norm": 0.12399282306432724,
      "learning_rate": 5.190380761523046e-05,
      "loss": 0.021,
      "step": 742
    },
    {
      "epoch": 27.51851851851852,
      "grad_norm": 0.11026856303215027,
      "learning_rate": 5.1703406813627255e-05,
      "loss": 0.0194,
      "step": 743
    },
    {
      "epoch": 27.555555555555557,
      "grad_norm": 0.12768156826496124,
      "learning_rate": 5.150300601202405e-05,
      "loss": 0.0227,
      "step": 744
    },
    {
      "epoch": 27.59259259259259,
      "grad_norm": 0.10362526029348373,
      "learning_rate": 5.130260521042084e-05,
      "loss": 0.021,
      "step": 745
    },
    {
      "epoch": 27.62962962962963,
      "grad_norm": 0.14409653842449188,
      "learning_rate": 5.110220440881763e-05,
      "loss": 0.0261,
      "step": 746
    },
    {
      "epoch": 27.666666666666668,
      "grad_norm": 0.12788249552249908,
      "learning_rate": 5.090180360721443e-05,
      "loss": 0.0219,
      "step": 747
    },
    {
      "epoch": 27.703703703703702,
      "grad_norm": 0.10368750244379044,
      "learning_rate": 5.070140280561122e-05,
      "loss": 0.0186,
      "step": 748
    },
    {
      "epoch": 27.74074074074074,
      "grad_norm": 0.13793909549713135,
      "learning_rate": 5.050100200400801e-05,
      "loss": 0.0267,
      "step": 749
    },
    {
      "epoch": 27.77777777777778,
      "grad_norm": 0.09323755651712418,
      "learning_rate": 5.0300601202404805e-05,
      "loss": 0.0196,
      "step": 750
    },
    {
      "epoch": 27.814814814814813,
      "grad_norm": 0.12050902843475342,
      "learning_rate": 5.01002004008016e-05,
      "loss": 0.0228,
      "step": 751
    },
    {
      "epoch": 27.85185185185185,
      "grad_norm": 0.15089069306850433,
      "learning_rate": 4.98997995991984e-05,
      "loss": 0.0257,
      "step": 752
    },
    {
      "epoch": 27.88888888888889,
      "grad_norm": 0.16015440225601196,
      "learning_rate": 4.9699398797595193e-05,
      "loss": 0.0257,
      "step": 753
    },
    {
      "epoch": 27.925925925925927,
      "grad_norm": 0.1409383863210678,
      "learning_rate": 4.9498997995991985e-05,
      "loss": 0.026,
      "step": 754
    },
    {
      "epoch": 27.962962962962962,
      "grad_norm": 0.11602102965116501,
      "learning_rate": 4.929859719438878e-05,
      "loss": 0.0227,
      "step": 755
    },
    {
      "epoch": 28.0,
      "grad_norm": 0.08299291878938675,
      "learning_rate": 4.9098196392785576e-05,
      "loss": 0.0157,
      "step": 756
    },
    {
      "epoch": 28.037037037037038,
      "grad_norm": 0.12254366278648376,
      "learning_rate": 4.889779559118237e-05,
      "loss": 0.0211,
      "step": 757
    },
    {
      "epoch": 28.074074074074073,
      "grad_norm": 0.15464158356189728,
      "learning_rate": 4.869739478957916e-05,
      "loss": 0.0245,
      "step": 758
    },
    {
      "epoch": 28.11111111111111,
      "grad_norm": 0.09997693449258804,
      "learning_rate": 4.849699398797595e-05,
      "loss": 0.0168,
      "step": 759
    },
    {
      "epoch": 28.14814814814815,
      "grad_norm": 0.12702712416648865,
      "learning_rate": 4.829659318637275e-05,
      "loss": 0.0229,
      "step": 760
    },
    {
      "epoch": 28.185185185185187,
      "grad_norm": 0.148958221077919,
      "learning_rate": 4.809619238476954e-05,
      "loss": 0.0245,
      "step": 761
    },
    {
      "epoch": 28.22222222222222,
      "grad_norm": 0.11258652061223984,
      "learning_rate": 4.789579158316633e-05,
      "loss": 0.0207,
      "step": 762
    },
    {
      "epoch": 28.25925925925926,
      "grad_norm": 0.13560080528259277,
      "learning_rate": 4.7695390781563125e-05,
      "loss": 0.0238,
      "step": 763
    },
    {
      "epoch": 28.296296296296298,
      "grad_norm": 0.12687471508979797,
      "learning_rate": 4.7494989979959916e-05,
      "loss": 0.0217,
      "step": 764
    },
    {
      "epoch": 28.333333333333332,
      "grad_norm": 0.13901302218437195,
      "learning_rate": 4.7294589178356715e-05,
      "loss": 0.0235,
      "step": 765
    },
    {
      "epoch": 28.37037037037037,
      "grad_norm": 0.12470946460962296,
      "learning_rate": 4.7094188376753514e-05,
      "loss": 0.0196,
      "step": 766
    },
    {
      "epoch": 28.40740740740741,
      "grad_norm": 0.13360358774662018,
      "learning_rate": 4.6893787575150305e-05,
      "loss": 0.0259,
      "step": 767
    },
    {
      "epoch": 28.444444444444443,
      "grad_norm": 0.10590317100286484,
      "learning_rate": 4.66933867735471e-05,
      "loss": 0.0208,
      "step": 768
    },
    {
      "epoch": 28.48148148148148,
      "grad_norm": 0.11734151095151901,
      "learning_rate": 4.649298597194389e-05,
      "loss": 0.0221,
      "step": 769
    },
    {
      "epoch": 28.51851851851852,
      "grad_norm": 0.11749283224344254,
      "learning_rate": 4.629258517034069e-05,
      "loss": 0.0218,
      "step": 770
    },
    {
      "epoch": 28.555555555555557,
      "grad_norm": 0.1398768126964569,
      "learning_rate": 4.609218436873748e-05,
      "loss": 0.0205,
      "step": 771
    },
    {
      "epoch": 28.59259259259259,
      "grad_norm": 0.09294318407773972,
      "learning_rate": 4.589178356713427e-05,
      "loss": 0.0184,
      "step": 772
    },
    {
      "epoch": 28.62962962962963,
      "grad_norm": 0.10243476182222366,
      "learning_rate": 4.569138276553106e-05,
      "loss": 0.0231,
      "step": 773
    },
    {
      "epoch": 28.666666666666668,
      "grad_norm": 0.13858994841575623,
      "learning_rate": 4.549098196392786e-05,
      "loss": 0.0222,
      "step": 774
    },
    {
      "epoch": 28.703703703703702,
      "grad_norm": 0.1248805820941925,
      "learning_rate": 4.529058116232465e-05,
      "loss": 0.0219,
      "step": 775
    },
    {
      "epoch": 28.74074074074074,
      "grad_norm": 0.11493215709924698,
      "learning_rate": 4.5090180360721445e-05,
      "loss": 0.0198,
      "step": 776
    },
    {
      "epoch": 28.77777777777778,
      "grad_norm": 0.1423059105873108,
      "learning_rate": 4.488977955911824e-05,
      "loss": 0.0218,
      "step": 777
    },
    {
      "epoch": 28.814814814814813,
      "grad_norm": 0.10683955997228622,
      "learning_rate": 4.4689378757515035e-05,
      "loss": 0.0223,
      "step": 778
    },
    {
      "epoch": 28.85185185185185,
      "grad_norm": 0.1464165300130844,
      "learning_rate": 4.448897795591183e-05,
      "loss": 0.0284,
      "step": 779
    },
    {
      "epoch": 28.88888888888889,
      "grad_norm": 0.14034102857112885,
      "learning_rate": 4.428857715430862e-05,
      "loss": 0.0243,
      "step": 780
    },
    {
      "epoch": 28.925925925925927,
      "grad_norm": 0.1050499677658081,
      "learning_rate": 4.408817635270541e-05,
      "loss": 0.0156,
      "step": 781
    },
    {
      "epoch": 28.962962962962962,
      "grad_norm": 0.12857648730278015,
      "learning_rate": 4.388777555110221e-05,
      "loss": 0.0261,
      "step": 782
    },
    {
      "epoch": 29.0,
      "grad_norm": 0.12818214297294617,
      "learning_rate": 4.3687374749499e-05,
      "loss": 0.0211,
      "step": 783
    },
    {
      "epoch": 29.037037037037038,
      "grad_norm": 0.1145898625254631,
      "learning_rate": 4.348697394789579e-05,
      "loss": 0.0215,
      "step": 784
    },
    {
      "epoch": 29.074074074074073,
      "grad_norm": 0.10299823433160782,
      "learning_rate": 4.3286573146292584e-05,
      "loss": 0.0174,
      "step": 785
    },
    {
      "epoch": 29.11111111111111,
      "grad_norm": 0.12267718464136124,
      "learning_rate": 4.3086172344689376e-05,
      "loss": 0.0202,
      "step": 786
    },
    {
      "epoch": 29.14814814814815,
      "grad_norm": 0.10469891130924225,
      "learning_rate": 4.2885771543086175e-05,
      "loss": 0.02,
      "step": 787
    },
    {
      "epoch": 29.185185185185187,
      "grad_norm": 0.11210443079471588,
      "learning_rate": 4.2685370741482966e-05,
      "loss": 0.022,
      "step": 788
    },
    {
      "epoch": 29.22222222222222,
      "grad_norm": 0.11387807130813599,
      "learning_rate": 4.248496993987976e-05,
      "loss": 0.0189,
      "step": 789
    },
    {
      "epoch": 29.25925925925926,
      "grad_norm": 0.10538382083177567,
      "learning_rate": 4.228456913827655e-05,
      "loss": 0.022,
      "step": 790
    },
    {
      "epoch": 29.296296296296298,
      "grad_norm": 0.0974680483341217,
      "learning_rate": 4.208416833667335e-05,
      "loss": 0.0186,
      "step": 791
    },
    {
      "epoch": 29.333333333333332,
      "grad_norm": 0.10920742899179459,
      "learning_rate": 4.188376753507014e-05,
      "loss": 0.0225,
      "step": 792
    },
    {
      "epoch": 29.37037037037037,
      "grad_norm": 0.10204175859689713,
      "learning_rate": 4.168336673346693e-05,
      "loss": 0.0177,
      "step": 793
    },
    {
      "epoch": 29.40740740740741,
      "grad_norm": 0.10397164523601532,
      "learning_rate": 4.148296593186373e-05,
      "loss": 0.0195,
      "step": 794
    },
    {
      "epoch": 29.444444444444443,
      "grad_norm": 0.12552697956562042,
      "learning_rate": 4.128256513026052e-05,
      "loss": 0.0217,
      "step": 795
    },
    {
      "epoch": 29.48148148148148,
      "grad_norm": 0.10960248112678528,
      "learning_rate": 4.108216432865732e-05,
      "loss": 0.0201,
      "step": 796
    },
    {
      "epoch": 29.51851851851852,
      "grad_norm": 0.12575161457061768,
      "learning_rate": 4.088176352705411e-05,
      "loss": 0.0211,
      "step": 797
    },
    {
      "epoch": 29.555555555555557,
      "grad_norm": 0.1191244125366211,
      "learning_rate": 4.0681362725450904e-05,
      "loss": 0.022,
      "step": 798
    },
    {
      "epoch": 29.59259259259259,
      "grad_norm": 0.1492454707622528,
      "learning_rate": 4.0480961923847696e-05,
      "loss": 0.0204,
      "step": 799
    },
    {
      "epoch": 29.62962962962963,
      "grad_norm": 0.14364421367645264,
      "learning_rate": 4.0280561122244495e-05,
      "loss": 0.0262,
      "step": 800
    },
    {
      "epoch": 29.666666666666668,
      "grad_norm": 0.12845928966999054,
      "learning_rate": 4.0080160320641287e-05,
      "loss": 0.0225,
      "step": 801
    },
    {
      "epoch": 29.703703703703702,
      "grad_norm": 0.1213652640581131,
      "learning_rate": 3.987975951903808e-05,
      "loss": 0.0234,
      "step": 802
    },
    {
      "epoch": 29.74074074074074,
      "grad_norm": 0.11829835921525955,
      "learning_rate": 3.967935871743487e-05,
      "loss": 0.0219,
      "step": 803
    },
    {
      "epoch": 29.77777777777778,
      "grad_norm": 0.12840600311756134,
      "learning_rate": 3.947895791583167e-05,
      "loss": 0.0236,
      "step": 804
    },
    {
      "epoch": 29.814814814814813,
      "grad_norm": 0.1625537872314453,
      "learning_rate": 3.927855711422846e-05,
      "loss": 0.0264,
      "step": 805
    },
    {
      "epoch": 29.85185185185185,
      "grad_norm": 0.1570960432291031,
      "learning_rate": 3.907815631262525e-05,
      "loss": 0.0274,
      "step": 806
    },
    {
      "epoch": 29.88888888888889,
      "grad_norm": 0.1121605783700943,
      "learning_rate": 3.8877755511022044e-05,
      "loss": 0.0209,
      "step": 807
    },
    {
      "epoch": 29.925925925925927,
      "grad_norm": 0.1242719367146492,
      "learning_rate": 3.867735470941884e-05,
      "loss": 0.0232,
      "step": 808
    },
    {
      "epoch": 29.962962962962962,
      "grad_norm": 0.17524023354053497,
      "learning_rate": 3.8476953907815634e-05,
      "loss": 0.0298,
      "step": 809
    },
    {
      "epoch": 30.0,
      "grad_norm": 0.11426681280136108,
      "learning_rate": 3.8276553106212426e-05,
      "loss": 0.0204,
      "step": 810
    },
    {
      "epoch": 30.037037037037038,
      "grad_norm": 0.13320808112621307,
      "learning_rate": 3.807615230460922e-05,
      "loss": 0.0243,
      "step": 811
    },
    {
      "epoch": 30.074074074074073,
      "grad_norm": 0.09758416563272476,
      "learning_rate": 3.787575150300601e-05,
      "loss": 0.019,
      "step": 812
    },
    {
      "epoch": 30.11111111111111,
      "grad_norm": 0.12119023501873016,
      "learning_rate": 3.767535070140281e-05,
      "loss": 0.0214,
      "step": 813
    },
    {
      "epoch": 30.14814814814815,
      "grad_norm": 0.10509318858385086,
      "learning_rate": 3.74749498997996e-05,
      "loss": 0.0199,
      "step": 814
    },
    {
      "epoch": 30.185185185185187,
      "grad_norm": 0.11631841957569122,
      "learning_rate": 3.727454909819639e-05,
      "loss": 0.0211,
      "step": 815
    },
    {
      "epoch": 30.22222222222222,
      "grad_norm": 0.11893568187952042,
      "learning_rate": 3.7074148296593183e-05,
      "loss": 0.0222,
      "step": 816
    },
    {
      "epoch": 30.25925925925926,
      "grad_norm": 0.09955564886331558,
      "learning_rate": 3.687374749498998e-05,
      "loss": 0.0193,
      "step": 817
    },
    {
      "epoch": 30.296296296296298,
      "grad_norm": 0.12098170816898346,
      "learning_rate": 3.6673346693386774e-05,
      "loss": 0.0236,
      "step": 818
    },
    {
      "epoch": 30.333333333333332,
      "grad_norm": 0.1304439902305603,
      "learning_rate": 3.6472945891783566e-05,
      "loss": 0.0223,
      "step": 819
    },
    {
      "epoch": 30.37037037037037,
      "grad_norm": 0.10944367945194244,
      "learning_rate": 3.627254509018036e-05,
      "loss": 0.018,
      "step": 820
    },
    {
      "epoch": 30.40740740740741,
      "grad_norm": 0.15779706835746765,
      "learning_rate": 3.6072144288577156e-05,
      "loss": 0.0229,
      "step": 821
    },
    {
      "epoch": 30.444444444444443,
      "grad_norm": 0.07527511566877365,
      "learning_rate": 3.5871743486973954e-05,
      "loss": 0.0172,
      "step": 822
    },
    {
      "epoch": 30.48148148148148,
      "grad_norm": 0.14506907761096954,
      "learning_rate": 3.5671342685370746e-05,
      "loss": 0.0235,
      "step": 823
    },
    {
      "epoch": 30.51851851851852,
      "grad_norm": 0.13856640458106995,
      "learning_rate": 3.547094188376754e-05,
      "loss": 0.0265,
      "step": 824
    },
    {
      "epoch": 30.555555555555557,
      "grad_norm": 0.1421939730644226,
      "learning_rate": 3.527054108216433e-05,
      "loss": 0.023,
      "step": 825
    },
    {
      "epoch": 30.59259259259259,
      "grad_norm": 0.10106267035007477,
      "learning_rate": 3.507014028056113e-05,
      "loss": 0.018,
      "step": 826
    },
    {
      "epoch": 30.62962962962963,
      "grad_norm": 0.1340084969997406,
      "learning_rate": 3.486973947895792e-05,
      "loss": 0.0234,
      "step": 827
    },
    {
      "epoch": 30.666666666666668,
      "grad_norm": 0.11651624739170074,
      "learning_rate": 3.466933867735471e-05,
      "loss": 0.0196,
      "step": 828
    },
    {
      "epoch": 30.703703703703702,
      "grad_norm": 0.1539447158575058,
      "learning_rate": 3.4468937875751504e-05,
      "loss": 0.0268,
      "step": 829
    },
    {
      "epoch": 30.74074074074074,
      "grad_norm": 0.11960262805223465,
      "learning_rate": 3.42685370741483e-05,
      "loss": 0.0223,
      "step": 830
    },
    {
      "epoch": 30.77777777777778,
      "grad_norm": 0.13391391932964325,
      "learning_rate": 3.4068136272545094e-05,
      "loss": 0.0233,
      "step": 831
    },
    {
      "epoch": 30.814814814814813,
      "grad_norm": 0.11242396384477615,
      "learning_rate": 3.3867735470941886e-05,
      "loss": 0.0206,
      "step": 832
    },
    {
      "epoch": 30.85185185185185,
      "grad_norm": 0.1203906238079071,
      "learning_rate": 3.366733466933868e-05,
      "loss": 0.019,
      "step": 833
    },
    {
      "epoch": 30.88888888888889,
      "grad_norm": 0.11147458106279373,
      "learning_rate": 3.3466933867735476e-05,
      "loss": 0.0189,
      "step": 834
    },
    {
      "epoch": 30.925925925925927,
      "grad_norm": 0.14842064678668976,
      "learning_rate": 3.326653306613227e-05,
      "loss": 0.0237,
      "step": 835
    },
    {
      "epoch": 30.962962962962962,
      "grad_norm": 0.1528906226158142,
      "learning_rate": 3.306613226452906e-05,
      "loss": 0.0241,
      "step": 836
    },
    {
      "epoch": 31.0,
      "grad_norm": 0.13834241032600403,
      "learning_rate": 3.286573146292585e-05,
      "loss": 0.0251,
      "step": 837
    },
    {
      "epoch": 31.037037037037038,
      "grad_norm": 0.09814367443323135,
      "learning_rate": 3.266533066132264e-05,
      "loss": 0.0189,
      "step": 838
    },
    {
      "epoch": 31.074074074074073,
      "grad_norm": 0.12543442845344543,
      "learning_rate": 3.246492985971944e-05,
      "loss": 0.0259,
      "step": 839
    },
    {
      "epoch": 31.11111111111111,
      "grad_norm": 0.10042287409305573,
      "learning_rate": 3.2264529058116233e-05,
      "loss": 0.018,
      "step": 840
    },
    {
      "epoch": 31.14814814814815,
      "grad_norm": 0.12029018998146057,
      "learning_rate": 3.2064128256513025e-05,
      "loss": 0.0211,
      "step": 841
    },
    {
      "epoch": 31.185185185185187,
      "grad_norm": 0.117877297103405,
      "learning_rate": 3.186372745490982e-05,
      "loss": 0.0219,
      "step": 842
    },
    {
      "epoch": 31.22222222222222,
      "grad_norm": 0.11280486732721329,
      "learning_rate": 3.1663326653306616e-05,
      "loss": 0.0192,
      "step": 843
    },
    {
      "epoch": 31.25925925925926,
      "grad_norm": 0.11535570025444031,
      "learning_rate": 3.146292585170341e-05,
      "loss": 0.0198,
      "step": 844
    },
    {
      "epoch": 31.296296296296298,
      "grad_norm": 0.12578098475933075,
      "learning_rate": 3.12625250501002e-05,
      "loss": 0.0204,
      "step": 845
    },
    {
      "epoch": 31.333333333333332,
      "grad_norm": 0.1406942754983902,
      "learning_rate": 3.106212424849699e-05,
      "loss": 0.019,
      "step": 846
    },
    {
      "epoch": 31.37037037037037,
      "grad_norm": 0.1435045599937439,
      "learning_rate": 3.086172344689379e-05,
      "loss": 0.0219,
      "step": 847
    },
    {
      "epoch": 31.40740740740741,
      "grad_norm": 0.12756717205047607,
      "learning_rate": 3.066132264529058e-05,
      "loss": 0.0227,
      "step": 848
    },
    {
      "epoch": 31.444444444444443,
      "grad_norm": 0.11470496654510498,
      "learning_rate": 3.046092184368738e-05,
      "loss": 0.0201,
      "step": 849
    },
    {
      "epoch": 31.48148148148148,
      "grad_norm": 0.11585598438978195,
      "learning_rate": 3.026052104208417e-05,
      "loss": 0.0184,
      "step": 850
    },
    {
      "epoch": 31.51851851851852,
      "grad_norm": 0.11298004537820816,
      "learning_rate": 3.0060120240480967e-05,
      "loss": 0.0228,
      "step": 851
    },
    {
      "epoch": 31.555555555555557,
      "grad_norm": 0.12406837940216064,
      "learning_rate": 2.985971943887776e-05,
      "loss": 0.0202,
      "step": 852
    },
    {
      "epoch": 31.59259259259259,
      "grad_norm": 0.1165970042347908,
      "learning_rate": 2.9659318637274554e-05,
      "loss": 0.0208,
      "step": 853
    },
    {
      "epoch": 31.62962962962963,
      "grad_norm": 0.1263897567987442,
      "learning_rate": 2.9458917835671345e-05,
      "loss": 0.0246,
      "step": 854
    },
    {
      "epoch": 31.666666666666668,
      "grad_norm": 0.13853511214256287,
      "learning_rate": 2.925851703406814e-05,
      "loss": 0.0245,
      "step": 855
    },
    {
      "epoch": 31.703703703703702,
      "grad_norm": 0.11749710887670517,
      "learning_rate": 2.9058116232464932e-05,
      "loss": 0.0177,
      "step": 856
    },
    {
      "epoch": 31.74074074074074,
      "grad_norm": 0.1582382172346115,
      "learning_rate": 2.8857715430861727e-05,
      "loss": 0.0238,
      "step": 857
    },
    {
      "epoch": 31.77777777777778,
      "grad_norm": 0.1544945240020752,
      "learning_rate": 2.865731462925852e-05,
      "loss": 0.0239,
      "step": 858
    },
    {
      "epoch": 31.814814814814813,
      "grad_norm": 0.12094996869564056,
      "learning_rate": 2.8456913827655314e-05,
      "loss": 0.0233,
      "step": 859
    },
    {
      "epoch": 31.85185185185185,
      "grad_norm": 0.13803395628929138,
      "learning_rate": 2.8256513026052106e-05,
      "loss": 0.0254,
      "step": 860
    },
    {
      "epoch": 31.88888888888889,
      "grad_norm": 0.12196546047925949,
      "learning_rate": 2.8056112224448898e-05,
      "loss": 0.0221,
      "step": 861
    },
    {
      "epoch": 31.925925925925927,
      "grad_norm": 0.12061236798763275,
      "learning_rate": 2.7855711422845693e-05,
      "loss": 0.0216,
      "step": 862
    },
    {
      "epoch": 31.962962962962962,
      "grad_norm": 0.1225355863571167,
      "learning_rate": 2.7655310621242485e-05,
      "loss": 0.0229,
      "step": 863
    },
    {
      "epoch": 32.0,
      "grad_norm": 0.14704079926013947,
      "learning_rate": 2.745490981963928e-05,
      "loss": 0.0251,
      "step": 864
    },
    {
      "epoch": 32.03703703703704,
      "grad_norm": 0.10722658783197403,
      "learning_rate": 2.7254509018036072e-05,
      "loss": 0.0198,
      "step": 865
    },
    {
      "epoch": 32.074074074074076,
      "grad_norm": 0.09797307848930359,
      "learning_rate": 2.7054108216432867e-05,
      "loss": 0.0186,
      "step": 866
    },
    {
      "epoch": 32.111111111111114,
      "grad_norm": 0.12489169090986252,
      "learning_rate": 2.685370741482966e-05,
      "loss": 0.0218,
      "step": 867
    },
    {
      "epoch": 32.148148148148145,
      "grad_norm": 0.12327344715595245,
      "learning_rate": 2.6653306613226454e-05,
      "loss": 0.0208,
      "step": 868
    },
    {
      "epoch": 32.18518518518518,
      "grad_norm": 0.11047308892011642,
      "learning_rate": 2.6452905811623246e-05,
      "loss": 0.0205,
      "step": 869
    },
    {
      "epoch": 32.22222222222222,
      "grad_norm": 0.09973940998315811,
      "learning_rate": 2.625250501002004e-05,
      "loss": 0.0188,
      "step": 870
    },
    {
      "epoch": 32.25925925925926,
      "grad_norm": 0.12270811200141907,
      "learning_rate": 2.6052104208416833e-05,
      "loss": 0.0202,
      "step": 871
    },
    {
      "epoch": 32.2962962962963,
      "grad_norm": 0.11607006192207336,
      "learning_rate": 2.5851703406813628e-05,
      "loss": 0.0201,
      "step": 872
    },
    {
      "epoch": 32.333333333333336,
      "grad_norm": 0.12418902665376663,
      "learning_rate": 2.565130260521042e-05,
      "loss": 0.0213,
      "step": 873
    },
    {
      "epoch": 32.370370370370374,
      "grad_norm": 0.13776393234729767,
      "learning_rate": 2.5450901803607215e-05,
      "loss": 0.0222,
      "step": 874
    },
    {
      "epoch": 32.407407407407405,
      "grad_norm": 0.10824692994356155,
      "learning_rate": 2.5250501002004006e-05,
      "loss": 0.0186,
      "step": 875
    },
    {
      "epoch": 32.44444444444444,
      "grad_norm": 0.14757047593593597,
      "learning_rate": 2.50501002004008e-05,
      "loss": 0.0268,
      "step": 876
    },
    {
      "epoch": 32.48148148148148,
      "grad_norm": 0.09154849499464035,
      "learning_rate": 2.4849699398797597e-05,
      "loss": 0.0163,
      "step": 877
    },
    {
      "epoch": 32.51851851851852,
      "grad_norm": 0.13254225254058838,
      "learning_rate": 2.464929859719439e-05,
      "loss": 0.0235,
      "step": 878
    },
    {
      "epoch": 32.55555555555556,
      "grad_norm": 0.1262555718421936,
      "learning_rate": 2.4448897795591184e-05,
      "loss": 0.0202,
      "step": 879
    },
    {
      "epoch": 32.592592592592595,
      "grad_norm": 0.1142440140247345,
      "learning_rate": 2.4248496993987975e-05,
      "loss": 0.0202,
      "step": 880
    },
    {
      "epoch": 32.629629629629626,
      "grad_norm": 0.15125496685504913,
      "learning_rate": 2.404809619238477e-05,
      "loss": 0.023,
      "step": 881
    },
    {
      "epoch": 32.666666666666664,
      "grad_norm": 0.11355181038379669,
      "learning_rate": 2.3847695390781562e-05,
      "loss": 0.0203,
      "step": 882
    },
    {
      "epoch": 32.7037037037037,
      "grad_norm": 0.13875840604305267,
      "learning_rate": 2.3647294589178358e-05,
      "loss": 0.0219,
      "step": 883
    },
    {
      "epoch": 32.74074074074074,
      "grad_norm": 0.11910561472177505,
      "learning_rate": 2.3446893787575153e-05,
      "loss": 0.0239,
      "step": 884
    },
    {
      "epoch": 32.77777777777778,
      "grad_norm": 0.1369435340166092,
      "learning_rate": 2.3246492985971944e-05,
      "loss": 0.0225,
      "step": 885
    },
    {
      "epoch": 32.81481481481482,
      "grad_norm": 0.13384242355823517,
      "learning_rate": 2.304609218436874e-05,
      "loss": 0.0217,
      "step": 886
    },
    {
      "epoch": 32.851851851851855,
      "grad_norm": 0.1589844971895218,
      "learning_rate": 2.284569138276553e-05,
      "loss": 0.0246,
      "step": 887
    },
    {
      "epoch": 32.888888888888886,
      "grad_norm": 0.15792196989059448,
      "learning_rate": 2.2645290581162327e-05,
      "loss": 0.0253,
      "step": 888
    },
    {
      "epoch": 32.925925925925924,
      "grad_norm": 0.13627003133296967,
      "learning_rate": 2.244488977955912e-05,
      "loss": 0.0209,
      "step": 889
    },
    {
      "epoch": 32.96296296296296,
      "grad_norm": 0.14136220514774323,
      "learning_rate": 2.2244488977955913e-05,
      "loss": 0.0238,
      "step": 890
    },
    {
      "epoch": 33.0,
      "grad_norm": 0.11269474774599075,
      "learning_rate": 2.2044088176352705e-05,
      "loss": 0.0218,
      "step": 891
    },
    {
      "epoch": 33.03703703703704,
      "grad_norm": 0.12332484126091003,
      "learning_rate": 2.18436873747495e-05,
      "loss": 0.0209,
      "step": 892
    },
    {
      "epoch": 33.074074074074076,
      "grad_norm": 0.12890344858169556,
      "learning_rate": 2.1643286573146292e-05,
      "loss": 0.0202,
      "step": 893
    },
    {
      "epoch": 33.111111111111114,
      "grad_norm": 0.13154862821102142,
      "learning_rate": 2.1442885771543087e-05,
      "loss": 0.0204,
      "step": 894
    },
    {
      "epoch": 33.148148148148145,
      "grad_norm": 0.1281120479106903,
      "learning_rate": 2.124248496993988e-05,
      "loss": 0.0248,
      "step": 895
    },
    {
      "epoch": 33.18518518518518,
      "grad_norm": 0.11215731501579285,
      "learning_rate": 2.1042084168336674e-05,
      "loss": 0.021,
      "step": 896
    },
    {
      "epoch": 33.22222222222222,
      "grad_norm": 0.13047204911708832,
      "learning_rate": 2.0841683366733466e-05,
      "loss": 0.0216,
      "step": 897
    },
    {
      "epoch": 33.25925925925926,
      "grad_norm": 0.1485886424779892,
      "learning_rate": 2.064128256513026e-05,
      "loss": 0.024,
      "step": 898
    },
    {
      "epoch": 33.2962962962963,
      "grad_norm": 0.12551189959049225,
      "learning_rate": 2.0440881763527056e-05,
      "loss": 0.0206,
      "step": 899
    },
    {
      "epoch": 33.333333333333336,
      "grad_norm": 0.12064637988805771,
      "learning_rate": 2.0240480961923848e-05,
      "loss": 0.0198,
      "step": 900
    },
    {
      "epoch": 33.370370370370374,
      "grad_norm": 0.12191728502511978,
      "learning_rate": 2.0040080160320643e-05,
      "loss": 0.0213,
      "step": 901
    },
    {
      "epoch": 33.407407407407405,
      "grad_norm": 0.13351301848888397,
      "learning_rate": 1.9839679358717435e-05,
      "loss": 0.0216,
      "step": 902
    },
    {
      "epoch": 33.44444444444444,
      "grad_norm": 0.13217252492904663,
      "learning_rate": 1.963927855711423e-05,
      "loss": 0.0193,
      "step": 903
    },
    {
      "epoch": 33.48148148148148,
      "grad_norm": 0.12013563513755798,
      "learning_rate": 1.9438877755511022e-05,
      "loss": 0.0192,
      "step": 904
    },
    {
      "epoch": 33.51851851851852,
      "grad_norm": 0.15662236511707306,
      "learning_rate": 1.9238476953907817e-05,
      "loss": 0.0249,
      "step": 905
    },
    {
      "epoch": 33.55555555555556,
      "grad_norm": 0.10358313471078873,
      "learning_rate": 1.903807615230461e-05,
      "loss": 0.0175,
      "step": 906
    },
    {
      "epoch": 33.592592592592595,
      "grad_norm": 0.11853252351284027,
      "learning_rate": 1.8837675350701404e-05,
      "loss": 0.0212,
      "step": 907
    },
    {
      "epoch": 33.629629629629626,
      "grad_norm": 0.1346239149570465,
      "learning_rate": 1.8637274549098196e-05,
      "loss": 0.02,
      "step": 908
    },
    {
      "epoch": 33.666666666666664,
      "grad_norm": 0.1553751528263092,
      "learning_rate": 1.843687374749499e-05,
      "loss": 0.0246,
      "step": 909
    },
    {
      "epoch": 33.7037037037037,
      "grad_norm": 0.11390073597431183,
      "learning_rate": 1.8236472945891783e-05,
      "loss": 0.0188,
      "step": 910
    },
    {
      "epoch": 33.74074074074074,
      "grad_norm": 0.0979580208659172,
      "learning_rate": 1.8036072144288578e-05,
      "loss": 0.0175,
      "step": 911
    },
    {
      "epoch": 33.77777777777778,
      "grad_norm": 0.14626945555210114,
      "learning_rate": 1.7835671342685373e-05,
      "loss": 0.023,
      "step": 912
    },
    {
      "epoch": 33.81481481481482,
      "grad_norm": 0.1321498304605484,
      "learning_rate": 1.7635270541082165e-05,
      "loss": 0.0237,
      "step": 913
    },
    {
      "epoch": 33.851851851851855,
      "grad_norm": 0.11950874328613281,
      "learning_rate": 1.743486973947896e-05,
      "loss": 0.022,
      "step": 914
    },
    {
      "epoch": 33.888888888888886,
      "grad_norm": 0.11554690450429916,
      "learning_rate": 1.7234468937875752e-05,
      "loss": 0.0212,
      "step": 915
    },
    {
      "epoch": 33.925925925925924,
      "grad_norm": 0.09814371913671494,
      "learning_rate": 1.7034068136272547e-05,
      "loss": 0.017,
      "step": 916
    },
    {
      "epoch": 33.96296296296296,
      "grad_norm": 0.11823789030313492,
      "learning_rate": 1.683366733466934e-05,
      "loss": 0.0239,
      "step": 917
    },
    {
      "epoch": 34.0,
      "grad_norm": 0.16489100456237793,
      "learning_rate": 1.6633266533066134e-05,
      "loss": 0.0248,
      "step": 918
    },
    {
      "epoch": 34.03703703703704,
      "grad_norm": 0.10887830704450607,
      "learning_rate": 1.6432865731462926e-05,
      "loss": 0.0193,
      "step": 919
    },
    {
      "epoch": 34.074074074074076,
      "grad_norm": 0.10065386444330215,
      "learning_rate": 1.623246492985972e-05,
      "loss": 0.0181,
      "step": 920
    },
    {
      "epoch": 34.111111111111114,
      "grad_norm": 0.12003359943628311,
      "learning_rate": 1.6032064128256513e-05,
      "loss": 0.0196,
      "step": 921
    },
    {
      "epoch": 34.148148148148145,
      "grad_norm": 0.0964493378996849,
      "learning_rate": 1.5831663326653308e-05,
      "loss": 0.0131,
      "step": 922
    },
    {
      "epoch": 34.18518518518518,
      "grad_norm": 0.10737652331590652,
      "learning_rate": 1.56312625250501e-05,
      "loss": 0.0215,
      "step": 923
    },
    {
      "epoch": 34.22222222222222,
      "grad_norm": 0.10346271097660065,
      "learning_rate": 1.5430861723446895e-05,
      "loss": 0.0185,
      "step": 924
    },
    {
      "epoch": 34.25925925925926,
      "grad_norm": 0.10327797383069992,
      "learning_rate": 1.523046092184369e-05,
      "loss": 0.018,
      "step": 925
    },
    {
      "epoch": 34.2962962962963,
      "grad_norm": 0.10295984894037247,
      "learning_rate": 1.5030060120240483e-05,
      "loss": 0.0189,
      "step": 926
    },
    {
      "epoch": 34.333333333333336,
      "grad_norm": 0.14879319071769714,
      "learning_rate": 1.4829659318637277e-05,
      "loss": 0.0252,
      "step": 927
    },
    {
      "epoch": 34.370370370370374,
      "grad_norm": 0.17383703589439392,
      "learning_rate": 1.462925851703407e-05,
      "loss": 0.0253,
      "step": 928
    },
    {
      "epoch": 34.407407407407405,
      "grad_norm": 0.13976429402828217,
      "learning_rate": 1.4428857715430864e-05,
      "loss": 0.0245,
      "step": 929
    },
    {
      "epoch": 34.44444444444444,
      "grad_norm": 0.1539016216993332,
      "learning_rate": 1.4228456913827657e-05,
      "loss": 0.0259,
      "step": 930
    },
    {
      "epoch": 34.48148148148148,
      "grad_norm": 0.11989405006170273,
      "learning_rate": 1.4028056112224449e-05,
      "loss": 0.022,
      "step": 931
    },
    {
      "epoch": 34.51851851851852,
      "grad_norm": 0.12484333664178848,
      "learning_rate": 1.3827655310621242e-05,
      "loss": 0.0215,
      "step": 932
    },
    {
      "epoch": 34.55555555555556,
      "grad_norm": 0.1183113381266594,
      "learning_rate": 1.3627254509018036e-05,
      "loss": 0.0204,
      "step": 933
    },
    {
      "epoch": 34.592592592592595,
      "grad_norm": 0.12217496335506439,
      "learning_rate": 1.342685370741483e-05,
      "loss": 0.0225,
      "step": 934
    },
    {
      "epoch": 34.629629629629626,
      "grad_norm": 0.1154973953962326,
      "learning_rate": 1.3226452905811623e-05,
      "loss": 0.0188,
      "step": 935
    },
    {
      "epoch": 34.666666666666664,
      "grad_norm": 0.11576449871063232,
      "learning_rate": 1.3026052104208416e-05,
      "loss": 0.019,
      "step": 936
    },
    {
      "epoch": 34.7037037037037,
      "grad_norm": 0.15286089479923248,
      "learning_rate": 1.282565130260521e-05,
      "loss": 0.024,
      "step": 937
    },
    {
      "epoch": 34.74074074074074,
      "grad_norm": 0.12987518310546875,
      "learning_rate": 1.2625250501002003e-05,
      "loss": 0.0205,
      "step": 938
    },
    {
      "epoch": 34.77777777777778,
      "grad_norm": 0.12005002051591873,
      "learning_rate": 1.2424849699398798e-05,
      "loss": 0.0223,
      "step": 939
    },
    {
      "epoch": 34.81481481481482,
      "grad_norm": 0.13551415503025055,
      "learning_rate": 1.2224448897795592e-05,
      "loss": 0.022,
      "step": 940
    },
    {
      "epoch": 34.851851851851855,
      "grad_norm": 0.12061487138271332,
      "learning_rate": 1.2024048096192385e-05,
      "loss": 0.0217,
      "step": 941
    },
    {
      "epoch": 34.888888888888886,
      "grad_norm": 0.15077608823776245,
      "learning_rate": 1.1823647294589179e-05,
      "loss": 0.0241,
      "step": 942
    },
    {
      "epoch": 34.925925925925924,
      "grad_norm": 0.12646080553531647,
      "learning_rate": 1.1623246492985972e-05,
      "loss": 0.0235,
      "step": 943
    },
    {
      "epoch": 34.96296296296296,
      "grad_norm": 0.12024138867855072,
      "learning_rate": 1.1422845691382766e-05,
      "loss": 0.0206,
      "step": 944
    },
    {
      "epoch": 35.0,
      "grad_norm": 0.12281608581542969,
      "learning_rate": 1.122244488977956e-05,
      "loss": 0.0206,
      "step": 945
    },
    {
      "epoch": 35.03703703703704,
      "grad_norm": 0.12031093239784241,
      "learning_rate": 1.1022044088176353e-05,
      "loss": 0.0211,
      "step": 946
    },
    {
      "epoch": 35.074074074074076,
      "grad_norm": 0.10294829308986664,
      "learning_rate": 1.0821643286573146e-05,
      "loss": 0.0167,
      "step": 947
    },
    {
      "epoch": 35.111111111111114,
      "grad_norm": 0.15221378207206726,
      "learning_rate": 1.062124248496994e-05,
      "loss": 0.0247,
      "step": 948
    },
    {
      "epoch": 35.148148148148145,
      "grad_norm": 0.14258946478366852,
      "learning_rate": 1.0420841683366733e-05,
      "loss": 0.0217,
      "step": 949
    },
    {
      "epoch": 35.18518518518518,
      "grad_norm": 0.13070468604564667,
      "learning_rate": 1.0220440881763528e-05,
      "loss": 0.0197,
      "step": 950
    },
    {
      "epoch": 35.22222222222222,
      "grad_norm": 0.09948030859231949,
      "learning_rate": 1.0020040080160322e-05,
      "loss": 0.0174,
      "step": 951
    },
    {
      "epoch": 35.25925925925926,
      "grad_norm": 0.14215950667858124,
      "learning_rate": 9.819639278557115e-06,
      "loss": 0.0203,
      "step": 952
    },
    {
      "epoch": 35.2962962962963,
      "grad_norm": 0.09568937122821808,
      "learning_rate": 9.619238476953909e-06,
      "loss": 0.0169,
      "step": 953
    },
    {
      "epoch": 35.333333333333336,
      "grad_norm": 0.13030047714710236,
      "learning_rate": 9.418837675350702e-06,
      "loss": 0.0221,
      "step": 954
    },
    {
      "epoch": 35.370370370370374,
      "grad_norm": 0.12690897285938263,
      "learning_rate": 9.218436873747496e-06,
      "loss": 0.0226,
      "step": 955
    },
    {
      "epoch": 35.407407407407405,
      "grad_norm": 0.13108162581920624,
      "learning_rate": 9.018036072144289e-06,
      "loss": 0.0219,
      "step": 956
    },
    {
      "epoch": 35.44444444444444,
      "grad_norm": 0.12648101150989532,
      "learning_rate": 8.817635270541082e-06,
      "loss": 0.0207,
      "step": 957
    },
    {
      "epoch": 35.48148148148148,
      "grad_norm": 0.12825319170951843,
      "learning_rate": 8.617234468937876e-06,
      "loss": 0.0228,
      "step": 958
    },
    {
      "epoch": 35.51851851851852,
      "grad_norm": 0.13205444812774658,
      "learning_rate": 8.41683366733467e-06,
      "loss": 0.0208,
      "step": 959
    },
    {
      "epoch": 35.55555555555556,
      "grad_norm": 0.10195484012365341,
      "learning_rate": 8.216432865731463e-06,
      "loss": 0.0194,
      "step": 960
    },
    {
      "epoch": 35.592592592592595,
      "grad_norm": 0.11535032093524933,
      "learning_rate": 8.016032064128256e-06,
      "loss": 0.019,
      "step": 961
    },
    {
      "epoch": 35.629629629629626,
      "grad_norm": 0.14503641426563263,
      "learning_rate": 7.81563126252505e-06,
      "loss": 0.024,
      "step": 962
    },
    {
      "epoch": 35.666666666666664,
      "grad_norm": 0.10808087885379791,
      "learning_rate": 7.615230460921845e-06,
      "loss": 0.0223,
      "step": 963
    },
    {
      "epoch": 35.7037037037037,
      "grad_norm": 0.11599882692098618,
      "learning_rate": 7.414829659318638e-06,
      "loss": 0.0213,
      "step": 964
    },
    {
      "epoch": 35.74074074074074,
      "grad_norm": 0.12752510607242584,
      "learning_rate": 7.214428857715432e-06,
      "loss": 0.0234,
      "step": 965
    },
    {
      "epoch": 35.77777777777778,
      "grad_norm": 0.15204395353794098,
      "learning_rate": 7.0140280561122245e-06,
      "loss": 0.0251,
      "step": 966
    },
    {
      "epoch": 35.81481481481482,
      "grad_norm": 0.10353588312864304,
      "learning_rate": 6.813627254509018e-06,
      "loss": 0.0178,
      "step": 967
    },
    {
      "epoch": 35.851851851851855,
      "grad_norm": 0.13647359609603882,
      "learning_rate": 6.613226452905811e-06,
      "loss": 0.0233,
      "step": 968
    },
    {
      "epoch": 35.888888888888886,
      "grad_norm": 0.14595043659210205,
      "learning_rate": 6.412825651302605e-06,
      "loss": 0.0255,
      "step": 969
    },
    {
      "epoch": 35.925925925925924,
      "grad_norm": 0.12824155390262604,
      "learning_rate": 6.212424849699399e-06,
      "loss": 0.0198,
      "step": 970
    },
    {
      "epoch": 35.96296296296296,
      "grad_norm": 0.1400662511587143,
      "learning_rate": 6.012024048096193e-06,
      "loss": 0.0203,
      "step": 971
    },
    {
      "epoch": 36.0,
      "grad_norm": 0.1014123409986496,
      "learning_rate": 5.811623246492986e-06,
      "loss": 0.017,
      "step": 972
    },
    {
      "epoch": 36.03703703703704,
      "grad_norm": 0.11156478524208069,
      "learning_rate": 5.61122244488978e-06,
      "loss": 0.0197,
      "step": 973
    },
    {
      "epoch": 36.074074074074076,
      "grad_norm": 0.13545890152454376,
      "learning_rate": 5.410821643286573e-06,
      "loss": 0.0205,
      "step": 974
    },
    {
      "epoch": 36.111111111111114,
      "grad_norm": 0.12780331075191498,
      "learning_rate": 5.2104208416833665e-06,
      "loss": 0.0217,
      "step": 975
    },
    {
      "epoch": 36.148148148148145,
      "grad_norm": 0.1330299973487854,
      "learning_rate": 5.010020040080161e-06,
      "loss": 0.0227,
      "step": 976
    },
    {
      "epoch": 36.18518518518518,
      "grad_norm": 0.14134016633033752,
      "learning_rate": 4.809619238476954e-06,
      "loss": 0.0215,
      "step": 977
    },
    {
      "epoch": 36.22222222222222,
      "grad_norm": 0.11627551913261414,
      "learning_rate": 4.609218436873748e-06,
      "loss": 0.021,
      "step": 978
    },
    {
      "epoch": 36.25925925925926,
      "grad_norm": 0.12162772566080093,
      "learning_rate": 4.408817635270541e-06,
      "loss": 0.019,
      "step": 979
    },
    {
      "epoch": 36.2962962962963,
      "grad_norm": 0.12578049302101135,
      "learning_rate": 4.208416833667335e-06,
      "loss": 0.0229,
      "step": 980
    },
    {
      "epoch": 36.333333333333336,
      "grad_norm": 0.11840208619832993,
      "learning_rate": 4.008016032064128e-06,
      "loss": 0.0185,
      "step": 981
    },
    {
      "epoch": 36.370370370370374,
      "grad_norm": 0.14518216252326965,
      "learning_rate": 3.8076152304609225e-06,
      "loss": 0.0234,
      "step": 982
    },
    {
      "epoch": 36.407407407407405,
      "grad_norm": 0.14974501729011536,
      "learning_rate": 3.607214428857716e-06,
      "loss": 0.0222,
      "step": 983
    },
    {
      "epoch": 36.44444444444444,
      "grad_norm": 0.10942941904067993,
      "learning_rate": 3.406813627254509e-06,
      "loss": 0.0213,
      "step": 984
    },
    {
      "epoch": 36.48148148148148,
      "grad_norm": 0.12075510621070862,
      "learning_rate": 3.2064128256513024e-06,
      "loss": 0.0193,
      "step": 985
    },
    {
      "epoch": 36.51851851851852,
      "grad_norm": 0.09871549159288406,
      "learning_rate": 3.0060120240480963e-06,
      "loss": 0.0183,
      "step": 986
    },
    {
      "epoch": 36.55555555555556,
      "grad_norm": 0.1312011033296585,
      "learning_rate": 2.80561122244489e-06,
      "loss": 0.0247,
      "step": 987
    },
    {
      "epoch": 36.592592592592595,
      "grad_norm": 0.12420856207609177,
      "learning_rate": 2.6052104208416833e-06,
      "loss": 0.0174,
      "step": 988
    },
    {
      "epoch": 36.629629629629626,
      "grad_norm": 0.11512743681669235,
      "learning_rate": 2.404809619238477e-06,
      "loss": 0.0218,
      "step": 989
    },
    {
      "epoch": 36.666666666666664,
      "grad_norm": 0.1025930717587471,
      "learning_rate": 2.2044088176352706e-06,
      "loss": 0.0183,
      "step": 990
    },
    {
      "epoch": 36.7037037037037,
      "grad_norm": 0.14355269074440002,
      "learning_rate": 2.004008016032064e-06,
      "loss": 0.0212,
      "step": 991
    },
    {
      "epoch": 36.74074074074074,
      "grad_norm": 0.13891130685806274,
      "learning_rate": 1.803607214428858e-06,
      "loss": 0.0232,
      "step": 992
    },
    {
      "epoch": 36.77777777777778,
      "grad_norm": 0.11977527290582657,
      "learning_rate": 1.6032064128256512e-06,
      "loss": 0.0191,
      "step": 993
    },
    {
      "epoch": 36.81481481481482,
      "grad_norm": 0.14400973916053772,
      "learning_rate": 1.402805611222445e-06,
      "loss": 0.0218,
      "step": 994
    },
    {
      "epoch": 36.851851851851855,
      "grad_norm": 0.13667039573192596,
      "learning_rate": 1.2024048096192386e-06,
      "loss": 0.0234,
      "step": 995
    },
    {
      "epoch": 36.888888888888886,
      "grad_norm": 0.12580057978630066,
      "learning_rate": 1.002004008016032e-06,
      "loss": 0.0211,
      "step": 996
    },
    {
      "epoch": 36.925925925925924,
      "grad_norm": 0.09683796763420105,
      "learning_rate": 8.016032064128256e-07,
      "loss": 0.0178,
      "step": 997
    },
    {
      "epoch": 36.96296296296296,
      "grad_norm": 0.11867684125900269,
      "learning_rate": 6.012024048096193e-07,
      "loss": 0.0183,
      "step": 998
    },
    {
      "epoch": 37.0,
      "grad_norm": 0.12983517348766327,
      "learning_rate": 4.008016032064128e-07,
      "loss": 0.0225,
      "step": 999
    },
    {
      "epoch": 37.03703703703704,
      "grad_norm": 0.1135377511382103,
      "learning_rate": 2.004008016032064e-07,
      "loss": 0.0198,
      "step": 1000
    },
    {
      "epoch": 37.03703703703704,
      "step": 1000,
      "total_flos": 4.053729257703014e+16,
      "train_loss": 0.16643435242865234,
      "train_runtime": 7509.3036,
      "train_samples_per_second": 0.533,
      "train_steps_per_second": 0.133
    }
  ],
  "logging_steps": 1,
  "max_steps": 1000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 38,
  "save_steps": 500,
  "total_flos": 4.053729257703014e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
