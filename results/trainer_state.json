{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.7037037037037037,
  "eval_steps": 500,
  "global_step": 100,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.037037037037037035,
      "grad_norm": 3.1930129528045654,
      "learning_rate": 0.0001,
      "loss": 4.2035,
      "step": 1
    },
    {
      "epoch": 0.07407407407407407,
      "grad_norm": 3.2198219299316406,
      "learning_rate": 0.0002,
      "loss": 4.5528,
      "step": 2
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 2.335556745529175,
      "learning_rate": 0.00019795918367346938,
      "loss": 4.0377,
      "step": 3
    },
    {
      "epoch": 0.14814814814814814,
      "grad_norm": 2.676669120788574,
      "learning_rate": 0.0001959183673469388,
      "loss": 3.6744,
      "step": 4
    },
    {
      "epoch": 0.18518518518518517,
      "grad_norm": 2.552551031112671,
      "learning_rate": 0.00019387755102040816,
      "loss": 3.0695,
      "step": 5
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 2.707568407058716,
      "learning_rate": 0.00019183673469387756,
      "loss": 2.3562,
      "step": 6
    },
    {
      "epoch": 0.25925925925925924,
      "grad_norm": 4.3245625495910645,
      "learning_rate": 0.00018979591836734697,
      "loss": 2.6092,
      "step": 7
    },
    {
      "epoch": 0.2962962962962963,
      "grad_norm": 2.9887053966522217,
      "learning_rate": 0.00018775510204081634,
      "loss": 1.8859,
      "step": 8
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 2.427330255508423,
      "learning_rate": 0.00018571428571428572,
      "loss": 2.1173,
      "step": 9
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 2.2078428268432617,
      "learning_rate": 0.00018367346938775512,
      "loss": 1.6469,
      "step": 10
    },
    {
      "epoch": 0.4074074074074074,
      "grad_norm": 1.8940097093582153,
      "learning_rate": 0.0001816326530612245,
      "loss": 1.6861,
      "step": 11
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": NaN,
      "learning_rate": 0.0001816326530612245,
      "loss": 1.7832,
      "step": 12
    },
    {
      "epoch": 0.48148148148148145,
      "grad_norm": 5.24237585067749,
      "learning_rate": 0.0001795918367346939,
      "loss": 1.5285,
      "step": 13
    },
    {
      "epoch": 0.5185185185185185,
      "grad_norm": 2.240490436553955,
      "learning_rate": 0.00017755102040816327,
      "loss": 1.8779,
      "step": 14
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 5.590706825256348,
      "learning_rate": 0.00017551020408163265,
      "loss": 1.4134,
      "step": 15
    },
    {
      "epoch": 0.5925925925925926,
      "grad_norm": 1.4625368118286133,
      "learning_rate": 0.00017346938775510205,
      "loss": 1.5196,
      "step": 16
    },
    {
      "epoch": 0.6296296296296297,
      "grad_norm": 1.284386157989502,
      "learning_rate": 0.00017142857142857143,
      "loss": 1.5036,
      "step": 17
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 1.4751670360565186,
      "learning_rate": 0.00016938775510204083,
      "loss": 1.4972,
      "step": 18
    },
    {
      "epoch": 0.7037037037037037,
      "grad_norm": 1.6895999908447266,
      "learning_rate": 0.00016734693877551023,
      "loss": 1.2411,
      "step": 19
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 1.1008516550064087,
      "learning_rate": 0.0001653061224489796,
      "loss": 1.479,
      "step": 20
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 1.5900424718856812,
      "learning_rate": 0.00016326530612244898,
      "loss": 1.721,
      "step": 21
    },
    {
      "epoch": 0.8148148148148148,
      "grad_norm": 1.287550926208496,
      "learning_rate": 0.00016122448979591838,
      "loss": 1.0774,
      "step": 22
    },
    {
      "epoch": 0.8518518518518519,
      "grad_norm": 1.306200623512268,
      "learning_rate": 0.00015918367346938776,
      "loss": 1.4775,
      "step": 23
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.2184394598007202,
      "learning_rate": 0.00015714285714285716,
      "loss": 1.1601,
      "step": 24
    },
    {
      "epoch": 0.9259259259259259,
      "grad_norm": 1.3299381732940674,
      "learning_rate": 0.00015510204081632654,
      "loss": 1.4824,
      "step": 25
    },
    {
      "epoch": 0.9629629629629629,
      "grad_norm": 1.2562974691390991,
      "learning_rate": 0.0001530612244897959,
      "loss": 1.5145,
      "step": 26
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.2835960388183594,
      "learning_rate": 0.0001510204081632653,
      "loss": 1.562,
      "step": 27
    },
    {
      "epoch": 1.037037037037037,
      "grad_norm": 1.1650491952896118,
      "learning_rate": 0.00014897959183673472,
      "loss": 1.2128,
      "step": 28
    },
    {
      "epoch": 1.074074074074074,
      "grad_norm": 1.2172163724899292,
      "learning_rate": 0.0001469387755102041,
      "loss": 0.9919,
      "step": 29
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 1.4455448389053345,
      "learning_rate": 0.0001448979591836735,
      "loss": 1.0853,
      "step": 30
    },
    {
      "epoch": 1.1481481481481481,
      "grad_norm": 1.2032299041748047,
      "learning_rate": 0.00014285714285714287,
      "loss": 1.0893,
      "step": 31
    },
    {
      "epoch": 1.1851851851851851,
      "grad_norm": 1.282399296760559,
      "learning_rate": 0.00014081632653061224,
      "loss": 1.146,
      "step": 32
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 1.8321062326431274,
      "learning_rate": 0.00013877551020408165,
      "loss": 1.2051,
      "step": 33
    },
    {
      "epoch": 1.2592592592592593,
      "grad_norm": 1.5444517135620117,
      "learning_rate": 0.00013673469387755102,
      "loss": 1.3936,
      "step": 34
    },
    {
      "epoch": 1.2962962962962963,
      "grad_norm": 1.394068956375122,
      "learning_rate": 0.0001346938775510204,
      "loss": 1.4489,
      "step": 35
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 1.353715181350708,
      "learning_rate": 0.0001326530612244898,
      "loss": 1.0651,
      "step": 36
    },
    {
      "epoch": 1.3703703703703702,
      "grad_norm": 1.2550854682922363,
      "learning_rate": 0.00013061224489795917,
      "loss": 1.4743,
      "step": 37
    },
    {
      "epoch": 1.4074074074074074,
      "grad_norm": 1.1577472686767578,
      "learning_rate": 0.00012857142857142858,
      "loss": 1.1833,
      "step": 38
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 1.3438795804977417,
      "learning_rate": 0.00012653061224489798,
      "loss": 1.2138,
      "step": 39
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 1.155227541923523,
      "learning_rate": 0.00012448979591836735,
      "loss": 1.0745,
      "step": 40
    },
    {
      "epoch": 1.5185185185185186,
      "grad_norm": 1.1297647953033447,
      "learning_rate": 0.00012244897959183676,
      "loss": 0.9412,
      "step": 41
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 1.183184027671814,
      "learning_rate": 0.00012040816326530613,
      "loss": 1.1134,
      "step": 42
    },
    {
      "epoch": 1.5925925925925926,
      "grad_norm": 1.0724462270736694,
      "learning_rate": 0.00011836734693877552,
      "loss": 0.9647,
      "step": 43
    },
    {
      "epoch": 1.6296296296296298,
      "grad_norm": 1.3824230432510376,
      "learning_rate": 0.0001163265306122449,
      "loss": 0.9288,
      "step": 44
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 1.3321043252944946,
      "learning_rate": 0.00011428571428571428,
      "loss": 1.4542,
      "step": 45
    },
    {
      "epoch": 1.7037037037037037,
      "grad_norm": 1.2708547115325928,
      "learning_rate": 0.00011224489795918367,
      "loss": 1.0004,
      "step": 46
    },
    {
      "epoch": 1.7407407407407407,
      "grad_norm": 1.4334988594055176,
      "learning_rate": 0.00011020408163265306,
      "loss": 1.3144,
      "step": 47
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 1.326064944267273,
      "learning_rate": 0.00010816326530612246,
      "loss": 1.2947,
      "step": 48
    },
    {
      "epoch": 1.8148148148148149,
      "grad_norm": 1.0875673294067383,
      "learning_rate": 0.00010612244897959185,
      "loss": 1.1808,
      "step": 49
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 1.2442972660064697,
      "learning_rate": 0.00010408163265306123,
      "loss": 1.1764,
      "step": 50
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 1.3583104610443115,
      "learning_rate": 0.00010204081632653062,
      "loss": 1.238,
      "step": 51
    },
    {
      "epoch": 1.925925925925926,
      "grad_norm": 1.355242371559143,
      "learning_rate": 0.0001,
      "loss": 0.9865,
      "step": 52
    },
    {
      "epoch": 1.9629629629629628,
      "grad_norm": 1.367920160293579,
      "learning_rate": 9.79591836734694e-05,
      "loss": 1.2652,
      "step": 53
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.2265360355377197,
      "learning_rate": 9.591836734693878e-05,
      "loss": 1.4121,
      "step": 54
    },
    {
      "epoch": 2.037037037037037,
      "grad_norm": 1.316346526145935,
      "learning_rate": 9.387755102040817e-05,
      "loss": 1.0814,
      "step": 55
    },
    {
      "epoch": 2.074074074074074,
      "grad_norm": 1.203069806098938,
      "learning_rate": 9.183673469387756e-05,
      "loss": 0.8386,
      "step": 56
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 1.3500429391860962,
      "learning_rate": 8.979591836734695e-05,
      "loss": 0.8509,
      "step": 57
    },
    {
      "epoch": 2.148148148148148,
      "grad_norm": 1.3519675731658936,
      "learning_rate": 8.775510204081632e-05,
      "loss": 1.0939,
      "step": 58
    },
    {
      "epoch": 2.185185185185185,
      "grad_norm": 1.34112548828125,
      "learning_rate": 8.571428571428571e-05,
      "loss": 1.0793,
      "step": 59
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 1.4693788290023804,
      "learning_rate": 8.367346938775511e-05,
      "loss": 1.0799,
      "step": 60
    },
    {
      "epoch": 2.259259259259259,
      "grad_norm": 1.3565605878829956,
      "learning_rate": 8.163265306122449e-05,
      "loss": 0.7943,
      "step": 61
    },
    {
      "epoch": 2.2962962962962963,
      "grad_norm": 1.6935101747512817,
      "learning_rate": 7.959183673469388e-05,
      "loss": 1.0747,
      "step": 62
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 1.5701003074645996,
      "learning_rate": 7.755102040816327e-05,
      "loss": 0.782,
      "step": 63
    },
    {
      "epoch": 2.3703703703703702,
      "grad_norm": 1.641190528869629,
      "learning_rate": 7.551020408163266e-05,
      "loss": 0.9575,
      "step": 64
    },
    {
      "epoch": 2.4074074074074074,
      "grad_norm": 1.5583246946334839,
      "learning_rate": 7.346938775510205e-05,
      "loss": 0.8789,
      "step": 65
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 1.389485478401184,
      "learning_rate": 7.142857142857143e-05,
      "loss": 0.9477,
      "step": 66
    },
    {
      "epoch": 2.4814814814814814,
      "grad_norm": 1.3585546016693115,
      "learning_rate": 6.938775510204082e-05,
      "loss": 0.7998,
      "step": 67
    },
    {
      "epoch": 2.5185185185185186,
      "grad_norm": 1.582020878791809,
      "learning_rate": 6.73469387755102e-05,
      "loss": 0.7905,
      "step": 68
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 1.5046128034591675,
      "learning_rate": 6.530612244897959e-05,
      "loss": 1.0267,
      "step": 69
    },
    {
      "epoch": 2.5925925925925926,
      "grad_norm": 1.4073487520217896,
      "learning_rate": 6.326530612244899e-05,
      "loss": 1.0292,
      "step": 70
    },
    {
      "epoch": 2.6296296296296298,
      "grad_norm": 1.4532362222671509,
      "learning_rate": 6.122448979591838e-05,
      "loss": 1.1837,
      "step": 71
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 1.5134122371673584,
      "learning_rate": 5.918367346938776e-05,
      "loss": 0.8836,
      "step": 72
    },
    {
      "epoch": 2.7037037037037037,
      "grad_norm": 1.5409324169158936,
      "learning_rate": 5.714285714285714e-05,
      "loss": 0.6746,
      "step": 73
    },
    {
      "epoch": 2.7407407407407405,
      "grad_norm": 1.686130404472351,
      "learning_rate": 5.510204081632653e-05,
      "loss": 0.8394,
      "step": 74
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 1.4209401607513428,
      "learning_rate": 5.3061224489795926e-05,
      "loss": 0.8264,
      "step": 75
    },
    {
      "epoch": 2.814814814814815,
      "grad_norm": 1.4342833757400513,
      "learning_rate": 5.102040816326531e-05,
      "loss": 0.8445,
      "step": 76
    },
    {
      "epoch": 2.851851851851852,
      "grad_norm": 1.4283452033996582,
      "learning_rate": 4.89795918367347e-05,
      "loss": 1.0321,
      "step": 77
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 1.4428730010986328,
      "learning_rate": 4.6938775510204086e-05,
      "loss": 0.7895,
      "step": 78
    },
    {
      "epoch": 2.925925925925926,
      "grad_norm": 1.4847509860992432,
      "learning_rate": 4.4897959183673474e-05,
      "loss": 0.7645,
      "step": 79
    },
    {
      "epoch": 2.962962962962963,
      "grad_norm": 1.761368989944458,
      "learning_rate": 4.2857142857142856e-05,
      "loss": 0.8032,
      "step": 80
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.4736109972000122,
      "learning_rate": 4.0816326530612245e-05,
      "loss": 0.9776,
      "step": 81
    },
    {
      "epoch": 3.037037037037037,
      "grad_norm": 1.65175461769104,
      "learning_rate": 3.8775510204081634e-05,
      "loss": 0.7442,
      "step": 82
    },
    {
      "epoch": 3.074074074074074,
      "grad_norm": 1.4563769102096558,
      "learning_rate": 3.673469387755102e-05,
      "loss": 0.7113,
      "step": 83
    },
    {
      "epoch": 3.111111111111111,
      "grad_norm": 1.5737709999084473,
      "learning_rate": 3.469387755102041e-05,
      "loss": 1.1708,
      "step": 84
    },
    {
      "epoch": 3.148148148148148,
      "grad_norm": 1.433469533920288,
      "learning_rate": 3.265306122448979e-05,
      "loss": 0.6768,
      "step": 85
    },
    {
      "epoch": 3.185185185185185,
      "grad_norm": 1.65626060962677,
      "learning_rate": 3.061224489795919e-05,
      "loss": 0.7308,
      "step": 86
    },
    {
      "epoch": 3.2222222222222223,
      "grad_norm": 1.6836720705032349,
      "learning_rate": 2.857142857142857e-05,
      "loss": 0.7407,
      "step": 87
    },
    {
      "epoch": 3.259259259259259,
      "grad_norm": 1.7914049625396729,
      "learning_rate": 2.6530612244897963e-05,
      "loss": 0.8776,
      "step": 88
    },
    {
      "epoch": 3.2962962962962963,
      "grad_norm": 1.9324854612350464,
      "learning_rate": 2.448979591836735e-05,
      "loss": 0.7594,
      "step": 89
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 1.8412007093429565,
      "learning_rate": 2.2448979591836737e-05,
      "loss": 1.0077,
      "step": 90
    },
    {
      "epoch": 3.3703703703703702,
      "grad_norm": 2.0423827171325684,
      "learning_rate": 2.0408163265306123e-05,
      "loss": 0.5968,
      "step": 91
    },
    {
      "epoch": 3.4074074074074074,
      "grad_norm": 1.882962703704834,
      "learning_rate": 1.836734693877551e-05,
      "loss": 0.4788,
      "step": 92
    },
    {
      "epoch": 3.4444444444444446,
      "grad_norm": 1.8317875862121582,
      "learning_rate": 1.6326530612244897e-05,
      "loss": 0.7228,
      "step": 93
    },
    {
      "epoch": 3.4814814814814814,
      "grad_norm": 2.0094902515411377,
      "learning_rate": 1.4285714285714285e-05,
      "loss": 0.4513,
      "step": 94
    },
    {
      "epoch": 3.5185185185185186,
      "grad_norm": 1.970187783241272,
      "learning_rate": 1.2244897959183674e-05,
      "loss": 0.5107,
      "step": 95
    },
    {
      "epoch": 3.5555555555555554,
      "grad_norm": 1.9955490827560425,
      "learning_rate": 1.0204081632653061e-05,
      "loss": 0.4677,
      "step": 96
    },
    {
      "epoch": 3.5925925925925926,
      "grad_norm": 2.086177110671997,
      "learning_rate": 8.163265306122448e-06,
      "loss": 0.8073,
      "step": 97
    },
    {
      "epoch": 3.6296296296296298,
      "grad_norm": 1.9866368770599365,
      "learning_rate": 6.122448979591837e-06,
      "loss": 0.9197,
      "step": 98
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 2.0212972164154053,
      "learning_rate": 4.081632653061224e-06,
      "loss": 0.8482,
      "step": 99
    },
    {
      "epoch": 3.7037037037037037,
      "grad_norm": 1.9462895393371582,
      "learning_rate": 2.040816326530612e-06,
      "loss": 0.6561,
      "step": 100
    },
    {
      "epoch": 3.7037037037037037,
      "step": 100,
      "total_flos": 4054646644088832.0,
      "train_loss": 1.2613574823737144,
      "train_runtime": 750.4131,
      "train_samples_per_second": 0.533,
      "train_steps_per_second": 0.133
    }
  ],
  "logging_steps": 1,
  "max_steps": 100,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "total_flos": 4054646644088832.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
